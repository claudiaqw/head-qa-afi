\chapter{Estado del Arte}\label{chapter:background}

En el presente capítulo se discuten brevemente aquellos conceptos que constituyen el fundamento teórico para el desarrollo de este trabajo.

\section{Sistemas Pregunta/Respuesta}

Los sistemas Preguntas/Respuestas han sido uno de los problemas planteados por los humanos como primer paso para el entendimiento del lenguaje natural, de ahí que los diferentes enfoques para intentar resolver este tipo de problemas se remonten al siglo anterior.

Los primeros enfoques se caracterizaban por el uso de reglas predefinidas, estas soluciones aunque logran dar una respuesta muy exacta a determinadas preguntas en dominios específicos, no escalan bien cuando se trata de textos con formatos y temas variados. Posteriormente, los avances se centran en el uso de \textit{feature engineering}, herramientas linguísticas o recursos de la lengua externos como WordNet. \cite{2013-yih-lexical-semantic-model}, \cite{2013-yao-edit-distance} y \cite{2010-textual-entailment} son algunos de los exponentes de esta tendencia. Aunque estos métodos muestran efectividad, pueden verse afectados por la disponibilidad de recursos adicionales, el esfuerzo de la ingeniería de características y la complejidad sistemática al introducir herramientas lingüísticas, como árboles de análisis y árboles de dependencia. 

Los avances más recientes en Q/A, y en el área de procesamiento del lenguaje natural en general, han sido protagonizados por modelos basados en redes neuronales. Muchas propuestas han sido enfocadas desde el aprendizaje supervisado, de ahí que existan varios conjuntos de datos como son bAbI y SQuAD. En algunos de estos datasets los sistemas artificiales han llegado a obtener un rendimiento cercano a los resultados alcanzados por humanos. 

Sin embargo, estos modelos se adaptan a los datasets de manera que los sistemas pueden lograr alcanzar buenos resultados con un conocimiento muy superficial. Los sistemas QA de múltiples opciones surgen para contrarrestar este efecto y son conocidos en la literatura como sistemas de selección de respuestas (en inglés, sistemas \textit{Answer Selection}). 

Adicionalmente, la mayor parte de estos datasets antes mencionados están enfocados en el idioma inglés y en dominios del conocimiento generales. Esto ha ocasionado que la mayoría de los avances en el área del descubrimiento de conocimiento a partir de texto en lenguaje natural se hayan concentrado en este idioma, provocando incluso que autores de origen hispano enfoquen sus trabajos y aportes en el idioma inglés. Adicionalmente y por la dificultad que conlleva la construcción de datasets de este tipo, existen muy pocos datasets sobre dominios específicos como Medicina, Biología, entre otros. Aunque existen trabajos en estos dominios como \cite{2015-semantic-medical-texts} y \cite{2018-nentidis-results}, los autores \cite{2019-head-qa} hallan en esta problemática la motivación para la presentación de Head QA, un dataset Q/A específicamente de selección de respuestas en español e inglés, que combina la necesidad de conocimiento y razonamiento en dominios complejos y que resulta difícil responder correctamente, incluso para humanos con años de entrenamiento.

Dado que los mejores resultados en esta área han sido alcanzados mediante la aplicación de modelos de aprendizaje profundo, este trabajo se concentrará en el desarrollo e implementación de modelos neuronales aplicados al conjunto de datos Head QA. Por esta razón, en la siguiente sección serán abordados los avances más recientes en el aprendizaje profundo aplicado al procesamiento del lenguaje natural con el fin de sentar las bases de los modelos a proponer.

\section{Recuperación de Información}

En \cite{2019-head-qa}, se presentan un conjunto de modelos que pueden ser utilizados como \textit{baselines} para el conjunto de datos en inglés y español. Todos los \textit{baselines} siguen el paradigma de recuperación de información clásica utilizando Wikipedia como fuente de información externa. 

Un sistema de recuperación de información clásico tiene como base un corpus compuesto por un conjunto de documentos $D$. El objetivo es a partir de una consulta $q$ expresada en lenguaje natural, obtener un ordenamiento de los documentos de acuerdo a su relevancia o nivel de similaridad con dicha consulta. De manera que, los primeros lugares del \textit{ranking} correspondan a los documentos más relevantes a la consulta.

Desde el punto de vista de recuperación de información, un sistema Q/A de selección puede ser planteado como: 

Sean $(q_{i}, [a_{i0}, a_{i1}, ..., a_{im}])$ una pregunta y las posibles respuestas. Se crean $m$ consultas de la forma $q_{i} + a_{im}$ que son enviadas al motor de búsqueda de manera independiente. El motor de búsqueda devuelve para cada consulta enviada un número que indica la mayor relevancia obtenida, es decir, el \textit{score} del documento que mayor puntuación alcanzó en cada caso. Luego, la respuesta correcta $a_{im}$ es seleccionada en base a la consulta $q_{i} + a{i}{m}$ que mayor probabilidad alcanzó, lo que indica que es la respuesta donde se encontró el documento con mayor relevancia de todo el corpus. 

En \cite{2019-head-qa} para el conjunto de datos en español se utilizó como motor de búsqueda el presentado en \cite{2017-chen-wikipedia}, el cual calcula la similitud entre la consulta y los documentos representados vectorialmente con el $tf\_idf$ como la suma ponderada de los vectores palabra, además también tiene en cuenta el orden y el conteo de bigramas.

Otro de los trabajos enfocados en este conjunto de datos específico se presenta en \cite{2020-multi-step}. Los autores presentan un sistema multipasos basado en un framework de extraccción de conocimiento llamado MurKe. Inicialmente, extrae información de un gran corpus conformado por documentos de salud. Para encontrar la cadena de razonamiento y elegir la respuesta correcta, MurKe itera entre seleccionar los documentos de respaldo, reformular la representación de la consulta utilizando los documentos de respaldo y obtener la puntuación de vinculación para cada elección utilizando el modelo de vinculación. El módulo de reformulación aprovecha documentos seleccionados para evidencia faltante, lo que mantiene la interpretabilidad. Además, hacen un uso completo de los modelos pre-entrenados listos para usar. Con menos peso entrenable, el modelo previamente entrenado puede adaptarse fácilmente a las tareas de atención médica con ejemplos de entrenamiento limitados. A partir de los resultados experimentales, este sistema es capaz de superar varias líneas de base sólidas en el conjunto de datos HeadQA. 

En el dataset no ha sido aplicado ninguna técnica de selección de respuestas sobre la base del aprendizaje supervisado y sin el uso de fuentes de conocimiento externas, por lo que este trabajo se enfocará en la aplicación de modelos supervisados. Dada la dificultad de la tarea en el ámbito del entendimiento del lenguaje humano y teniendo el cuenta el estado del arte en procesamiento del lenguaje natural, los modelos a implementar estarán orientados al uso de técnicas de aprendizaje profundo. Es importante destacar que la naturaleza de este tipo de problemas conlleva a que modelos que utilizan conocimiento externo alcancen mejores resultados, pues los modelos supervisados encuentran muy difícil adaptarse a la complejidad y especificidad de las preguntas, sobre todo cuando no se cuenta con un conjunto de datos suficientemente extenso.

\section{Aprendizaje Profundo en NLP}

En la presente sección se introduce brevemente el concepto de \textit{word embedding}, el cual constituye un enfoque clave en gran parte de las propuestas contemporáneas. De igual manera se presenta una revisión de los principales resultados logrados en los sistemas Q/A utilizando el enfoque del aprendizaje profundo.

Uno de los principales retos en los problemas de procesamiento del lenguaje natural es encontrar una representación vectorial de un texto que sea lo suficientemente rica y expresiva. En otras palabras, una forma matemática de representar la información relevante y el conocimiento contenidos en el lenguaje humano. Intuitivamente, mientras más rica sea la representación, más información estarán utilizando los modelos de aprendizaje y estarán más cerca de lograr buenos resultados en el entendimiento del lenguaje natural.

Un documento textual es representado matemáticamente por un vector numérico. El primer acercamiento en este sentido fue la representación \textit{one-hot}, donde a cada \textit{token}\footnote{En los idiomas inglés y español, en el área de procesamiento del lenguaje natural, generalmente se utiliza el término \textit{token} para referirse a una palabra. En la presente tesis se seguirá este convenio.} se le asocia un índice y es representado por un vector con tantas componentes como \textit{tokens} tiene el vocabulario (\cite{goyal-2018-deepnlp}). De esta manera es posible hacerle corresponder a cada palabra una componente del vector, el cual tiene valor 0 en todas las componentes, excepto en la posición correspondiente a la palabra que representa, donde toma valor 1. La estrategia tradicional empleada sigue el mismo principio con la diferencia de que la componente de la palabra activa en lugar de 1, toma un valor mayor que puede ser la cantidad de veces que aparece la palabra en el documento. Otra variante muy utilizada es utilizar el resultado de la expresión conocida como $tf$\footnote{del inglés, \textit{term frequency}}$-idf$\footnote{del inglés, \textit{inverse document frequency}}. Ambas representaciones requieren vectores de gran dimensión y son insuficientes para capturar la semántica de las palabras.

Como alternativa, se incorporan otras características del lenguaje como anotaciones de las partes de la oración (POS, del inglés \textit{part-of-speech}), funciones gramaticales y análisis sintáctico. Según establece \cite{chollet-2017-deeplearningpython}, estas herramientas no son exactas y muchas veces cometen errores que afectan el rendimiento de la tarea principal. Además, al basarse solamente en elementos sintácticos de una oración tampoco se logra atrapar toda la riqueza semántica de las palabras.

Los \textit{word embeddings}, introducidos por \cite{mikolov-2013-word2vec}, son una forma diferente de representación. Un \textit{word embedding} es una forma de representación distribuida de las palabras de un vocabulario en un vector. Cada palabra es representada por un vector de pequeñas dimensiones respecto al tamaño del vocabulario. La gran novedad de esta representación es su capacidad para capturar el significado de una palabra teniendo en cuenta el contexto en que aparece, captando tanto rasgos sintácticos como semánticos que permiten establecer similitud entre las palabras.

La representación embebida de una palabra, como también se puede llamar en español, es aprendida en tareas auxiliares enfocadas en predecir la palabra que mejor se ajusta al contexto, entrenadas sobre grandes cantidades de texto. A estas tareas auxiliares se les conoce como \textit{language modeling} y a los modelos resultantes se les llama en la literatura \textit{language models} (\cite{rao-2019-nlpPython}). Word2vec (\cite{mikolov-2013-word2vec}), GloVe (\cite{pennington-2014-glove}) y FastText (\cite{mikolov-2016-fastext}) son algunos de los \textit{frameworks} que brindan técnicas para el aprendizaje de los \textit{word embeddings} y, además, facilitan el uso de \textit{embeddings} pre-entrenados. Al enfoque de utilizar \textit{embeddings} preentrenados en una tarea auxiliar en la solución de un problema diferente se le llama \textit{fine tuning} (\cite{rao-2019-nlpPython}), también es conocido en la literatura como \textit{transfer learning}.

Otro enfoque que ha tenido gran aceptación en el área del procesamiento del lenguaje es el aprendizaje profundo por el poder de aprendizaje y generalización que han demostrado estos modelos en diferentes áreas del conocimiento. Los primeros trabajos en utilizar el enfoque de aprendizaje profundo tratan el problema de selección de respuestas como un problema de clasificación binario, intentando asignar una clase a cada par (pregunta, respuesta).

Dentro del aprendizaje profundo, las Redes Neuronales Convolucionales (CNN, del inglés \textit{Convolutional Neural Networks}) y las Redes Recurrentes (RNN, del inglés \textit{Recurrent Neural Networks}) son ampliamente utilizadas para hallar representaciones vectoriales ricas semánticamente de secuencias textuales. Ambas son tipos especiales de redes neuronales, con diferencias en sus arquitecturas que las hacen más adecuadas para un dominio u otro.

Intuitivamente, las Redes Recurrentes se adaptan mejor que otras arquitecturas de redes neuronales a las tareas de procesamiento del lenguaje natural porque son capaces de aceptar como entrada secuencias de tamaño variable (característica inherente de las oraciones) y analizar cada uno de los \textit{tokens} secuencialmente. Al analizar una oración \textit{token} por \textit{token}, al final de la secuencia logran captar información de cada palabra vista y de esa manera, obtienen una representación de toda la oración. Sin embargo, en la práctica, cuando las secuencias son muy largas se ha demostrado que la representación final pierde información de las primeras palabras analizadas (\cite{rao-2019-nlpPython}). Con el fin de superar este problema surgen las redes LSTM (del inglés, \textit{Long-Short Term Memory}) las cuales tienen una arquitectura más compleja que permite atrapar dependencias a lo largo de la secuencia.

\cite{vaswani-2017-transformer} propone una nueva arquitectura de redes que recibe el nombre de \textit{Transformer} basada únicamente en mecanismos de atención. Aunque la arquitectura fue probada por primera vez en la tarea de traducción automática, alcanzando resultados superiores en calidad, paralelización y tiempo de entrenamiento, los autores afirman que la nueva arquitectura \textit{Transformer} generaliza bien a otras tareas y con datos de entrenamiento diferentes. Aprovechando la nueva arquitectura propuesta por \cite{vaswani-2017-transformer}, \cite{2018-devlin-bert} propone el modelo BERT (del inglés, \textit{Bidirectional Encoder Representations}) el cual aplica una arquitectura bidireccional basada en \textit{Tranformer} para resolver la tarea de \textit{Language Modeling} y defiende que obtienen representaciones pre-entrenadas del lenguaje mucho más ricas semánticamente que los \textit{word embeddings} vistos anteriormente.

En la siguiente sección se analizan las propuestas existentes en la literatura que siguen el enfoque del aprendizaje profundo para resolver el problema de selección de respuestas.

\subsection{Aprendizaje profundo en sistemas Q/A}

Los avances más recientes en procesamiento del lenguaje natural, casi en su mayoría, están enfocados en el empleo del aprendizaje profundo. La razón principal es que superan las técnicas tradicionales sin depender de ningún recurso externo. Además, no necesitan ningún esfuerzo de ingeniería de funciones o recursos codificados a mano más allá de algunos grandes sin etiqueta corpus en el que aprender las incrustaciones de palabras iniciales, como los \textit{word embeddings}. Este hecho no es ajeno a los sistemas Q/A. El aprendizaje profundo constituye una opción más acertada porque la recuperación de información tradicional se basa en la coincidencia, exacta o parcial, de palabras. Sin embargo, en el problema de selección de la respuesta correcta un enfoque de coincidencia no es suficiente, porque las respuestan guardan cierto parecido sintáctico entre sí, y la clave para diferenciar la correcta está en la comprensión y razonamiento de la pregunta. Aunque aún no se puede afirmar que la computadora logre razonar, los modelos de aprendizaje profundo han demostrado un cierto conocimiento del idioma y el campo específico en que sean entrenados.

El problema puede ser planteado en término generales como: 

Dada una pregunta $q_{i}$ en el dataset y un conjunto de oraciones candidatas ${c_{i1}, c_{i2}, ..., c_{im}}$, la tarea es identificar oraciones candidatas que contienen
la respuesta correcta a la pregunta. A partir de la definición, el problema se puede formular como un problema de \textit{ranking}, donde el objetivo es dar una mejor posición a las oraciones candidatas que son relevantes a la pregunta.

Desde el punto de vista del aprendizaje automático, según \cite{2018-lai-review} existen tres tipos de enfoques para tratar el problema de selección de respuestas. A continuación, se explican brevemente cada uno de ellos.

\begin{itemize}
	\item Pointwise: El problema de \textit{ranking} se transforma en un problema de clasificación binario, donde las instancias de entrenamiento tienen la forma $(q_{i}, c_{ij}, y_{ij})$ y $y_{ij}$ puede tomar valores 0 o 1 si la respuesta es correcta o no. En la etapa de inferencia, el resultado de la función $h_{0}$ toma valores en el rango $0 \leq h_{0} \leq 1$, donde cada número salida puede ser interpretado como la probabilidad de que la respuesta sea correcta o no, de manera que la respuesta correcta es entre las candidatas aquella que mayor valor de $h_0$ tenga.
	\item Pairwise: La función de \textit{ranking} se entrena explícitamente para puntuar las respuestas correctas con un mayor valor que las incorrectas. Dada una pregunta, la instancia entrenante toma dos respuestas candidatas, una correcta y la otra incorrecta, hecho que es recogido por la función de pérdida, de manera que el modelo aprende cuál de las respuestas es más relevante a la pregunta. 
	\item Listwise: A diferencia de los métodos anteriores que ignoran que la selección se hace sobre una lista de respuestas candidatas, en este enfoque una instancia consiste en la pregunta y la lista de respuestas. La función de pérdida se adapta, de igual manera, para determinar cuál de las oraciones candidatas es la más relevante a la pregunta.
\end{itemize}

Existen tres tipos de arquitecturas de red generales para calcular la relevancia de una oración candidata a una pregunta. El matiz que las diferencia es la forma de aprender la función de \textit{ranking} $h_{0}$. Según \cite{2018-lai-review}, los límites entre cada una de las arquitecturas no están siempre bien definidos.

\subsubsection{Arquitecturas siamesas}

La arquitectura siamesa en esencia consiste en que el codificador o en inglés \textit{encoder} crea por separado las representaciones vectoriales de las oraciones de entrada, en este caso, de la pregunta y la respuesta que está siendo analizada. De manera que, las oraciones no influyen en el cálculo de la representación de cada una de ellas. Luego, las oraciones codificadas se comparan a través de una medida de similutid, que puede ser el coseno como en \cite{2015-feng-siamese-cosine}, cualquier operación elemento a elemento \cite{2015-tai-element-wise} o combinaciones basadas en redes neuronales como proponen \cite{2015-manning-inference}.

Uno de los primeros modelos de aprendizaje profundo aplicado a la selección de respuestas fue el propuesto por \cite{2014-yu-answer-sentence}. En esta primera propuesta el modelo genera la representación vectorial de cada secuencia tomando la media de todos los vectores palabras de la oración, además integra características adicionales como la cantidad de palabras que se superponen en ambas secuencias. Este modelo logró un mejor rendimiento que los modelos tradicionales basados en la extracción e ingeniería de características lingüísticas.

Una arquitectura innovadora fue la presentada por \cite{2015-tan-qalstm} con el nombre de QA-LSTM, la cual utiliza una red bidireccional de tipo LSTM (del inglés, \textit{Long Short-Term Memory}), que constituye un tipo especial de red neuronal recurrente, y una capa \textit{pooling} para obtener una representación vectorial distribuida de las secuencias de entrada de manera independiente. Posteriormente, el modelo utiliza la similitud del coseno para computar la distancia entre las representaciones. 

Un ejemplo del uso de las redes convolucionales para generar una representación de las oraciones de entrada es el publicado por \cite{2015-moschitti-cnn}, donde las representaciones se obtienen tras aplicar diferentes tipos de \textit{pooling} en varios niveles de granularidad de la red. La comparación entre las secuencias se realiza utilizando varias métricas de similitud también en diferentes niveles de granularidad de la red. Posteriormente, los resultados de las métricas son utilizados como entrada a una capa \textit{fully connected} para obtener la relevancia final. 

\subsubsection{Arquitectura basada en mecanismos de atención}

En una arquitectura siamesa, las oraciones de entrada se codifican en representaciones vectoriales de longitud fija e independientes y luego, se comparan. A pesar de su sencillez conceptual, una desventaja es la ausencia de interacción explícita entre las oraciones de entrada durante el proceso de codificación. A una pregunta siempre se le asigna al mismo vector independientemente de la respuesta candidata en consideración, y viceversa. 

\cite{2015-tan-qalstm} enriquece el modelo QA-LSTM presentado con un mecanismo de atención. 
Conceptualmente, un mecanismo de atención da mayor importancia a ciertas palabras en la respuesta candidata, y los pesos se calculan de acuerdo con la pregunta. En este caso, el mecanismo de atención se aplica solo en una única dirección, pues solo se emplea para codificar la respuesta candidata en función de la pregunta. Por tra parte, \cite{2016-santos-attentive} emplea un mecanismo de atención en doble sentido, de manera que tanto la pregunta como la respuesta candidata influyen en la representación de la otra.  

\subsubsection{Arquitectura \textit{Compare-Aggregate}}

La idea de esta arquitectura es comparar unidades de la red más pequeñas que las oraciones como comúnmente se hace. Se comparan unidades como palabras o incluso letras de las oraciones de entrada y luego los resultados de la comparación se agregan mediante redes recurrentes o convolucionales para tomar una decisión definitiva, como se presenta en \cite{2018-tran-context}.

De manera general un modelo \textit{Compare-Aggregate} consta de 5 capas, como se detalla en \cite{2017-wang-bimpm}. La primera capa se encarga de la representación de las palabras (\textit{word representation layer}), su fin es representar cada palabra en un vector $d$-dimensional. El objetivo de la capa de representación del contexto (\textit{context representation layer}) es obtener una nueva representación para cada posición en las oraciones de entrada que capture alguna información contextual además de la posición de la palabra. La capa de comparación (\textit{matching layer}) compara cada representación contextual de una oración con todas las representaciones contextuales de la otra oración. La salida de esta capa son dos secuencias de vectores coincidentes, donde cada vector coincidente corresponde al resultado de la comparación de una posición de una oración con todas las posiciones de la otra oración. Posteriormente, la capa de agregación (\textit{aggregation layer}) se encarga de agregar los resultados de la comparación de la capa anterior. Por último, la capa de predicción (\textit{prediction layer}) encargada de obtener el valor final que indica la relevancia de la respuesta para la pregunta dada.

Las arquitecturas \textit{Compare-Aggregate} pueden capturar más características interactivas entre las oraciones de entrada que las arquitecturas siamesas y las arquitecturas \textit{Attentive}, por lo tanto, por lo general, tienen un mejor rendimiento como se ha demostrado cuando se evalúan en conjuntos de datos públicos como TrecQA.

Aunque el uso del modelo de lenguaje pre-entrenado BERT ha sido ampliamente utilizado en varias tareas de procesamiento del lenguaje natural, tras esta revisión bibliográfica, se ha podido constatar que su aplicación no abunda cuando se trata de sistemas Q/A de selección de respuestas, por lo que se considera interesante incluirlo en algunos de los modelos a desarrollar en el presente trabajo.

Tras analizar las principales propuestas en la tarea de selección de respuestas se puede concluir que es un tema muy actual y una tarea todavía no resuelta. Las aplicaciones son indiscutibles y sin duda alguna, se hace necesario transferir los resultados obtenidos en el idioma inglés al idioma español.


\section{Consideraciones generales}

La selección de respuestas es un problema importante en el procesamiento del lenguaje natural y muchos métodos de aprendizaje profundo han sido propuestos para la tarea. En este capítulo, se ha brindado una revisión integral y sistemática de varios métodos de aprendizaje profundo para la selección de respuestas en dos dimensiones: enfoques de aprendizaje (\textit{pointwise}, \textit{pairwise} y \textit{listwise}) y arquitecturas de redes neuronales (arquitectura siamesa, arquitectura basada en atención, y arquitectura \textit{Compare-Aggregate}).

El escaso avance en el idioma español es perceptible. La mayoría de los esfuerzos se han concentrado en sistemas Q/A para la lengua inglesa. Aunque se carece de antecedentes directos que puedan ser utilizados como punto de partida, los avances en el idioma inglés pueden ser aprovechados teniendo en cuenta las diferencias de complejidad gramatical de ambos idiomas.

A pesar de que el trabajo está orientado al idioma español, la presente propuesta puede ser generalizada a cualquier idioma pues no se hace uso de recursos propios de la gramática española.