{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'pharmacology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  51.6431 valid loss 0.806 and accuracy 0.6864\n",
      "Epoch 1 train loss  51.3075 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 2 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 3 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 4 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 5 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 6 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 7 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 8 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 9 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 10 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 11 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 12 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 13 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 14 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 15 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 16 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 17 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 18 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 19 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 20 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 21 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 22 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 23 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 24 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 25 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 26 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 27 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 28 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n",
      "Epoch 29 train loss  50.7432 valid loss 0.921 and accuracy 0.7467\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2089]), points: -37\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2407]), points: -17\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6605 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7199 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7003 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6956 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6922 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6884 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6815 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6690 valid loss 0.027 and accuracy 0.2533\n",
      "Epoch 8 train loss  0.6297 valid loss 0.027 and accuracy 0.2868\n",
      "Epoch 9 train loss  0.5625 valid loss 0.029 and accuracy 0.3393\n",
      "Epoch 10 train loss  0.5025 valid loss 0.025 and accuracy 0.5424\n",
      "Epoch 11 train loss  0.4381 valid loss 0.025 and accuracy 0.5714\n",
      "Epoch 12 train loss  0.3862 valid loss 0.025 and accuracy 0.5904\n",
      "Epoch 13 train loss  0.3464 valid loss 0.025 and accuracy 0.6138\n",
      "Epoch 14 train loss  0.3158 valid loss 0.026 and accuracy 0.6261\n",
      "Epoch 15 train loss  0.2922 valid loss 0.026 and accuracy 0.6105\n",
      "Epoch 16 train loss  0.2718 valid loss 0.028 and accuracy 0.6194\n",
      "Epoch 17 train loss  0.2511 valid loss 0.029 and accuracy 0.6295\n",
      "Epoch 18 train loss  0.2420 valid loss 0.029 and accuracy 0.6295\n",
      "Epoch 19 train loss  0.2396 valid loss 0.029 and accuracy 0.6138\n",
      "Epoch 20 train loss  0.2174 valid loss 0.030 and accuracy 0.6138\n",
      "Epoch 21 train loss  0.2102 valid loss 0.035 and accuracy 0.6417\n",
      "Epoch 22 train loss  0.2028 valid loss 0.035 and accuracy 0.6362\n",
      "Epoch 23 train loss  0.2059 valid loss 0.036 and accuracy 0.6362\n",
      "Epoch 24 train loss  0.1879 valid loss 0.036 and accuracy 0.6350\n",
      "Epoch 25 train loss  0.1808 valid loss 0.038 and accuracy 0.6395\n",
      "Epoch 26 train loss  0.1818 valid loss 0.038 and accuracy 0.6317\n",
      "Epoch 27 train loss  0.1770 valid loss 0.040 and accuracy 0.6406\n",
      "Epoch 28 train loss  0.1736 valid loss 0.040 and accuracy 0.6384\n",
      "Epoch 29 train loss  0.1629 valid loss 0.040 and accuracy 0.6239\n",
      "Epoch 30 train loss  0.1676 valid loss 0.042 and accuracy 0.6406\n",
      "Epoch 31 train loss  0.1601 valid loss 0.041 and accuracy 0.6406\n",
      "Epoch 32 train loss  0.1610 valid loss 0.043 and accuracy 0.6350\n",
      "Epoch 33 train loss  0.1602 valid loss 0.043 and accuracy 0.6362\n",
      "Epoch 34 train loss  0.1527 valid loss 0.043 and accuracy 0.6429\n",
      "Epoch 35 train loss  0.1511 valid loss 0.045 and accuracy 0.6350\n",
      "Epoch 36 train loss  0.1465 valid loss 0.048 and accuracy 0.6417\n",
      "Epoch 37 train loss  0.1454 valid loss 0.046 and accuracy 0.6473\n",
      "Epoch 38 train loss  0.1476 valid loss 0.045 and accuracy 0.6473\n",
      "Epoch 39 train loss  0.1396 valid loss 0.048 and accuracy 0.6429\n",
      "Epoch 40 train loss  0.1431 valid loss 0.052 and accuracy 0.6518\n",
      "Epoch 41 train loss  0.1317 valid loss 0.055 and accuracy 0.6540\n",
      "Epoch 42 train loss  0.1377 valid loss 0.051 and accuracy 0.6350\n",
      "Epoch 43 train loss  0.1359 valid loss 0.055 and accuracy 0.6440\n",
      "Epoch 44 train loss  0.1373 valid loss 0.054 and accuracy 0.6384\n",
      "Epoch 45 train loss  0.1349 valid loss 0.055 and accuracy 0.6429\n",
      "Epoch 46 train loss  0.1275 valid loss 0.056 and accuracy 0.6473\n",
      "Epoch 47 train loss  0.1273 valid loss 0.054 and accuracy 0.6518\n",
      "Epoch 48 train loss  0.1348 valid loss 0.055 and accuracy 0.6484\n",
      "Epoch 49 train loss  0.1324 valid loss 0.055 and accuracy 0.6462\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.1911]), points: -53\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2319]), points: -33\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3722 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  49.2457 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  60.8508 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 19 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 20 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 21 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 22 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 23 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  50.7543 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  50.7363 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  48.7161 valid loss 0.786 and accuracy 0.7500\n",
      "Epoch 41 train loss  43.0828 valid loss 0.731 and accuracy 0.7500\n",
      "Epoch 42 train loss  39.5696 valid loss 0.680 and accuracy 0.7500\n",
      "Epoch 43 train loss  36.7107 valid loss 0.633 and accuracy 0.7500\n",
      "Epoch 44 train loss  34.0950 valid loss 0.590 and accuracy 0.7500\n",
      "Epoch 45 train loss  31.6937 valid loss 0.550 and accuracy 0.7500\n",
      "Epoch 46 train loss  29.4857 valid loss 0.513 and accuracy 0.7500\n",
      "Epoch 47 train loss  27.4431 valid loss 0.477 and accuracy 0.7500\n",
      "Epoch 48 train loss  25.5393 valid loss 0.444 and accuracy 0.7500\n",
      "Epoch 49 train loss  23.7126 valid loss 0.410 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2489]), points: -1\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2101]), points: -73\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5021 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4939 valid loss 0.003 and accuracy 0.7507\n",
      "Epoch 2 train loss  0.4668 valid loss 0.003 and accuracy 0.7449\n",
      "Epoch 3 train loss  0.4160 valid loss 0.003 and accuracy 0.6735\n",
      "Epoch 4 train loss  0.3501 valid loss 0.003 and accuracy 0.6267\n",
      "Epoch 5 train loss  0.3087 valid loss 0.004 and accuracy 0.6535\n",
      "Epoch 6 train loss  0.2404 valid loss 0.004 and accuracy 0.6816\n",
      "Epoch 7 train loss  0.1908 valid loss 0.006 and accuracy 0.7033\n",
      "Epoch 8 train loss  0.1823 valid loss 0.005 and accuracy 0.6526\n",
      "Epoch 9 train loss  0.1488 valid loss 0.005 and accuracy 0.6449\n",
      "Epoch 10 train loss  0.1085 valid loss 0.005 and accuracy 0.6575\n",
      "Epoch 11 train loss  0.1040 valid loss 0.008 and accuracy 0.6691\n",
      "Epoch 12 train loss  0.0998 valid loss 0.007 and accuracy 0.6754\n",
      "Epoch 13 train loss  0.0904 valid loss 0.006 and accuracy 0.6660\n",
      "Epoch 14 train loss  0.0773 valid loss 0.010 and accuracy 0.6875\n",
      "Epoch 15 train loss  0.0661 valid loss 0.007 and accuracy 0.6763\n",
      "Epoch 16 train loss  0.0572 valid loss 0.009 and accuracy 0.6892\n",
      "Epoch 17 train loss  0.0645 valid loss 0.008 and accuracy 0.6506\n",
      "Epoch 18 train loss  0.0513 valid loss 0.008 and accuracy 0.6809\n",
      "Epoch 19 train loss  0.0539 valid loss 0.009 and accuracy 0.6838\n",
      "Epoch 20 train loss  0.0581 valid loss 0.008 and accuracy 0.6801\n",
      "Epoch 21 train loss  0.0592 valid loss 0.008 and accuracy 0.6704\n",
      "Epoch 22 train loss  0.0490 valid loss 0.009 and accuracy 0.6937\n",
      "Epoch 23 train loss  0.0464 valid loss 0.010 and accuracy 0.6735\n",
      "Epoch 24 train loss  0.0417 valid loss 0.007 and accuracy 0.6904\n",
      "Epoch 25 train loss  0.0366 valid loss 0.008 and accuracy 0.6619\n",
      "Epoch 26 train loss  0.0386 valid loss 0.006 and accuracy 0.6838\n",
      "Epoch 27 train loss  0.0400 valid loss 0.008 and accuracy 0.6779\n",
      "Epoch 28 train loss  0.0390 valid loss 0.009 and accuracy 0.7046\n",
      "Epoch 29 train loss  0.0369 valid loss 0.007 and accuracy 0.6592\n",
      "Epoch 30 train loss  0.0364 valid loss 0.007 and accuracy 0.6825\n",
      "Epoch 31 train loss  0.0348 valid loss 0.007 and accuracy 0.6551\n",
      "Epoch 32 train loss  0.0351 valid loss 0.008 and accuracy 0.6792\n",
      "Epoch 33 train loss  0.0311 valid loss 0.008 and accuracy 0.6586\n",
      "Epoch 34 train loss  0.0347 valid loss 0.006 and accuracy 0.6640\n",
      "Epoch 35 train loss  0.0284 valid loss 0.007 and accuracy 0.6846\n",
      "Epoch 36 train loss  0.0319 valid loss 0.007 and accuracy 0.6730\n",
      "Epoch 37 train loss  0.0425 valid loss 0.008 and accuracy 0.6983\n",
      "Epoch 38 train loss  0.0344 valid loss 0.007 and accuracy 0.6833\n",
      "Epoch 39 train loss  0.0241 valid loss 0.010 and accuracy 0.7026\n",
      "Epoch 40 train loss  0.0308 valid loss 0.006 and accuracy 0.6972\n",
      "Epoch 41 train loss  0.0287 valid loss 0.010 and accuracy 0.6419\n",
      "Epoch 42 train loss  0.0235 valid loss 0.008 and accuracy 0.6960\n",
      "Epoch 43 train loss  0.0242 valid loss 0.008 and accuracy 0.6748\n",
      "Epoch 44 train loss  0.0225 valid loss 0.012 and accuracy 0.7057\n",
      "Epoch 45 train loss  0.0180 valid loss 0.010 and accuracy 0.6816\n",
      "Epoch 46 train loss  0.0189 valid loss 0.010 and accuracy 0.6824\n",
      "Epoch 47 train loss  0.0200 valid loss 0.010 and accuracy 0.6937\n",
      "Epoch 48 train loss  0.0266 valid loss 0.012 and accuracy 0.7055\n",
      "Epoch 49 train loss  0.0301 valid loss 0.009 and accuracy 0.6686\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2133]), points: -33\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2473]), points: -5\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5017 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4944 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.4704 valid loss 0.004 and accuracy 0.7305\n",
      "Epoch 3 train loss  0.4199 valid loss 0.003 and accuracy 0.7066\n",
      "Epoch 4 train loss  0.3674 valid loss 0.004 and accuracy 0.6358\n",
      "Epoch 5 train loss  0.3099 valid loss 0.005 and accuracy 0.6781\n",
      "Epoch 6 train loss  0.2714 valid loss 0.004 and accuracy 0.6612\n",
      "Epoch 7 train loss  0.2186 valid loss 0.005 and accuracy 0.6305\n",
      "Epoch 8 train loss  0.1704 valid loss 0.006 and accuracy 0.6950\n",
      "Epoch 9 train loss  0.1544 valid loss 0.008 and accuracy 0.6840\n",
      "Epoch 10 train loss  0.1417 valid loss 0.006 and accuracy 0.6425\n",
      "Epoch 11 train loss  0.1163 valid loss 0.008 and accuracy 0.6254\n",
      "Epoch 12 train loss  0.0957 valid loss 0.005 and accuracy 0.6733\n",
      "Epoch 13 train loss  0.0822 valid loss 0.006 and accuracy 0.6588\n",
      "Epoch 14 train loss  0.0717 valid loss 0.007 and accuracy 0.6572\n",
      "Epoch 15 train loss  0.0704 valid loss 0.007 and accuracy 0.6949\n",
      "Epoch 16 train loss  0.0654 valid loss 0.005 and accuracy 0.6908\n",
      "Epoch 17 train loss  0.0657 valid loss 0.007 and accuracy 0.6886\n",
      "Epoch 18 train loss  0.0548 valid loss 0.006 and accuracy 0.6996\n",
      "Epoch 19 train loss  0.0504 valid loss 0.007 and accuracy 0.6901\n",
      "Epoch 20 train loss  0.0548 valid loss 0.006 and accuracy 0.6750\n",
      "Epoch 21 train loss  0.0495 valid loss 0.007 and accuracy 0.6903\n",
      "Epoch 22 train loss  0.0483 valid loss 0.008 and accuracy 0.6857\n",
      "Epoch 23 train loss  0.0459 valid loss 0.007 and accuracy 0.6857\n",
      "Epoch 24 train loss  0.0410 valid loss 0.008 and accuracy 0.6980\n",
      "Epoch 25 train loss  0.0386 valid loss 0.009 and accuracy 0.6743\n",
      "Epoch 26 train loss  0.0424 valid loss 0.009 and accuracy 0.6583\n",
      "Epoch 27 train loss  0.0420 valid loss 0.006 and accuracy 0.7156\n",
      "Epoch 28 train loss  0.0372 valid loss 0.009 and accuracy 0.6961\n",
      "Epoch 29 train loss  0.0325 valid loss 0.006 and accuracy 0.6844\n",
      "Epoch 30 train loss  0.0409 valid loss 0.009 and accuracy 0.6842\n",
      "Epoch 31 train loss  0.0398 valid loss 0.008 and accuracy 0.6958\n",
      "Epoch 32 train loss  0.0309 valid loss 0.007 and accuracy 0.6901\n",
      "Epoch 33 train loss  0.0271 valid loss 0.012 and accuracy 0.7279\n",
      "Epoch 34 train loss  0.0360 valid loss 0.005 and accuracy 0.6932\n",
      "Epoch 35 train loss  0.0353 valid loss 0.009 and accuracy 0.6967\n",
      "Epoch 36 train loss  0.0298 valid loss 0.007 and accuracy 0.6928\n",
      "Epoch 37 train loss  0.0305 valid loss 0.007 and accuracy 0.7015\n",
      "Epoch 38 train loss  0.0274 valid loss 0.008 and accuracy 0.6770\n",
      "Epoch 39 train loss  0.0256 valid loss 0.008 and accuracy 0.7103\n",
      "Epoch 40 train loss  0.0264 valid loss 0.008 and accuracy 0.6941\n",
      "Epoch 41 train loss  0.0262 valid loss 0.006 and accuracy 0.6877\n",
      "Epoch 42 train loss  0.0227 valid loss 0.006 and accuracy 0.6934\n",
      "Epoch 43 train loss  0.0255 valid loss 0.006 and accuracy 0.6704\n",
      "Epoch 44 train loss  0.0278 valid loss 0.007 and accuracy 0.7020\n",
      "Epoch 45 train loss  0.0415 valid loss 0.007 and accuracy 0.6654\n",
      "Epoch 46 train loss  0.0308 valid loss 0.007 and accuracy 0.6906\n",
      "Epoch 47 train loss  0.0191 valid loss 0.007 and accuracy 0.6989\n",
      "Epoch 48 train loss  0.0146 valid loss 0.008 and accuracy 0.7094\n",
      "Epoch 49 train loss  0.0152 valid loss 0.007 and accuracy 0.6778\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2622]), points: 11\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2495]), points: -1\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-waters",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "every-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models/logistic_regressor_pharmacology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV\n",
      "Accuracy media 0.20888889\n",
      "Puntos media -37.0\n",
      "[tensor(0.2089)]\n",
      "[-37]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24068221\n",
      "Puntos media -8.5\n",
      "[tensor(0.2325), tensor(0.2489)]\n",
      "[-16, -1]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.19111112\n",
      "Puntos media -53.0\n",
      "[tensor(0.1911)]\n",
      "[-53]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23200604\n",
      "Puntos media -16.5\n",
      "[tensor(0.2588), tensor(0.2052)]\n",
      "[8, -41]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.2488889\n",
      "Puntos media -1.0\n",
      "[tensor(0.2489)]\n",
      "[-1]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.21006665\n",
      "Puntos media -36.5\n",
      "[tensor(0.2105), tensor(0.2096)]\n",
      "[-36, -37]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "centered-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.21333334\n",
      "Puntos media -33.0\n",
      "[tensor(0.2133)]\n",
      "[-33]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24725159\n",
      "Puntos media -2.5\n",
      "[tensor(0.2412), tensor(0.2533)]\n",
      "[-8, 3]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.26222223\n",
      "Puntos media 11.0\n",
      "[tensor(0.2622)]\n",
      "[11]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24952118\n",
      "Puntos media -0.5\n",
      "[tensor(0.2807), tensor(0.2183)]\n",
      "[28, -29]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-archives",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
