{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'pharmacology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  49.1814 valid loss 1.036 and accuracy 0.7478\n",
      "Epoch 1 train loss  49.2098 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 2 train loss  49.1945 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 3 train loss  49.1931 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 4 train loss  49.1918 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 5 train loss  49.1913 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 6 train loss  49.1916 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 7 train loss  49.1917 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 8 train loss  49.1918 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 9 train loss  49.1924 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 10 train loss  49.1923 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 11 train loss  49.1929 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 12 train loss  49.1926 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 13 train loss  49.1935 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 14 train loss  49.1930 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 15 train loss  49.1941 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 16 train loss  49.1935 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 17 train loss  49.1946 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 18 train loss  49.1941 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 19 train loss  49.1951 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 20 train loss  49.1958 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 21 train loss  49.1897 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 22 train loss  49.2050 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 23 train loss  49.2025 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 24 train loss  49.2023 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 25 train loss  49.2023 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 26 train loss  49.2036 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 27 train loss  49.2029 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 28 train loss  49.2035 valid loss 1.036 and accuracy 0.7500\n",
      "Epoch 29 train loss  49.2035 valid loss 1.036 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2578]), points: 7\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2757]), points: 47\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  1.7767 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 1 train loss  1.7754 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 2 train loss  1.7744 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 3 train loss  1.7736 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 4 train loss  1.7739 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 5 train loss  1.7735 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 6 train loss  1.7725 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 7 train loss  1.7716 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 8 train loss  1.7697 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 9 train loss  1.7671 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 10 train loss  1.7636 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 11 train loss  1.7586 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 12 train loss  1.7507 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 13 train loss  1.7404 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 14 train loss  1.7282 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 15 train loss  1.7200 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 16 train loss  1.7074 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 17 train loss  1.6918 valid loss 0.036 and accuracy 0.7500\n",
      "Epoch 18 train loss  1.6822 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 19 train loss  1.6720 valid loss 0.039 and accuracy 0.7500\n",
      "Epoch 20 train loss  1.6659 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 21 train loss  1.6593 valid loss 0.041 and accuracy 0.7500\n",
      "Epoch 22 train loss  1.6558 valid loss 0.043 and accuracy 0.7500\n",
      "Epoch 23 train loss  1.6565 valid loss 0.043 and accuracy 0.7500\n",
      "Epoch 24 train loss  1.6493 valid loss 0.043 and accuracy 0.7500\n",
      "Epoch 25 train loss  1.6439 valid loss 0.047 and accuracy 0.7500\n",
      "Epoch 26 train loss  1.6425 valid loss 0.047 and accuracy 0.7500\n",
      "Epoch 27 train loss  1.6410 valid loss 0.045 and accuracy 0.7500\n",
      "Epoch 28 train loss  1.6408 valid loss 0.045 and accuracy 0.7500\n",
      "Epoch 29 train loss  1.6403 valid loss 0.046 and accuracy 0.7500\n",
      "Epoch 30 train loss  1.6385 valid loss 0.047 and accuracy 0.7500\n",
      "Epoch 31 train loss  1.6358 valid loss 0.049 and accuracy 0.7500\n",
      "Epoch 32 train loss  1.6340 valid loss 0.048 and accuracy 0.7500\n",
      "Epoch 33 train loss  1.6316 valid loss 0.046 and accuracy 0.7500\n",
      "Epoch 34 train loss  1.6312 valid loss 0.053 and accuracy 0.7500\n",
      "Epoch 35 train loss  1.6320 valid loss 0.050 and accuracy 0.7500\n",
      "Epoch 36 train loss  1.6300 valid loss 0.051 and accuracy 0.7500\n",
      "Epoch 37 train loss  1.6283 valid loss 0.050 and accuracy 0.7500\n",
      "Epoch 38 train loss  1.6316 valid loss 0.053 and accuracy 0.7500\n",
      "Epoch 39 train loss  1.6287 valid loss 0.053 and accuracy 0.7500\n",
      "Epoch 40 train loss  1.6268 valid loss 0.053 and accuracy 0.7500\n",
      "Epoch 41 train loss  1.6261 valid loss 0.053 and accuracy 0.7500\n",
      "Epoch 42 train loss  1.6252 valid loss 0.054 and accuracy 0.7500\n",
      "Epoch 43 train loss  1.6247 valid loss 0.051 and accuracy 0.7500\n",
      "Epoch 44 train loss  1.6257 valid loss 0.055 and accuracy 0.7500\n",
      "Epoch 45 train loss  1.6242 valid loss 0.055 and accuracy 0.7500\n",
      "Epoch 46 train loss  1.6230 valid loss 0.055 and accuracy 0.7500\n",
      "Epoch 47 train loss  1.6248 valid loss 0.054 and accuracy 0.7500\n",
      "Epoch 48 train loss  1.6238 valid loss 0.054 and accuracy 0.7500\n",
      "Epoch 49 train loss  1.6211 valid loss 0.057 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2133]), points: -33\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2429]), points: -13\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  1.7802 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 1 train loss  1.7764 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 2 train loss  1.7760 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 3 train loss  1.7753 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 4 train loss  1.7751 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 5 train loss  1.7752 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 6 train loss  1.7750 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 7 train loss  1.7748 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 8 train loss  1.7747 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 9 train loss  1.7741 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 10 train loss  1.7734 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 11 train loss  1.7725 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 12 train loss  1.7724 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 13 train loss  1.7711 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 14 train loss  1.7698 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 15 train loss  1.7676 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 16 train loss  1.7693 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 17 train loss  1.7678 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 18 train loss  1.7669 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 19 train loss  1.7628 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 20 train loss  1.7626 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 21 train loss  1.7608 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 22 train loss  1.7585 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 23 train loss  1.7571 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 24 train loss  1.7538 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 25 train loss  1.7537 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 26 train loss  1.7543 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 27 train loss  1.7561 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 28 train loss  1.7519 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 29 train loss  1.7516 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 30 train loss  1.7506 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 31 train loss  1.7466 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 32 train loss  1.7446 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 33 train loss  1.7433 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 34 train loss  1.7415 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 35 train loss  1.7377 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 36 train loss  1.7399 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 37 train loss  1.7362 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 38 train loss  1.7387 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 39 train loss  1.7338 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 40 train loss  1.7336 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 41 train loss  1.7313 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 42 train loss  1.7274 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 43 train loss  1.7301 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 44 train loss  1.7279 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 45 train loss  1.7244 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 46 train loss  1.7263 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 47 train loss  1.7221 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 48 train loss  1.7201 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 49 train loss  1.7176 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 50 train loss  1.7164 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 51 train loss  1.7185 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 52 train loss  1.7174 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 53 train loss  1.7145 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 54 train loss  1.7134 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 55 train loss  1.7102 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 56 train loss  1.7097 valid loss 0.033 and accuracy 0.7500\n",
      "Epoch 57 train loss  1.7084 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 58 train loss  1.7150 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 59 train loss  1.7081 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 60 train loss  1.7074 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 61 train loss  1.7097 valid loss 0.034 and accuracy 0.7500\n",
      "Epoch 62 train loss  1.7029 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 63 train loss  1.7055 valid loss 0.036 and accuracy 0.7500\n",
      "Epoch 64 train loss  1.7049 valid loss 0.036 and accuracy 0.7500\n",
      "Epoch 65 train loss  1.7006 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 66 train loss  1.6997 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 67 train loss  1.7016 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 68 train loss  1.7022 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 69 train loss  1.6997 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 70 train loss  1.7003 valid loss 0.036 and accuracy 0.7500\n",
      "Epoch 71 train loss  1.6978 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 72 train loss  1.7014 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 73 train loss  1.6961 valid loss 0.038 and accuracy 0.7500\n",
      "Epoch 74 train loss  1.6969 valid loss 0.036 and accuracy 0.7500\n",
      "Epoch 75 train loss  1.6934 valid loss 0.042 and accuracy 0.7500\n",
      "Epoch 76 train loss  1.6945 valid loss 0.041 and accuracy 0.7500\n",
      "Epoch 77 train loss  1.6946 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 78 train loss  1.6957 valid loss 0.039 and accuracy 0.7500\n",
      "Epoch 79 train loss  1.6957 valid loss 0.039 and accuracy 0.7500\n",
      "Epoch 80 train loss  1.6892 valid loss 0.038 and accuracy 0.7500\n",
      "Epoch 81 train loss  1.6950 valid loss 0.036 and accuracy 0.7500\n",
      "Epoch 82 train loss  1.6911 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 83 train loss  1.6939 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 84 train loss  1.6966 valid loss 0.038 and accuracy 0.7500\n",
      "Epoch 85 train loss  1.6984 valid loss 0.038 and accuracy 0.7500\n",
      "Epoch 86 train loss  1.6945 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 87 train loss  1.6951 valid loss 0.041 and accuracy 0.7500\n",
      "Epoch 88 train loss  1.6882 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 89 train loss  1.6974 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 90 train loss  1.6877 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 91 train loss  1.6910 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 92 train loss  1.6908 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 93 train loss  1.6907 valid loss 0.035 and accuracy 0.7500\n",
      "Epoch 94 train loss  1.6852 valid loss 0.038 and accuracy 0.7500\n",
      "Epoch 95 train loss  1.6878 valid loss 0.038 and accuracy 0.7500\n",
      "Epoch 96 train loss  1.6858 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 97 train loss  1.6864 valid loss 0.037 and accuracy 0.7500\n",
      "Epoch 98 train loss  1.6832 valid loss 0.040 and accuracy 0.7500\n",
      "Epoch 99 train loss  1.6824 valid loss 0.035 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.1911]), points: -53\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2735]), points: 43\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-83c2c9d13cb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n\u001b[0m\u001b[0;32m      2\u001b[0m                pretrained_embeddings=embedding_matrix)\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_qa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\mds\\TFM\\head-qa-afi\\code\\ir_models.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_size, hidden_size, x_size, n_classes, embedding_size, padding_idx, pretrained_embeddings)\u001b[0m\n\u001b[0;32m     12\u001b[0m     def __init__(self, vocab_size, hidden_size, x_size, n_classes, embedding_size=300,\n\u001b[0;32m     13\u001b[0m                  padding_idx=0, pretrained_embeddings=None): \n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM_QA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-royal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
