{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r9eDs7ke1zta",
   "metadata": {
    "id": "r9eDs7ke1zta"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NvTFFxwY5gcf",
   "metadata": {
    "id": "NvTFFxwY5gcf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/head-qa-afi/code')\n",
    "sys.path.insert(0,'/content/head-qa-afi/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83CL8XlF8ofG",
   "metadata": {
    "id": "83CL8XlF8ofG"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/claudiaqw/head-qa-afi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hkAA-rKA2y50",
   "metadata": {
    "id": "hkAA-rKA2y50"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!python -m spacy download es_core_news_sm\n",
    "!pip install pickle5\n",
    "# !python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rWIy6aIz14ET",
   "metadata": {
    "id": "rWIy6aIz14ET"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "import spacy\n",
    "import pickle5 as pickle\n",
    "import collections\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from training import flat_accuracy, format_time, train_model, valid_model\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glySPyzQ25PF",
   "metadata": {
    "id": "glySPyzQ25PF"
   },
   "outputs": [],
   "source": [
    "BASE_BERT = 'dccuchile/bert-base-spanish-wwm-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mvG7EF7E26tl",
   "metadata": {
    "id": "mvG7EF7E26tl"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppO775Fz7cyY",
   "metadata": {
    "id": "ppO775Fz7cyY"
   },
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def load_dataset_from_pickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "def filter_by_category(dataset, category):\n",
    "    filtered_dataset = []\n",
    "    for instance in dataset:\n",
    "        categ = instance['category']\n",
    "        if categ == category:\n",
    "            filtered_dataset.append(instance)\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8XS5I6FP5zjG",
   "metadata": {
    "id": "8XS5I6FP5zjG"
   },
   "outputs": [],
   "source": [
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir\n",
    "\n",
    "from ir_models import BERT_QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G8hvsuZq7Mo-",
   "metadata": {
    "id": "G8hvsuZq7Mo-"
   },
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('/content/head-qa-afi/data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('/content/head-qa-afi/data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('/content/head-qa-afi/data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('/content/head-qa-afi/data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVkPdQ5e29wF",
   "metadata": {
    "id": "rVkPdQ5e29wF"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BASE_BERT, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HIK0RVR63MbH",
   "metadata": {
    "id": "HIK0RVR63MbH"
   },
   "outputs": [],
   "source": [
    "train_inputs_0, train_masks_0, train_inputs_1, train_masks_1, train_labels = encoder_bert_ir(oversampled_training, tokenizer)\n",
    "valid_inputs_0, valid_masks_0, valid_inputs_1, valid_masks_1, valid_labels = encoder_bert_ir(validation_instances, tokenizer)\n",
    "test_inputs_0, test_masks_0, test_inputs_1, test_masks_1, test_labels = encoder_bert_ir(testing_instances, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T5610IBa4oIn",
   "metadata": {
    "id": "T5610IBa4oIn"
   },
   "outputs": [],
   "source": [
    "train_inputs_0 = torch.tensor(train_inputs_0)\n",
    "valid_inputs_0 = torch.tensor(valid_inputs_0)\n",
    "test_inputs_0 = torch.tensor(test_inputs_0)\n",
    "\n",
    "train_masks_0 = torch.tensor(train_masks_0)\n",
    "valid_masks_0 = torch.tensor(valid_masks_0)\n",
    "test_masks_0 = torch.tensor(test_masks_0)\n",
    "\n",
    "train_inputs_1 = torch.tensor(train_inputs_1)\n",
    "valid_inputs_1 = torch.tensor(valid_inputs_1)\n",
    "test_inputs_1 = torch.tensor(test_inputs_1)\n",
    "\n",
    "train_masks_1 = torch.tensor(train_masks_1)\n",
    "valid_masks_1 = torch.tensor(valid_masks_1)\n",
    "test_masks_1 = torch.tensor(test_masks_1)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zNzmxpgQ9bgz",
   "metadata": {
    "id": "zNzmxpgQ9bgz"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs_0, train_masks_0, train_inputs_1, train_masks_1, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "valid_data = TensorDataset(valid_inputs_0, valid_masks_0, valid_inputs_1, valid_masks_1, valid_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs_0, test_masks_0, test_inputs_1, test_masks_1, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zvnAOHH19f0r",
   "metadata": {
    "id": "zvnAOHH19f0r"
   },
   "outputs": [],
   "source": [
    "model = BERT_QA(pretrained_model=BASE_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eWwJ7QVs9y2d",
   "metadata": {
    "id": "eWwJ7QVs9y2d"
   },
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59DJQ5hC94H1",
   "metadata": {
    "id": "59DJQ5hC94H1"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqHh8Phk-dv3",
   "metadata": {
    "id": "lqHh8Phk-dv3"
   },
   "outputs": [],
   "source": [
    "epochs_results = train_model(model, train_dataloader, valid_dataloader, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "--FYVbPm-k95",
   "metadata": {
    "id": "--FYVbPm-k95"
   },
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, validation, encoder_bert, evaluator_bert)\n",
    "print(f'DEV')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "\n",
    "acc, points = evaluate(model, testing, encoder_bert, evaluator_bert)\n",
    "print(f'TEST')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "bert_qa_category.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
