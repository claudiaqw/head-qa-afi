{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vectorizer, HeadQA, HeadQA_IR, clean_words, parse_dataset, parse_ir_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import evaluate, evaluate_better, train_ir, validate_ir, evaluator_bert_ir, encoder_bert_ir, encoder_bert_ir_instance\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "from unsupervised_models import BERTSimilarity\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_BERT = 'dccuchile/bert-base-spanish-wwm-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset_ir(training)\n",
    "# validation_instances = parse_dataset_ir(validation)\n",
    "# testing_instances = parse_dataset_ir(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BASE_BERT, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Forma fibras extracelulares con gran resistencia a la tensión:',\n",
       " 'answer': 'Fibronectina.',\n",
       " 'tok_qtext': ['Forma',\n",
       "  'fibras',\n",
       "  'extracelulares',\n",
       "  'con',\n",
       "  'gran',\n",
       "  'resistencia',\n",
       "  'a',\n",
       "  'la',\n",
       "  'tensión',\n",
       "  ':'],\n",
       " 'tok_atext': ['Fibronectina', '.'],\n",
       " 'label': 0,\n",
       " 'category': 'biology'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inputs_0, valid_masks_0, valid_inputs_1, valid_masks_1, valid_labels = encoder_bert_ir(validation_instances, tokenizer)\n",
    "test_inputs_0, test_masks_0, test_inputs_1, test_masks_1, test_labels = encoder_bert_ir(testing_instances, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inputs_0 = torch.tensor(valid_inputs_0)\n",
    "test_inputs_0 = torch.tensor(test_inputs_0)\n",
    "\n",
    "valid_masks_0 = torch.tensor(valid_masks_0)\n",
    "test_masks_0 = torch.tensor(test_masks_0)\n",
    "\n",
    "valid_inputs_1 = torch.tensor(valid_inputs_1)\n",
    "test_inputs_1 = torch.tensor(test_inputs_1)\n",
    "\n",
    "valid_masks_1 = torch.tensor(valid_masks_1)\n",
    "test_masks_1 = torch.tensor(test_masks_1)\n",
    "\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "valid_data = TensorDataset(valid_inputs_0, valid_masks_0, valid_inputs_1, valid_masks_1, valid_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs_0, test_masks_0, test_inputs_1, test_masks_1, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BERTSimilarity(pretrained_model=BASE_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 199 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (31002, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.11.output.LayerNorm.weight                 (768,)\n",
      "bert.encoder.layer.11.output.LayerNorm.bias                   (768,)\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2189), -170)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, encoder_bert_ir_instance, evaluator_bert_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2360), -154)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, encoder_bert_ir_instance, evaluator_bert_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, encoder_bert_ir_instance, evaluator_bert_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21881276,\n",
       " -28.333333333333332,\n",
       " [tensor(0.2257),\n",
       "  tensor(0.2522),\n",
       "  tensor(0.2133),\n",
       "  tensor(0.2165),\n",
       "  tensor(0.1991),\n",
       "  tensor(0.2061)],\n",
       " [-22, 2, -33, -31, -46, -40])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'points_lis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-5a68850578c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_better\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_bert_ir_instance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator_bert_ir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints_lis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'points_lis' is not defined"
     ]
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, encoder_bert_ir_instance, evaluator_bert_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23575836,\n",
       " -12.833333333333334,\n",
       " [tensor(0.2105),\n",
       "  tensor(0.2332),\n",
       "  tensor(0.2018),\n",
       "  tensor(0.2543),\n",
       "  tensor(0.2522),\n",
       "  tensor(0.2597),\n",
       "  tensor(0.2168),\n",
       "  tensor(0.2586),\n",
       "  tensor(0.2140),\n",
       "  tensor(0.2814),\n",
       "  tensor(0.1911),\n",
       "  tensor(0.2555)],\n",
       " [-36, -15, -44, 4, 2, 9, -30, 8, -33, 29, -53, 5])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
