{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'psychology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  56.9051 valid loss 1.603 and accuracy 0.5971\n",
      "Epoch 1 train loss  50.0853 valid loss 1.856 and accuracy 0.4777\n",
      "Epoch 2 train loss  57.5077 valid loss 1.957 and accuracy 0.5179\n",
      "Epoch 3 train loss  43.7778 valid loss 2.762 and accuracy 0.3415\n",
      "Epoch 4 train loss  51.4445 valid loss 2.859 and accuracy 0.3415\n",
      "Epoch 5 train loss  50.0685 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 6 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 7 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 8 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 9 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 10 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 11 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 12 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 13 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 14 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 15 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 16 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 17 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 18 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 19 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 20 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 21 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 22 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 23 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 24 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 25 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 26 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 27 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 28 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n",
      "Epoch 29 train loss  50.7254 valid loss 2.762 and accuracy 0.2690\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2434]), points: -6\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2418]), points: -15\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6819 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7120 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6964 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6931 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6895 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6879 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6754 valid loss 0.025 and accuracy 0.2545\n",
      "Epoch 7 train loss  0.6679 valid loss 0.024 and accuracy 0.2790\n",
      "Epoch 8 train loss  0.6281 valid loss 0.024 and accuracy 0.3259\n",
      "Epoch 9 train loss  0.5839 valid loss 0.024 and accuracy 0.3873\n",
      "Epoch 10 train loss  0.5322 valid loss 0.024 and accuracy 0.4509\n",
      "Epoch 11 train loss  0.4781 valid loss 0.025 and accuracy 0.5424\n",
      "Epoch 12 train loss  0.4301 valid loss 0.028 and accuracy 0.5502\n",
      "Epoch 13 train loss  0.4043 valid loss 0.027 and accuracy 0.6038\n",
      "Epoch 14 train loss  0.3766 valid loss 0.028 and accuracy 0.6105\n",
      "Epoch 15 train loss  0.3536 valid loss 0.030 and accuracy 0.6138\n",
      "Epoch 16 train loss  0.3291 valid loss 0.029 and accuracy 0.6217\n",
      "Epoch 17 train loss  0.3209 valid loss 0.029 and accuracy 0.6161\n",
      "Epoch 18 train loss  0.3023 valid loss 0.031 and accuracy 0.6350\n",
      "Epoch 19 train loss  0.2820 valid loss 0.032 and accuracy 0.6228\n",
      "Epoch 20 train loss  0.2830 valid loss 0.031 and accuracy 0.6027\n",
      "Epoch 21 train loss  0.2660 valid loss 0.030 and accuracy 0.5926\n",
      "Epoch 22 train loss  0.2587 valid loss 0.031 and accuracy 0.6105\n",
      "Epoch 23 train loss  0.2526 valid loss 0.032 and accuracy 0.6205\n",
      "Epoch 24 train loss  0.2492 valid loss 0.031 and accuracy 0.6283\n",
      "Epoch 25 train loss  0.2429 valid loss 0.031 and accuracy 0.6205\n",
      "Epoch 26 train loss  0.2361 valid loss 0.032 and accuracy 0.6049\n",
      "Epoch 27 train loss  0.2353 valid loss 0.033 and accuracy 0.6272\n",
      "Epoch 28 train loss  0.2329 valid loss 0.033 and accuracy 0.6272\n",
      "Epoch 29 train loss  0.2226 valid loss 0.032 and accuracy 0.6071\n",
      "Epoch 30 train loss  0.2174 valid loss 0.033 and accuracy 0.6217\n",
      "Epoch 31 train loss  0.2210 valid loss 0.034 and accuracy 0.6127\n",
      "Epoch 32 train loss  0.2214 valid loss 0.034 and accuracy 0.6071\n",
      "Epoch 33 train loss  0.2171 valid loss 0.033 and accuracy 0.6083\n",
      "Epoch 34 train loss  0.2089 valid loss 0.034 and accuracy 0.6094\n",
      "Epoch 35 train loss  0.2100 valid loss 0.033 and accuracy 0.5971\n",
      "Epoch 36 train loss  0.2050 valid loss 0.034 and accuracy 0.6060\n",
      "Epoch 37 train loss  0.2053 valid loss 0.035 and accuracy 0.6172\n",
      "Epoch 38 train loss  0.2009 valid loss 0.035 and accuracy 0.6150\n",
      "Epoch 39 train loss  0.1992 valid loss 0.037 and accuracy 0.6172\n",
      "Epoch 40 train loss  0.1979 valid loss 0.037 and accuracy 0.6105\n",
      "Epoch 41 train loss  0.1976 valid loss 0.037 and accuracy 0.6105\n",
      "Epoch 42 train loss  0.1983 valid loss 0.038 and accuracy 0.6172\n",
      "Epoch 43 train loss  0.1955 valid loss 0.039 and accuracy 0.6038\n",
      "Epoch 44 train loss  0.1929 valid loss 0.038 and accuracy 0.6138\n",
      "Epoch 45 train loss  0.1923 valid loss 0.039 and accuracy 0.6172\n",
      "Epoch 46 train loss  0.1949 valid loss 0.041 and accuracy 0.6150\n",
      "Epoch 47 train loss  0.1886 valid loss 0.039 and accuracy 0.6105\n",
      "Epoch 48 train loss  0.1985 valid loss 0.039 and accuracy 0.6094\n",
      "Epoch 49 train loss  0.1889 valid loss 0.041 and accuracy 0.6172\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.3142]), points: 58\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2593]), points: 17\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3860 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  50.5580 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  71.7265 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 22 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 23 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 41 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 42 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 43 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 44 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 45 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 46 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 47 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 48 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 49 train loss  49.4420 valid loss 0.921 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2478]), points: -2\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2418]), points: -15\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5020 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4944 valid loss 0.003 and accuracy 0.7493\n",
      "Epoch 2 train loss  0.4679 valid loss 0.003 and accuracy 0.7364\n",
      "Epoch 3 train loss  0.4185 valid loss 0.004 and accuracy 0.6364\n",
      "Epoch 4 train loss  0.3610 valid loss 0.004 and accuracy 0.6754\n",
      "Epoch 5 train loss  0.2884 valid loss 0.004 and accuracy 0.6915\n",
      "Epoch 6 train loss  0.2470 valid loss 0.006 and accuracy 0.7197\n",
      "Epoch 7 train loss  0.2029 valid loss 0.007 and accuracy 0.6956\n",
      "Epoch 8 train loss  0.1497 valid loss 0.008 and accuracy 0.6649\n",
      "Epoch 9 train loss  0.1534 valid loss 0.007 and accuracy 0.6774\n",
      "Epoch 10 train loss  0.1213 valid loss 0.008 and accuracy 0.6608\n",
      "Epoch 11 train loss  0.0838 valid loss 0.008 and accuracy 0.6647\n",
      "Epoch 12 train loss  0.0928 valid loss 0.008 and accuracy 0.6561\n",
      "Epoch 13 train loss  0.0836 valid loss 0.008 and accuracy 0.6465\n",
      "Epoch 14 train loss  0.0633 valid loss 0.010 and accuracy 0.6675\n",
      "Epoch 15 train loss  0.0641 valid loss 0.008 and accuracy 0.6704\n",
      "Epoch 16 train loss  0.0612 valid loss 0.010 and accuracy 0.6506\n",
      "Epoch 17 train loss  0.0582 valid loss 0.010 and accuracy 0.6836\n",
      "Epoch 18 train loss  0.0528 valid loss 0.008 and accuracy 0.6713\n",
      "Epoch 19 train loss  0.0490 valid loss 0.011 and accuracy 0.6884\n",
      "Epoch 20 train loss  0.0499 valid loss 0.007 and accuracy 0.6803\n",
      "Epoch 21 train loss  0.0491 valid loss 0.012 and accuracy 0.6895\n",
      "Epoch 22 train loss  0.0474 valid loss 0.010 and accuracy 0.6759\n",
      "Epoch 23 train loss  0.0444 valid loss 0.011 and accuracy 0.6868\n",
      "Epoch 24 train loss  0.0421 valid loss 0.012 and accuracy 0.6978\n",
      "Epoch 25 train loss  0.0388 valid loss 0.012 and accuracy 0.6930\n",
      "Epoch 26 train loss  0.0423 valid loss 0.012 and accuracy 0.6996\n",
      "Epoch 27 train loss  0.0493 valid loss 0.011 and accuracy 0.6811\n",
      "Epoch 28 train loss  0.0488 valid loss 0.011 and accuracy 0.6757\n",
      "Epoch 29 train loss  0.0428 valid loss 0.009 and accuracy 0.6996\n",
      "Epoch 30 train loss  0.0254 valid loss 0.012 and accuracy 0.6526\n",
      "Epoch 31 train loss  0.0328 valid loss 0.010 and accuracy 0.7046\n",
      "Epoch 32 train loss  0.0320 valid loss 0.015 and accuracy 0.6943\n",
      "Epoch 33 train loss  0.0378 valid loss 0.012 and accuracy 0.7020\n",
      "Epoch 34 train loss  0.0396 valid loss 0.010 and accuracy 0.6882\n",
      "Epoch 35 train loss  0.0308 valid loss 0.015 and accuracy 0.7007\n",
      "Epoch 36 train loss  0.0326 valid loss 0.011 and accuracy 0.6735\n",
      "Epoch 37 train loss  0.0305 valid loss 0.012 and accuracy 0.6651\n",
      "Epoch 38 train loss  0.0264 valid loss 0.013 and accuracy 0.6761\n",
      "Epoch 39 train loss  0.0322 valid loss 0.015 and accuracy 0.6950\n",
      "Epoch 40 train loss  0.0294 valid loss 0.015 and accuracy 0.6721\n",
      "Epoch 41 train loss  0.0305 valid loss 0.016 and accuracy 0.6925\n",
      "Epoch 42 train loss  0.0302 valid loss 0.016 and accuracy 0.6961\n",
      "Epoch 43 train loss  0.0215 valid loss 0.016 and accuracy 0.6912\n",
      "Epoch 44 train loss  0.0204 valid loss 0.015 and accuracy 0.6759\n",
      "Epoch 45 train loss  0.0275 valid loss 0.014 and accuracy 0.6814\n",
      "Epoch 46 train loss  0.0246 valid loss 0.010 and accuracy 0.6618\n",
      "Epoch 47 train loss  0.0264 valid loss 0.017 and accuracy 0.7039\n",
      "Epoch 48 train loss  0.0340 valid loss 0.012 and accuracy 0.6671\n",
      "Epoch 49 train loss  0.0265 valid loss 0.014 and accuracy 0.6831\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2212]), points: -26\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2637]), points: 25\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5018 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4933 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.4598 valid loss 0.004 and accuracy 0.7412\n",
      "Epoch 3 train loss  0.4021 valid loss 0.004 and accuracy 0.6803\n",
      "Epoch 4 train loss  0.3371 valid loss 0.004 and accuracy 0.6581\n",
      "Epoch 5 train loss  0.2982 valid loss 0.005 and accuracy 0.7090\n",
      "Epoch 6 train loss  0.2329 valid loss 0.005 and accuracy 0.6858\n",
      "Epoch 7 train loss  0.1926 valid loss 0.008 and accuracy 0.7108\n",
      "Epoch 8 train loss  0.1703 valid loss 0.007 and accuracy 0.6772\n",
      "Epoch 9 train loss  0.1304 valid loss 0.008 and accuracy 0.6684\n",
      "Epoch 10 train loss  0.1069 valid loss 0.007 and accuracy 0.6410\n",
      "Epoch 11 train loss  0.0900 valid loss 0.010 and accuracy 0.6522\n",
      "Epoch 12 train loss  0.0839 valid loss 0.008 and accuracy 0.6483\n",
      "Epoch 13 train loss  0.0856 valid loss 0.007 and accuracy 0.6583\n",
      "Epoch 14 train loss  0.0698 valid loss 0.008 and accuracy 0.6667\n",
      "Epoch 15 train loss  0.0601 valid loss 0.009 and accuracy 0.6574\n",
      "Epoch 16 train loss  0.0679 valid loss 0.012 and accuracy 0.6401\n",
      "Epoch 17 train loss  0.0672 valid loss 0.008 and accuracy 0.6539\n",
      "Epoch 18 train loss  0.0574 valid loss 0.010 and accuracy 0.6588\n",
      "Epoch 19 train loss  0.0526 valid loss 0.010 and accuracy 0.6518\n",
      "Epoch 20 train loss  0.0495 valid loss 0.009 and accuracy 0.6579\n",
      "Epoch 21 train loss  0.0437 valid loss 0.011 and accuracy 0.6687\n",
      "Epoch 22 train loss  0.0476 valid loss 0.009 and accuracy 0.6373\n",
      "Epoch 23 train loss  0.0487 valid loss 0.011 and accuracy 0.6645\n",
      "Epoch 24 train loss  0.0460 valid loss 0.013 and accuracy 0.6961\n",
      "Epoch 25 train loss  0.0407 valid loss 0.009 and accuracy 0.6480\n",
      "Epoch 26 train loss  0.0449 valid loss 0.012 and accuracy 0.6980\n",
      "Epoch 27 train loss  0.0408 valid loss 0.009 and accuracy 0.6597\n",
      "Epoch 28 train loss  0.0328 valid loss 0.013 and accuracy 0.6993\n",
      "Epoch 29 train loss  0.0329 valid loss 0.011 and accuracy 0.6623\n",
      "Epoch 30 train loss  0.0345 valid loss 0.012 and accuracy 0.6947\n",
      "Epoch 31 train loss  0.0310 valid loss 0.011 and accuracy 0.6757\n",
      "Epoch 32 train loss  0.0348 valid loss 0.013 and accuracy 0.6877\n",
      "Epoch 33 train loss  0.0365 valid loss 0.011 and accuracy 0.6717\n",
      "Epoch 34 train loss  0.0393 valid loss 0.012 and accuracy 0.6965\n",
      "Epoch 35 train loss  0.0329 valid loss 0.011 and accuracy 0.6756\n",
      "Epoch 36 train loss  0.0296 valid loss 0.012 and accuracy 0.6689\n",
      "Epoch 37 train loss  0.0293 valid loss 0.013 and accuracy 0.6607\n",
      "Epoch 38 train loss  0.0281 valid loss 0.013 and accuracy 0.6713\n",
      "Epoch 39 train loss  0.0314 valid loss 0.015 and accuracy 0.6790\n",
      "Epoch 40 train loss  0.0259 valid loss 0.013 and accuracy 0.6820\n",
      "Epoch 41 train loss  0.0251 valid loss 0.011 and accuracy 0.6835\n",
      "Epoch 42 train loss  0.0309 valid loss 0.014 and accuracy 0.7057\n",
      "Epoch 43 train loss  0.0354 valid loss 0.015 and accuracy 0.6515\n",
      "Epoch 44 train loss  0.0279 valid loss 0.011 and accuracy 0.6943\n",
      "Epoch 45 train loss  0.0277 valid loss 0.013 and accuracy 0.6711\n",
      "Epoch 46 train loss  0.0242 valid loss 0.013 and accuracy 0.7063\n",
      "Epoch 47 train loss  0.0190 valid loss 0.015 and accuracy 0.6881\n",
      "Epoch 48 train loss  0.0158 valid loss 0.015 and accuracy 0.7072\n",
      "Epoch 49 train loss  0.0169 valid loss 0.016 and accuracy 0.6972\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.3009]), points: 46\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2681]), points: 33\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-converter",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "typical-holmes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models/logistic_regressor_psychology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV\n",
      "Accuracy media 0.24336283\n",
      "Puntos media -6.0\n",
      "[tensor(0.2434)]\n",
      "[-6]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24169081\n",
      "Puntos media -7.5\n",
      "[tensor(0.2478), tensor(0.2356)]\n",
      "[-2, -13]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.3141593\n",
      "Puntos media 58.0\n",
      "[tensor(0.3142)]\n",
      "[58]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25927538\n",
      "Puntos media 8.5\n",
      "[tensor(0.2652), tensor(0.2533)]\n",
      "[14, 3]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.24778761\n",
      "Puntos media -2.0\n",
      "[tensor(0.2478)]\n",
      "[-2]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24173912\n",
      "Puntos media -7.5\n",
      "[tensor(0.2435), tensor(0.2400)]\n",
      "[-6, -9]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "joint-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.22123894\n",
      "Puntos media -26.0\n",
      "[tensor(0.2212)]\n",
      "[-26]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.26367152\n",
      "Puntos media 12.5\n",
      "[tensor(0.2696), tensor(0.2578)]\n",
      "[18, 7]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.30088496\n",
      "Puntos media 46.0\n",
      "[tensor(0.3009)]\n",
      "[46]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2679227\n",
      "Puntos media 16.5\n",
      "[tensor(0.2870), tensor(0.2489)]\n",
      "[34, -1]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-favorite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
