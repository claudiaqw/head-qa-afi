{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "rural-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vectorizer, HeadQA, HeadQA_IR, clean_words, parse_dataset, parse_ir_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import train, validate, evaluate, evaluator_ir, train_ir, validate_ir, load_embeddings_from_file, make_embedding_matrix\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "applied-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_instances = parse_ir_dataset(training)\n",
    "#validation_instances = parse_ir_dataset(validation)\n",
    "#testing_instances = parse_ir_dataset(testing)\n",
    "\n",
    "#oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "#save_dataset_to_pickle('../data/training_ir.pickle', training_instances)\n",
    "#save_dataset_to_pickle('../data/validation_ir.pickle', validation_instances)\n",
    "#save_dataset_to_pickle('../data/testing_ir.pickle', testing_instances)\n",
    "#save_dataset_to_pickle('../data/oversampled_training_ir.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reasonable-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intermediate-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Los potenciales postsinápticos excitadores:',\n",
       " 'answer': 'Son de tipo todo o nada.',\n",
       " 'tok_qtext': ['Los', 'potenciales', 'postsinápticos', 'excitadores', ':'],\n",
       " 'tok_atext': ['Son', 'de', 'tipo', 'todo', 'o', 'nada', '.'],\n",
       " 'label': 0,\n",
       " 'category': 'biology'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controlling-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amino-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greek-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "transsexual-growth",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LSTM_CNN_QA(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, x_size, n_classes, embedding_size=300,\n",
    "                 padding_idx=0, pretrained_embeddings=None): \n",
    "        super(LSTM_CNN_QA, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size,num_embeddings=self.vocab_size,\n",
    "                                    padding_idx=padding_idx)\n",
    "        else:\n",
    "            print('Loading pretrained embeddings...')\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size, num_embeddings=self.vocab_size,\n",
    "                                    padding_idx=padding_idx, _weight=pretrained_embeddings)\n",
    "            self.emb.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True, dropout=0.5,bidirectional=True)\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=2, out_channels=10, kernel_size=3)   \n",
    "        \n",
    "#         self.conv_3 = nn.Conv1d(in_channels=self.embedding_size, out_channels= self.out_channels, kernel_size=3)\n",
    "#         self.conv_4 = nn.Conv1d(in_channels=self.embedding_size, out_channels= self.out_channels, kernel_size=4)\n",
    "#         self.conv_5 = nn.Conv1d(in_channels=self.embedding_size, out_channels= self.out_channels, kernel_size=5)\n",
    "        \n",
    "        self.cosine = nn.CosineSimilarity(dim=1)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, 64)  \n",
    "        self.linear1 = nn.Linear(64, self.n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_0, x_1):\n",
    "        x_0 = self.emb(x_0)\n",
    "        x_1 = self.emb(x_1)\n",
    "        out_0, (ht_0, ct_0) = self.lstm(x_0)\n",
    "        out_1, (ht_1, ct_1) = self.lstm(x_1) \n",
    "        ht_0 = ht_0.transpose(0, 1)\n",
    "        ht_1 = ht_1.transpose(0, 1)\n",
    "        ht_0 = self.conv(ht_0)\n",
    "        ht_1 = self.conv(ht_1)\n",
    "        x = self.cosine(out_0, out_1)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "incorrect-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "organized-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "tough-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "silent-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "indirect-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ahead-replication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for x_0, x_1, y in train_dt:\n",
    "    out = model(x_0.long(), x_1.long())\n",
    "    print(out.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "vulnerable-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ir(model, dataloader):\n",
    "    model.eval()\n",
    "    loss, right, total = 0, 0, 0\n",
    "    y_true, y_preds = [], []\n",
    "    for x_0, x_1, y in dataloader:\n",
    "        batch = y.shape[0]\n",
    "        out = model(x_0.long(), x_1.long())\n",
    "        loss = F.binary_cross_entropy(out, y.float())\n",
    "        loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        pred = torch.where(out > 0.4, 1, 0)\n",
    "        y_true.append(y)\n",
    "        y_preds.append(pred)\n",
    "        right += (pred == y).float().sum().item()\n",
    "    return loss/total, right/total, y_true, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "designing-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.7188 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.7116 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.6854 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.6437 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.6075 valid loss 0.006 and accuracy 0.7498\n",
      "Epoch 5 train loss  0.5735 valid loss 0.006 and accuracy 0.7491\n",
      "Epoch 6 train loss  0.5488 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 7 train loss  0.5308 valid loss 0.007 and accuracy 0.7489\n",
      "Epoch 8 train loss  0.5194 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 9 train loss  0.5082 valid loss 0.007 and accuracy 0.7487\n",
      "Epoch 10 train loss  0.4978 valid loss 0.007 and accuracy 0.7489\n",
      "Epoch 11 train loss  0.4862 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 12 train loss  0.4813 valid loss 0.007 and accuracy 0.7485\n",
      "Epoch 13 train loss  0.4789 valid loss 0.006 and accuracy 0.7498\n",
      "Epoch 14 train loss  0.4693 valid loss 0.006 and accuracy 0.7496\n",
      "Epoch 15 train loss  0.4689 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 16 train loss  0.4618 valid loss 0.007 and accuracy 0.7482\n",
      "Epoch 17 train loss  0.4620 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 18 train loss  0.4570 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 19 train loss  0.4532 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 20 train loss  0.4567 valid loss 0.007 and accuracy 0.7482\n",
      "Epoch 21 train loss  0.4490 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 22 train loss  0.4483 valid loss 0.006 and accuracy 0.7491\n",
      "Epoch 23 train loss  0.4444 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 24 train loss  0.4457 valid loss 0.006 and accuracy 0.7496\n",
      "Epoch 25 train loss  0.4444 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 26 train loss  0.4416 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 27 train loss  0.4388 valid loss 0.006 and accuracy 0.7493\n",
      "Epoch 28 train loss  0.4391 valid loss 0.006 and accuracy 0.7493\n",
      "Epoch 29 train loss  0.4378 valid loss 0.006 and accuracy 0.7506\n",
      "Epoch 30 train loss  0.4375 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 31 train loss  0.4373 valid loss 0.007 and accuracy 0.7489\n",
      "Epoch 32 train loss  0.4351 valid loss 0.006 and accuracy 0.7489\n",
      "Epoch 33 train loss  0.4347 valid loss 0.007 and accuracy 0.7489\n",
      "Epoch 34 train loss  0.4335 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 35 train loss  0.4338 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 36 train loss  0.4318 valid loss 0.006 and accuracy 0.7496\n",
      "Epoch 37 train loss  0.4307 valid loss 0.006 and accuracy 0.7493\n",
      "Epoch 38 train loss  0.4285 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 39 train loss  0.4309 valid loss 0.006 and accuracy 0.7504\n",
      "Epoch 40 train loss  0.4251 valid loss 0.006 and accuracy 0.7494\n",
      "Epoch 41 train loss  0.4256 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 42 train loss  0.4250 valid loss 0.006 and accuracy 0.7493\n",
      "Epoch 43 train loss  0.4263 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 44 train loss  0.4266 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 45 train loss  0.4243 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 46 train loss  0.4246 valid loss 0.006 and accuracy 0.7494\n",
      "Epoch 47 train loss  0.4209 valid loss 0.006 and accuracy 0.7504\n",
      "Epoch 48 train loss  0.4202 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 49 train loss  0.4212 valid loss 0.007 and accuracy 0.7493\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(model, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "spiritual-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2750]), 274)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "built-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2643]), 78)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "complimentary-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/lstm_cnn_qa'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-adoption",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
