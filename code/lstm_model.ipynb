{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import train, validate, evaluate, evaluator, evaluate_better, get_optimizer\n",
    "\n",
    "from supervised_models import BasicLSTM\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=oversampled_training, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3690 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.5278 valid loss 0.023 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.5082 valid loss 0.019 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.5299 valid loss 0.020 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.5222 valid loss 0.013 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.5012 valid loss 0.021 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.4874 valid loss 0.018 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.5184 valid loss 0.013 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.5377 valid loss 0.014 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.4955 valid loss 0.020 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.5038 valid loss 0.015 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.4897 valid loss 0.016 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.4769 valid loss 0.020 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.4689 valid loss 0.015 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.4903 valid loss 0.015 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.4667 valid loss 0.015 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.4689 valid loss 0.019 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.4698 valid loss 0.017 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.4533 valid loss 0.016 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.4305 valid loss 0.017 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.3937 valid loss 0.017 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.3365 valid loss 0.017 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.3033 valid loss 0.017 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.2777 valid loss 0.018 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.2538 valid loss 0.016 and accuracy 0.2517\n",
      "Epoch 25 train loss  0.2273 valid loss 0.016 and accuracy 0.2522\n",
      "Epoch 26 train loss  0.2139 valid loss 0.016 and accuracy 0.2675\n",
      "Epoch 27 train loss  0.2020 valid loss 0.019 and accuracy 0.2570\n",
      "Epoch 28 train loss  0.1904 valid loss 0.018 and accuracy 0.2568\n",
      "Epoch 29 train loss  0.1877 valid loss 0.017 and accuracy 0.2614\n",
      "Epoch 30 train loss  0.1768 valid loss 0.016 and accuracy 0.2789\n",
      "Epoch 31 train loss  0.1734 valid loss 0.018 and accuracy 0.2772\n",
      "Epoch 32 train loss  0.1738 valid loss 0.017 and accuracy 0.2930\n",
      "Epoch 33 train loss  0.1693 valid loss 0.015 and accuracy 0.2969\n",
      "Epoch 34 train loss  0.1712 valid loss 0.014 and accuracy 0.3123\n",
      "Epoch 35 train loss  0.1643 valid loss 0.014 and accuracy 0.3329\n",
      "Epoch 36 train loss  0.1583 valid loss 0.014 and accuracy 0.3425\n",
      "Epoch 37 train loss  0.1538 valid loss 0.014 and accuracy 0.3491\n",
      "Epoch 38 train loss  0.1502 valid loss 0.015 and accuracy 0.3493\n",
      "Epoch 39 train loss  0.1456 valid loss 0.014 and accuracy 0.3599\n",
      "Epoch 40 train loss  0.1481 valid loss 0.015 and accuracy 0.3610\n",
      "Epoch 41 train loss  0.1446 valid loss 0.014 and accuracy 0.3651\n",
      "Epoch 42 train loss  0.1528 valid loss 0.014 and accuracy 0.3697\n",
      "Epoch 43 train loss  0.1632 valid loss 0.013 and accuracy 0.3632\n",
      "Epoch 44 train loss  0.1703 valid loss 0.012 and accuracy 0.3662\n",
      "Epoch 45 train loss  0.1538 valid loss 0.013 and accuracy 0.3728\n",
      "Epoch 46 train loss  0.1383 valid loss 0.013 and accuracy 0.3664\n",
      "Epoch 47 train loss  0.1372 valid loss 0.013 and accuracy 0.3857\n",
      "Epoch 48 train loss  0.1348 valid loss 0.015 and accuracy 0.4013\n",
      "Epoch 49 train loss  0.1342 valid loss 0.014 and accuracy 0.3998\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2467]), -18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2633]), 146)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_lstm.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_lstm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/lstm'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLSTM(\n",
       "  (embeddings): Embedding(20403, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "model.load_state_dict(torch.load(os.getcwd() + '/trained_models/lstm'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22040337,\n",
       " -27.0,\n",
       " [tensor(0.2345),\n",
       "  tensor(0.1522),\n",
       "  tensor(0.2089),\n",
       "  tensor(0.2554),\n",
       "  tensor(0.2434),\n",
       "  tensor(0.2281)],\n",
       " [-14, -90, -37, 5, -6, -20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2512668,\n",
       " 1.1666666666666667,\n",
       " [tensor(0.2368),\n",
       "  tensor(0.2646),\n",
       "  tensor(0.2149),\n",
       "  tensor(0.2155),\n",
       "  tensor(0.2435),\n",
       "  tensor(0.2684),\n",
       "  tensor(0.2566),\n",
       "  tensor(0.3103),\n",
       "  tensor(0.2882),\n",
       "  tensor(0.2251),\n",
       "  tensor(0.2489),\n",
       "  tensor(0.2423)],\n",
       " [-12, 13, -32, -32, -6, 17, 6, 56, 35, -23, -1, -7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
