{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import train, validate, evaluate, evaluator, evaluate_better\n",
    "\n",
    "from supervised_models import BasicLSTM\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5090 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.5004 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.4986 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.4947 valid loss 0.004 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.4828 valid loss 0.004 and accuracy 0.7472\n",
      "Epoch 5 train loss  0.4529 valid loss 0.004 and accuracy 0.7338\n",
      "Epoch 6 train loss  0.4148 valid loss 0.005 and accuracy 0.7267\n",
      "Epoch 7 train loss  0.3680 valid loss 0.005 and accuracy 0.7169\n",
      "Epoch 8 train loss  0.3345 valid loss 0.006 and accuracy 0.7072\n",
      "Epoch 9 train loss  0.3067 valid loss 0.007 and accuracy 0.7068\n",
      "Epoch 10 train loss  0.2832 valid loss 0.007 and accuracy 0.7055\n",
      "Epoch 11 train loss  0.2681 valid loss 0.008 and accuracy 0.6892\n",
      "Epoch 12 train loss  0.2557 valid loss 0.008 and accuracy 0.6724\n",
      "Epoch 13 train loss  0.2434 valid loss 0.009 and accuracy 0.6509\n",
      "Epoch 14 train loss  0.2327 valid loss 0.009 and accuracy 0.6846\n",
      "Epoch 15 train loss  0.2245 valid loss 0.009 and accuracy 0.6754\n",
      "Epoch 16 train loss  0.2201 valid loss 0.009 and accuracy 0.6743\n",
      "Epoch 17 train loss  0.2157 valid loss 0.009 and accuracy 0.6857\n",
      "Epoch 18 train loss  0.2066 valid loss 0.010 and accuracy 0.6811\n",
      "Epoch 19 train loss  0.2034 valid loss 0.010 and accuracy 0.6770\n",
      "Epoch 20 train loss  0.2024 valid loss 0.009 and accuracy 0.7026\n",
      "Epoch 21 train loss  0.1995 valid loss 0.010 and accuracy 0.6871\n",
      "Epoch 22 train loss  0.1896 valid loss 0.011 and accuracy 0.6899\n",
      "Epoch 23 train loss  0.1916 valid loss 0.011 and accuracy 0.6950\n",
      "Epoch 24 train loss  0.1878 valid loss 0.011 and accuracy 0.6928\n",
      "Epoch 25 train loss  0.1865 valid loss 0.011 and accuracy 0.6789\n",
      "Epoch 26 train loss  0.1822 valid loss 0.011 and accuracy 0.6774\n",
      "Epoch 27 train loss  0.1820 valid loss 0.010 and accuracy 0.7096\n",
      "Epoch 28 train loss  0.1772 valid loss 0.011 and accuracy 0.6739\n",
      "Epoch 29 train loss  0.1773 valid loss 0.011 and accuracy 0.6748\n",
      "Epoch 30 train loss  0.1695 valid loss 0.012 and accuracy 0.6844\n",
      "Epoch 31 train loss  0.1713 valid loss 0.012 and accuracy 0.6757\n",
      "Epoch 32 train loss  0.1677 valid loss 0.013 and accuracy 0.6704\n",
      "Epoch 33 train loss  0.1670 valid loss 0.014 and accuracy 0.6517\n",
      "Epoch 34 train loss  0.1648 valid loss 0.013 and accuracy 0.6796\n",
      "Epoch 35 train loss  0.1657 valid loss 0.013 and accuracy 0.6912\n",
      "Epoch 36 train loss  0.1644 valid loss 0.013 and accuracy 0.7017\n",
      "Epoch 37 train loss  0.1613 valid loss 0.013 and accuracy 0.6890\n",
      "Epoch 38 train loss  0.1588 valid loss 0.012 and accuracy 0.7020\n",
      "Epoch 39 train loss  0.1569 valid loss 0.012 and accuracy 0.7079\n",
      "Epoch 40 train loss  0.1517 valid loss 0.013 and accuracy 0.6836\n",
      "Epoch 41 train loss  0.1556 valid loss 0.012 and accuracy 0.6869\n",
      "Epoch 42 train loss  0.1534 valid loss 0.012 and accuracy 0.6638\n",
      "Epoch 43 train loss  0.1525 valid loss 0.012 and accuracy 0.6897\n",
      "Epoch 44 train loss  0.1491 valid loss 0.014 and accuracy 0.6993\n",
      "Epoch 45 train loss  0.1501 valid loss 0.013 and accuracy 0.6983\n",
      "Epoch 46 train loss  0.1474 valid loss 0.014 and accuracy 0.6958\n",
      "Epoch 47 train loss  0.1485 valid loss 0.015 and accuracy 0.6941\n",
      "Epoch 48 train loss  0.1477 valid loss 0.014 and accuracy 0.7110\n",
      "Epoch 49 train loss  0.1450 valid loss 0.014 and accuracy 0.7094\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2687]), 102)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2509]), 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_lstm.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_lstm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5090134263756764, tensor(0.0035, requires_grad=True), 0.75],\n",
       " [0.5004443045840206, tensor(0.0034, requires_grad=True), 0.75],\n",
       " [0.4986275760524244, tensor(0.0035, requires_grad=True), 0.75],\n",
       " [0.4946762611348945, tensor(0.0035, requires_grad=True), 0.75],\n",
       " [0.4827755675976535, tensor(0.0037, requires_grad=True), 0.7472426470588235],\n",
       " [0.45287849709212064, tensor(0.0043, requires_grad=True), 0.7338235294117647],\n",
       " [0.4148019633738391, tensor(0.0050, requires_grad=True), 0.7266544117647059],\n",
       " [0.36803981342229497, tensor(0.0051, requires_grad=True), 0.7169117647058824],\n",
       " [0.3345039293349507, tensor(0.0062, requires_grad=True), 0.7071691176470588],\n",
       " [0.30671779653394077, tensor(0.0072, requires_grad=True), 0.7068014705882353],\n",
       " [0.2832131722695138, tensor(0.0073, requires_grad=True), 0.705514705882353],\n",
       " [0.26806766588285746, tensor(0.0081, requires_grad=True), 0.6891544117647059],\n",
       " [0.25569949992090824, tensor(0.0084, requires_grad=True), 0.6724264705882353],\n",
       " [0.24343141013060707, tensor(0.0092, requires_grad=True), 0.6509191176470588],\n",
       " [0.2326855543776449, tensor(0.0092, requires_grad=True), 0.6845588235294118],\n",
       " [0.22454304235976144, tensor(0.0093, requires_grad=True), 0.6753676470588236],\n",
       " [0.22010018503270953, tensor(0.0095, requires_grad=True), 0.674264705882353],\n",
       " [0.2156654456720115, tensor(0.0088, requires_grad=True), 0.6856617647058824],\n",
       " [0.20661162048892442, tensor(0.0095, requires_grad=True), 0.6810661764705882],\n",
       " [0.2033636562481343, tensor(0.0100, requires_grad=True), 0.6770220588235294],\n",
       " [0.20243966184691134, tensor(0.0093, requires_grad=True), 0.7025735294117647],\n",
       " [0.19951290875134697, tensor(0.0102, requires_grad=True), 0.6871323529411765],\n",
       " [0.18962816060576812, tensor(0.0109, requires_grad=True), 0.689889705882353],\n",
       " [0.19162430419665144, tensor(0.0109, requires_grad=True), 0.6950367647058824],\n",
       " [0.18778668495276607, tensor(0.0112, requires_grad=True), 0.6928308823529412],\n",
       " [0.18647179850545453, tensor(0.0109, requires_grad=True), 0.678860294117647],\n",
       " [0.1822085170501686, tensor(0.0114, requires_grad=True), 0.6773897058823529],\n",
       " [0.18196569169950055, tensor(0.0099, requires_grad=True), 0.7095588235294118],\n",
       " [0.17716932979086975, tensor(0.0114, requires_grad=True), 0.6738970588235295],\n",
       " [0.1772651593386005, tensor(0.0110, requires_grad=True), 0.6748161764705882],\n",
       " [0.1694582351741482, tensor(0.0117, requires_grad=True), 0.684375],\n",
       " [0.1712708422594356, tensor(0.0117, requires_grad=True), 0.6757352941176471],\n",
       " [0.16772360173446765, tensor(0.0126, requires_grad=True), 0.6704044117647059],\n",
       " [0.16698711610634823, tensor(0.0140, requires_grad=True), 0.6516544117647058],\n",
       " [0.16476374310273004, tensor(0.0132, requires_grad=True), 0.6795955882352941],\n",
       " [0.16570027823200875, tensor(0.0126, requires_grad=True), 0.6911764705882353],\n",
       " [0.16441030449161015, tensor(0.0127, requires_grad=True), 0.7016544117647059],\n",
       " [0.16131111095681877, tensor(0.0134, requires_grad=True), 0.6889705882352941],\n",
       " [0.15877412546044553, tensor(0.0119, requires_grad=True), 0.7020220588235294],\n",
       " [0.1569180070843368, tensor(0.0124, requires_grad=True), 0.7079044117647059],\n",
       " [0.15172213266858922, tensor(0.0132, requires_grad=True), 0.6836397058823529],\n",
       " [0.15563075820556338, tensor(0.0120, requires_grad=True), 0.6869485294117647],\n",
       " [0.15338906559389062, tensor(0.0125, requires_grad=True), 0.6637867647058824],\n",
       " [0.15251412407301523, tensor(0.0122, requires_grad=True), 0.6897058823529412],\n",
       " [0.1491115241171231, tensor(0.0139, requires_grad=True), 0.6992647058823529],\n",
       " [0.15007795167957846, tensor(0.0134, requires_grad=True), 0.6983455882352941],\n",
       " [0.14739522764947355, tensor(0.0139, requires_grad=True), 0.6957720588235294],\n",
       " [0.14849794514236006, tensor(0.0147, requires_grad=True), 0.6941176470588235],\n",
       " [0.1476959844755222, tensor(0.0144, requires_grad=True), 0.7110294117647059],\n",
       " [0.1449560174238812, tensor(0.0145, requires_grad=True), 0.709375]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/basic_lstm_sig'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLSTM(\n",
       "  (embeddings): Embedding(20403, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "model.load_state_dict(torch.load(os.getcwd() + '/trained_models/basic_lstm_sig'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26881942,\n",
       " 17.0,\n",
       " [tensor(0.2965),\n",
       "  tensor(0.2217),\n",
       "  tensor(0.2533),\n",
       "  tensor(0.2554),\n",
       "  tensor(0.3009),\n",
       "  tensor(0.2851)],\n",
       " [42, -26, 3, 5, 46, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25091484,\n",
       " 0.8333333333333334,\n",
       " [tensor(0.2325),\n",
       "  tensor(0.2601),\n",
       "  tensor(0.2544),\n",
       "  tensor(0.2414),\n",
       "  tensor(0.2696),\n",
       "  tensor(0.2597),\n",
       "  tensor(0.2345),\n",
       "  tensor(0.2802),\n",
       "  tensor(0.2489),\n",
       "  tensor(0.2035),\n",
       "  tensor(0.2356),\n",
       "  tensor(0.2907)],\n",
       " [-16, 9, 4, -8, 18, 9, -14, 28, -1, -43, -13, 37])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
