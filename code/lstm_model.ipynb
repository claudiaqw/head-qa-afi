{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import train, validate, evaluate, evaluator\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Los potenciales postsinápticos excitadores:',\n",
       " 'answer': 'Son de tipo todo o nada.',\n",
       " 'label': 0,\n",
       " 'sample_tok': ['Los',\n",
       "  'potenciales',\n",
       "  'postsinápticos',\n",
       "  'excitadores',\n",
       "  ':',\n",
       "  '[SEP]',\n",
       "  'Son',\n",
       "  'de',\n",
       "  'tipo',\n",
       "  'todo',\n",
       "  'o',\n",
       "  'nada',\n",
       "  '.'],\n",
       " 'category': 'biology'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, x_size, n_classes, embedding_dim=300): \n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, n_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out, (ht, ct) = self.lstm(x)        \n",
    "        x = self.linear(ht[-1])\n",
    "        return F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.7195 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.7177 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.7165 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.7118 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.6992 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 5 train loss  0.6729 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 6 train loss  0.6385 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 7 train loss  0.6119 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 8 train loss  0.5872 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 9 train loss  0.5690 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 10 train loss  0.5537 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 11 train loss  0.5439 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 12 train loss  0.5383 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 13 train loss  0.5255 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 14 train loss  0.5218 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 15 train loss  0.5197 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 16 train loss  0.5127 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 17 train loss  0.5134 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 18 train loss  0.5086 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 19 train loss  0.5070 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 20 train loss  0.5035 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 21 train loss  0.5014 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 22 train loss  0.5032 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 23 train loss  0.4984 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 24 train loss  0.4980 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 25 train loss  0.4948 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 26 train loss  0.4948 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 27 train loss  0.4940 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 28 train loss  0.4907 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 29 train loss  0.4910 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 30 train loss  0.4939 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 31 train loss  0.4908 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 32 train loss  0.4906 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 33 train loss  0.4882 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 34 train loss  0.4870 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 35 train loss  0.4863 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 36 train loss  0.4849 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 37 train loss  0.4882 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 38 train loss  0.4870 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 39 train loss  0.4830 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 40 train loss  0.4811 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 41 train loss  0.4832 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 42 train loss  0.4850 valid loss 0.012 and accuracy 0.7500\n",
      "Epoch 43 train loss  0.4842 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 44 train loss  0.4848 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 45 train loss  0.4829 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 46 train loss  0.4805 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 47 train loss  0.4808 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 48 train loss  0.4825 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 49 train loss  0.4798 valid loss 0.011 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2408]), -50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2542]), 46)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_lstm.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_lstm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7194906572261489, tensor(0.0054, requires_grad=True), 0.75],\n",
       " [0.7177084417228239, tensor(0.0054, requires_grad=True), 0.75],\n",
       " [0.7164642316749297, tensor(0.0054, requires_grad=True), 0.75],\n",
       " [0.7118355397718499, tensor(0.0055, requires_grad=True), 0.75],\n",
       " [0.6991902358560677, tensor(0.0059, requires_grad=True), 0.75],\n",
       " [0.672914225006678, tensor(0.0062, requires_grad=True), 0.75],\n",
       " [0.6385499821369907, tensor(0.0067, requires_grad=True), 0.75],\n",
       " [0.611919884078474, tensor(0.0070, requires_grad=True), 0.75],\n",
       " [0.5871523132525295, tensor(0.0074, requires_grad=True), 0.75],\n",
       " [0.5690496934465615, tensor(0.0076, requires_grad=True), 0.75],\n",
       " [0.5536535519433309, tensor(0.0077, requires_grad=True), 0.75],\n",
       " [0.543929350447942, tensor(0.0085, requires_grad=True), 0.75],\n",
       " [0.5382773188223322, tensor(0.0085, requires_grad=True), 0.75],\n",
       " [0.525461492409189, tensor(0.0088, requires_grad=True), 0.75],\n",
       " [0.5217669140861695, tensor(0.0086, requires_grad=True), 0.75],\n",
       " [0.5197156937007444, tensor(0.0089, requires_grad=True), 0.75],\n",
       " [0.5127092378685273, tensor(0.0094, requires_grad=True), 0.75],\n",
       " [0.5133769390812839, tensor(0.0086, requires_grad=True), 0.75],\n",
       " [0.5085560588951571, tensor(0.0087, requires_grad=True), 0.75],\n",
       " [0.5070198319044458, tensor(0.0085, requires_grad=True), 0.75],\n",
       " [0.5034935632383967, tensor(0.0095, requires_grad=True), 0.75],\n",
       " [0.5014465377991458, tensor(0.0089, requires_grad=True), 0.75],\n",
       " [0.5032065854733249, tensor(0.0095, requires_grad=True), 0.75],\n",
       " [0.49840688612087664, tensor(0.0099, requires_grad=True), 0.75],\n",
       " [0.498043233228017, tensor(0.0095, requires_grad=True), 0.75],\n",
       " [0.49482873900827157, tensor(0.0098, requires_grad=True), 0.75],\n",
       " [0.49480428157082523, tensor(0.0103, requires_grad=True), 0.75],\n",
       " [0.49395949179867665, tensor(0.0100, requires_grad=True), 0.75],\n",
       " [0.4907444698982928, tensor(0.0106, requires_grad=True), 0.75],\n",
       " [0.49101306027676683, tensor(0.0104, requires_grad=True), 0.75],\n",
       " [0.4938711224550224, tensor(0.0112, requires_grad=True), 0.75],\n",
       " [0.49076176149299344, tensor(0.0108, requires_grad=True), 0.75],\n",
       " [0.49057078440505336, tensor(0.0107, requires_grad=True), 0.75],\n",
       " [0.4882217591067395, tensor(0.0107, requires_grad=True), 0.75],\n",
       " [0.4870236872908581, tensor(0.0106, requires_grad=True), 0.75],\n",
       " [0.4862883942673005, tensor(0.0108, requires_grad=True), 0.75],\n",
       " [0.48490987922771867, tensor(0.0112, requires_grad=True), 0.75],\n",
       " [0.4882425181836967, tensor(0.0105, requires_grad=True), 0.75],\n",
       " [0.48701719225171103, tensor(0.0102, requires_grad=True), 0.75],\n",
       " [0.48295193056026137, tensor(0.0109, requires_grad=True), 0.75],\n",
       " [0.48105487615229137, tensor(0.0105, requires_grad=True), 0.75],\n",
       " [0.48319767347301346, tensor(0.0105, requires_grad=True), 0.75],\n",
       " [0.4849894696689514, tensor(0.0117, requires_grad=True), 0.75],\n",
       " [0.48418416273163023, tensor(0.0114, requires_grad=True), 0.75],\n",
       " [0.48482672421328993, tensor(0.0114, requires_grad=True), 0.75],\n",
       " [0.48294376580111953, tensor(0.0109, requires_grad=True), 0.75],\n",
       " [0.48049709042870853, tensor(0.0108, requires_grad=True), 0.75],\n",
       " [0.48084368827831314, tensor(0.0106, requires_grad=True), 0.75],\n",
       " [0.48253175472638693, tensor(0.0108, requires_grad=True), 0.75],\n",
       " [0.47982430651963476, tensor(0.0106, requires_grad=True), 0.75]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.getcwd() + '/trained_models/basic_lstm'\n",
    "# torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
