{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'biology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  39.7331 valid loss 2.185 and accuracy 0.5145\n",
      "Epoch 1 train loss  57.6679 valid loss 1.913 and accuracy 0.4922\n",
      "Epoch 2 train loss  51.0371 valid loss 2.647 and accuracy 0.3683\n",
      "Epoch 3 train loss  66.4870 valid loss 1.611 and accuracy 0.6451\n",
      "Epoch 4 train loss  44.4319 valid loss 1.422 and accuracy 0.6663\n",
      "Epoch 5 train loss  40.6669 valid loss 1.671 and accuracy 0.5737\n",
      "Epoch 6 train loss  47.9072 valid loss 1.381 and accuracy 0.6674\n",
      "Epoch 7 train loss  43.4086 valid loss 1.266 and accuracy 0.7054\n",
      "Epoch 8 train loss  41.7250 valid loss 1.266 and accuracy 0.7221\n",
      "Epoch 9 train loss  41.5001 valid loss 1.266 and accuracy 0.7154\n",
      "Epoch 10 train loss  41.2636 valid loss 1.381 and accuracy 0.7121\n",
      "Epoch 11 train loss  41.6210 valid loss 1.381 and accuracy 0.7076\n",
      "Epoch 12 train loss  41.6338 valid loss 1.381 and accuracy 0.7121\n",
      "Epoch 13 train loss  41.9512 valid loss 1.106 and accuracy 0.7299\n",
      "Epoch 14 train loss  41.6915 valid loss 1.266 and accuracy 0.7299\n",
      "Epoch 15 train loss  41.4558 valid loss 1.217 and accuracy 0.7199\n",
      "Epoch 16 train loss  41.4646 valid loss 1.203 and accuracy 0.7165\n",
      "Epoch 17 train loss  41.0853 valid loss 1.266 and accuracy 0.6875\n",
      "Epoch 18 train loss  40.9197 valid loss 1.160 and accuracy 0.6194\n",
      "Epoch 19 train loss  43.0629 valid loss 1.381 and accuracy 0.5201\n",
      "Epoch 20 train loss  45.1479 valid loss 1.611 and accuracy 0.4319\n",
      "Epoch 21 train loss  54.0505 valid loss 2.059 and accuracy 0.3571\n",
      "Epoch 22 train loss  55.1688 valid loss 2.074 and accuracy 0.3516\n",
      "Epoch 23 train loss  55.0551 valid loss 2.762 and accuracy 0.3304\n",
      "Epoch 24 train loss  56.0446 valid loss 2.647 and accuracy 0.3404\n",
      "Epoch 25 train loss  53.7930 valid loss 2.532 and accuracy 0.3404\n",
      "Epoch 26 train loss  53.4599 valid loss 2.187 and accuracy 0.3862\n",
      "Epoch 27 train loss  48.8512 valid loss 2.647 and accuracy 0.3348\n",
      "Epoch 28 train loss  52.5910 valid loss 2.729 and accuracy 0.3192\n",
      "Epoch 29 train loss  55.2482 valid loss 2.877 and accuracy 0.3025\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: biology\n",
      "accuracy: tensor([0.2301]), points: -18\n",
      "----------\n",
      "TEST Dominio: biology\n",
      "accuracy: tensor([0.2533]), points: 6\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6329 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7263 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6946 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6906 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6866 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6824 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6765 valid loss 0.024 and accuracy 0.2522\n",
      "Epoch 7 train loss  0.6659 valid loss 0.024 and accuracy 0.3136\n",
      "Epoch 8 train loss  0.6482 valid loss 0.024 and accuracy 0.3371\n",
      "Epoch 9 train loss  0.6114 valid loss 0.025 and accuracy 0.3940\n",
      "Epoch 10 train loss  0.5620 valid loss 0.025 and accuracy 0.5301\n",
      "Epoch 11 train loss  0.5115 valid loss 0.026 and accuracy 0.5737\n",
      "Epoch 12 train loss  0.4500 valid loss 0.027 and accuracy 0.6150\n",
      "Epoch 13 train loss  0.3888 valid loss 0.027 and accuracy 0.6105\n",
      "Epoch 14 train loss  0.3549 valid loss 0.028 and accuracy 0.6429\n",
      "Epoch 15 train loss  0.3264 valid loss 0.030 and accuracy 0.6429\n",
      "Epoch 16 train loss  0.2847 valid loss 0.030 and accuracy 0.6540\n",
      "Epoch 17 train loss  0.2487 valid loss 0.031 and accuracy 0.6395\n",
      "Epoch 18 train loss  0.2418 valid loss 0.033 and accuracy 0.6551\n",
      "Epoch 19 train loss  0.2249 valid loss 0.033 and accuracy 0.6685\n",
      "Epoch 20 train loss  0.2054 valid loss 0.034 and accuracy 0.6551\n",
      "Epoch 21 train loss  0.1809 valid loss 0.036 and accuracy 0.6629\n",
      "Epoch 22 train loss  0.1885 valid loss 0.035 and accuracy 0.6529\n",
      "Epoch 23 train loss  0.1739 valid loss 0.033 and accuracy 0.6451\n",
      "Epoch 24 train loss  0.1557 valid loss 0.036 and accuracy 0.6585\n",
      "Epoch 25 train loss  0.1595 valid loss 0.039 and accuracy 0.6618\n",
      "Epoch 26 train loss  0.1308 valid loss 0.037 and accuracy 0.6562\n",
      "Epoch 27 train loss  0.1387 valid loss 0.036 and accuracy 0.6551\n",
      "Epoch 28 train loss  0.1310 valid loss 0.033 and accuracy 0.6328\n",
      "Epoch 29 train loss  0.1208 valid loss 0.038 and accuracy 0.6484\n",
      "Epoch 30 train loss  0.1135 valid loss 0.038 and accuracy 0.6585\n",
      "Epoch 31 train loss  0.1102 valid loss 0.042 and accuracy 0.6607\n",
      "Epoch 32 train loss  0.1085 valid loss 0.043 and accuracy 0.6462\n",
      "Epoch 33 train loss  0.1076 valid loss 0.045 and accuracy 0.6462\n",
      "Epoch 34 train loss  0.1056 valid loss 0.044 and accuracy 0.6629\n",
      "Epoch 35 train loss  0.1011 valid loss 0.045 and accuracy 0.6607\n",
      "Epoch 36 train loss  0.0817 valid loss 0.043 and accuracy 0.6518\n",
      "Epoch 37 train loss  0.0903 valid loss 0.042 and accuracy 0.6451\n",
      "Epoch 38 train loss  0.0798 valid loss 0.044 and accuracy 0.6540\n",
      "Epoch 39 train loss  0.0762 valid loss 0.045 and accuracy 0.6429\n",
      "Epoch 40 train loss  0.0792 valid loss 0.043 and accuracy 0.6228\n",
      "Epoch 41 train loss  0.0778 valid loss 0.045 and accuracy 0.6440\n",
      "Epoch 42 train loss  0.0696 valid loss 0.047 and accuracy 0.6395\n",
      "Epoch 43 train loss  0.0595 valid loss 0.046 and accuracy 0.6529\n",
      "Epoch 44 train loss  0.0719 valid loss 0.046 and accuracy 0.6272\n",
      "Epoch 45 train loss  0.0646 valid loss 0.046 and accuracy 0.6417\n",
      "Epoch 46 train loss  0.0668 valid loss 0.047 and accuracy 0.6272\n",
      "Epoch 47 train loss  0.0569 valid loss 0.049 and accuracy 0.6350\n",
      "Epoch 48 train loss  0.0599 valid loss 0.050 and accuracy 0.6228\n",
      "Epoch 49 train loss  0.0582 valid loss 0.051 and accuracy 0.6384\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: biology\n",
      "accuracy: tensor([0.3009]), points: 46\n",
      "----------\n",
      "TEST Dominio: biology\n",
      "accuracy: tensor([0.2797]), points: 54\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4436 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  19.7950 valid loss 2.468 and accuracy 0.2500\n",
      "Epoch 2 train loss  13.1380 valid loss 1.054 and accuracy 0.2500\n",
      "Epoch 3 train loss  1.4101 valid loss 1.568 and accuracy 0.2500\n",
      "Epoch 4 train loss  2.1352 valid loss 0.311 and accuracy 0.2500\n",
      "Epoch 5 train loss  1.1977 valid loss 0.934 and accuracy 0.2489\n",
      "Epoch 6 train loss  1.3837 valid loss 0.231 and accuracy 0.2489\n",
      "Epoch 7 train loss  1.0271 valid loss 2.453 and accuracy 0.2500\n",
      "Epoch 8 train loss  2.4974 valid loss 0.331 and accuracy 0.2500\n",
      "Epoch 9 train loss  1.1671 valid loss 1.065 and accuracy 0.2500\n",
      "Epoch 10 train loss  1.3254 valid loss 0.270 and accuracy 0.2500\n",
      "Epoch 11 train loss  1.0261 valid loss 1.356 and accuracy 0.2500\n",
      "Epoch 12 train loss  2.0137 valid loss 2.438 and accuracy 0.2500\n",
      "Epoch 13 train loss  7.4514 valid loss 1.059 and accuracy 0.2500\n",
      "Epoch 14 train loss  2.0131 valid loss 2.439 and accuracy 0.2500\n",
      "Epoch 15 train loss  8.2253 valid loss 1.755 and accuracy 0.2500\n",
      "Epoch 16 train loss  3.2389 valid loss 0.656 and accuracy 0.2500\n",
      "Epoch 17 train loss  1.3245 valid loss 0.643 and accuracy 0.2500\n",
      "Epoch 18 train loss  1.2060 valid loss 0.528 and accuracy 0.2500\n",
      "Epoch 19 train loss  1.1099 valid loss 0.310 and accuracy 0.2500\n",
      "Epoch 20 train loss  1.0108 valid loss 1.558 and accuracy 0.2500\n",
      "Epoch 21 train loss  2.3622 valid loss 2.569 and accuracy 0.2500\n",
      "Epoch 22 train loss  3.5948 valid loss 0.451 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.9861 valid loss 0.321 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.9833 valid loss 0.324 and accuracy 0.2500\n",
      "Epoch 25 train loss  1.0187 valid loss 0.428 and accuracy 0.2500\n",
      "Epoch 26 train loss  1.1895 valid loss 0.645 and accuracy 0.2500\n",
      "Epoch 27 train loss  1.3713 valid loss 0.312 and accuracy 0.2500\n",
      "Epoch 28 train loss  1.1246 valid loss 1.852 and accuracy 0.2500\n",
      "Epoch 29 train loss  2.6246 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 30 train loss  7.4642 valid loss 2.455 and accuracy 0.2500\n",
      "Epoch 31 train loss  5.8356 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 32 train loss  8.0780 valid loss 2.269 and accuracy 0.2500\n",
      "Epoch 33 train loss  5.9849 valid loss 2.374 and accuracy 0.2500\n",
      "Epoch 34 train loss  13.4122 valid loss 2.243 and accuracy 0.2500\n",
      "Epoch 35 train loss  7.6091 valid loss 2.253 and accuracy 0.2511\n",
      "Epoch 36 train loss  7.5802 valid loss 2.253 and accuracy 0.2500\n",
      "Epoch 37 train loss  9.3038 valid loss 2.563 and accuracy 0.2500\n",
      "Epoch 38 train loss  7.7132 valid loss 2.548 and accuracy 0.2578\n",
      "Epoch 39 train loss  5.2783 valid loss 2.551 and accuracy 0.2567\n",
      "Epoch 40 train loss  6.5736 valid loss 2.136 and accuracy 0.2533\n",
      "Epoch 41 train loss  5.0026 valid loss 2.132 and accuracy 0.2556\n",
      "Epoch 42 train loss  6.3240 valid loss 1.451 and accuracy 0.2589\n",
      "Epoch 43 train loss  5.0538 valid loss 1.265 and accuracy 0.2634\n",
      "Epoch 44 train loss  3.8963 valid loss 0.825 and accuracy 0.2600\n",
      "Epoch 45 train loss  2.2773 valid loss 2.259 and accuracy 0.2500\n",
      "Epoch 46 train loss  3.0193 valid loss 0.685 and accuracy 0.2645\n",
      "Epoch 47 train loss  1.0935 valid loss 1.752 and accuracy 0.2522\n",
      "Epoch 48 train loss  1.5082 valid loss 1.244 and accuracy 0.2533\n",
      "Epoch 49 train loss  1.5572 valid loss 1.352 and accuracy 0.2533\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: biology\n",
      "accuracy: tensor([0.2965]), points: 42\n",
      "----------\n",
      "TEST Dominio: biology\n",
      "accuracy: tensor([0.2467]), points: -6\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "\n",
    "mixed_training_ir = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training_ir, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5014 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4968 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.4832 valid loss 0.004 and accuracy 0.7476\n",
      "Epoch 3 train loss  0.4524 valid loss 0.004 and accuracy 0.7406\n",
      "Epoch 4 train loss  0.4209 valid loss 0.004 and accuracy 0.7301\n",
      "Epoch 5 train loss  0.3816 valid loss 0.004 and accuracy 0.7077\n",
      "Epoch 6 train loss  0.3492 valid loss 0.005 and accuracy 0.6915\n",
      "Epoch 7 train loss  0.3077 valid loss 0.005 and accuracy 0.7033\n",
      "Epoch 8 train loss  0.2803 valid loss 0.005 and accuracy 0.6579\n",
      "Epoch 9 train loss  0.2766 valid loss 0.006 and accuracy 0.7068\n",
      "Epoch 10 train loss  0.2559 valid loss 0.006 and accuracy 0.7059\n",
      "Epoch 11 train loss  0.2371 valid loss 0.007 and accuracy 0.7153\n",
      "Epoch 12 train loss  0.2365 valid loss 0.007 and accuracy 0.7015\n",
      "Epoch 13 train loss  0.2183 valid loss 0.006 and accuracy 0.6868\n",
      "Epoch 14 train loss  0.2035 valid loss 0.007 and accuracy 0.6903\n",
      "Epoch 15 train loss  0.2087 valid loss 0.007 and accuracy 0.6994\n",
      "Epoch 16 train loss  0.2031 valid loss 0.007 and accuracy 0.6972\n",
      "Epoch 17 train loss  0.1882 valid loss 0.007 and accuracy 0.7086\n",
      "Epoch 18 train loss  0.1833 valid loss 0.008 and accuracy 0.6754\n",
      "Epoch 19 train loss  0.1877 valid loss 0.007 and accuracy 0.6939\n",
      "Epoch 20 train loss  0.1835 valid loss 0.008 and accuracy 0.6983\n",
      "Epoch 21 train loss  0.1791 valid loss 0.007 and accuracy 0.6743\n",
      "Epoch 22 train loss  0.1761 valid loss 0.007 and accuracy 0.6732\n",
      "Epoch 23 train loss  0.1788 valid loss 0.007 and accuracy 0.7103\n",
      "Epoch 24 train loss  0.1717 valid loss 0.007 and accuracy 0.7026\n",
      "Epoch 25 train loss  0.1699 valid loss 0.007 and accuracy 0.6930\n",
      "Epoch 26 train loss  0.1597 valid loss 0.007 and accuracy 0.6700\n",
      "Epoch 27 train loss  0.1664 valid loss 0.008 and accuracy 0.6842\n",
      "Epoch 28 train loss  0.1662 valid loss 0.007 and accuracy 0.6864\n",
      "Epoch 29 train loss  0.1683 valid loss 0.007 and accuracy 0.6835\n",
      "Epoch 30 train loss  0.1599 valid loss 0.008 and accuracy 0.6950\n",
      "Epoch 31 train loss  0.1550 valid loss 0.007 and accuracy 0.6910\n",
      "Epoch 32 train loss  0.1624 valid loss 0.007 and accuracy 0.7044\n",
      "Epoch 33 train loss  0.1598 valid loss 0.007 and accuracy 0.7064\n",
      "Epoch 34 train loss  0.1620 valid loss 0.008 and accuracy 0.6939\n",
      "Epoch 35 train loss  0.1542 valid loss 0.006 and accuracy 0.6919\n",
      "Epoch 36 train loss  0.1546 valid loss 0.008 and accuracy 0.6686\n",
      "Epoch 37 train loss  0.1541 valid loss 0.007 and accuracy 0.6816\n",
      "Epoch 38 train loss  0.1532 valid loss 0.007 and accuracy 0.6781\n",
      "Epoch 39 train loss  0.1499 valid loss 0.007 and accuracy 0.6881\n",
      "Epoch 40 train loss  0.1540 valid loss 0.007 and accuracy 0.6947\n",
      "Epoch 41 train loss  0.1495 valid loss 0.007 and accuracy 0.7064\n",
      "Epoch 42 train loss  0.1500 valid loss 0.007 and accuracy 0.7004\n",
      "Epoch 43 train loss  0.1504 valid loss 0.007 and accuracy 0.6993\n",
      "Epoch 44 train loss  0.1457 valid loss 0.009 and accuracy 0.7105\n",
      "Epoch 45 train loss  0.1474 valid loss 0.008 and accuracy 0.6914\n",
      "Epoch 46 train loss  0.1476 valid loss 0.008 and accuracy 0.7000\n",
      "Epoch 47 train loss  0.1463 valid loss 0.008 and accuracy 0.6869\n",
      "Epoch 48 train loss  0.1417 valid loss 0.008 and accuracy 0.6629\n",
      "Epoch 49 train loss  0.1439 valid loss 0.007 and accuracy 0.7035\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: biology\n",
      "accuracy: tensor([0.2655]), points: 14\n",
      "----------\n",
      "TEST Dominio: biology\n",
      "accuracy: tensor([0.2577]), points: 14\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5018 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4972 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.4877 valid loss 0.003 and accuracy 0.7487\n",
      "Epoch 3 train loss  0.4633 valid loss 0.004 and accuracy 0.7272\n",
      "Epoch 4 train loss  0.4308 valid loss 0.004 and accuracy 0.6982\n",
      "Epoch 5 train loss  0.3933 valid loss 0.004 and accuracy 0.6768\n",
      "Epoch 6 train loss  0.3573 valid loss 0.004 and accuracy 0.6888\n",
      "Epoch 7 train loss  0.3251 valid loss 0.004 and accuracy 0.7063\n",
      "Epoch 8 train loss  0.2972 valid loss 0.005 and accuracy 0.6522\n",
      "Epoch 9 train loss  0.2880 valid loss 0.004 and accuracy 0.6847\n",
      "Epoch 10 train loss  0.2664 valid loss 0.004 and accuracy 0.6879\n",
      "Epoch 11 train loss  0.2568 valid loss 0.004 and accuracy 0.6741\n",
      "Epoch 12 train loss  0.2373 valid loss 0.004 and accuracy 0.6432\n",
      "Epoch 13 train loss  0.2275 valid loss 0.005 and accuracy 0.6632\n",
      "Epoch 14 train loss  0.2143 valid loss 0.005 and accuracy 0.6831\n",
      "Epoch 15 train loss  0.2121 valid loss 0.006 and accuracy 0.7230\n",
      "Epoch 16 train loss  0.1992 valid loss 0.006 and accuracy 0.7116\n",
      "Epoch 17 train loss  0.1890 valid loss 0.007 and accuracy 0.7268\n",
      "Epoch 18 train loss  0.1920 valid loss 0.005 and accuracy 0.6985\n",
      "Epoch 19 train loss  0.1946 valid loss 0.006 and accuracy 0.6733\n",
      "Epoch 20 train loss  0.1841 valid loss 0.006 and accuracy 0.6702\n",
      "Epoch 21 train loss  0.1849 valid loss 0.007 and accuracy 0.6765\n",
      "Epoch 22 train loss  0.1816 valid loss 0.005 and accuracy 0.6691\n",
      "Epoch 23 train loss  0.1737 valid loss 0.006 and accuracy 0.6654\n",
      "Epoch 24 train loss  0.1682 valid loss 0.005 and accuracy 0.6548\n",
      "Epoch 25 train loss  0.1678 valid loss 0.007 and accuracy 0.6805\n",
      "Epoch 26 train loss  0.1711 valid loss 0.007 and accuracy 0.7051\n",
      "Epoch 27 train loss  0.1694 valid loss 0.008 and accuracy 0.6965\n",
      "Epoch 28 train loss  0.1606 valid loss 0.006 and accuracy 0.6645\n",
      "Epoch 29 train loss  0.1678 valid loss 0.006 and accuracy 0.6895\n",
      "Epoch 30 train loss  0.1635 valid loss 0.006 and accuracy 0.6868\n",
      "Epoch 31 train loss  0.1597 valid loss 0.006 and accuracy 0.7028\n",
      "Epoch 32 train loss  0.1605 valid loss 0.006 and accuracy 0.6982\n",
      "Epoch 33 train loss  0.1560 valid loss 0.006 and accuracy 0.6726\n",
      "Epoch 34 train loss  0.1554 valid loss 0.007 and accuracy 0.7088\n",
      "Epoch 35 train loss  0.1555 valid loss 0.006 and accuracy 0.6829\n",
      "Epoch 36 train loss  0.1569 valid loss 0.007 and accuracy 0.6290\n",
      "Epoch 37 train loss  0.1539 valid loss 0.007 and accuracy 0.6675\n",
      "Epoch 38 train loss  0.1507 valid loss 0.007 and accuracy 0.6950\n",
      "Epoch 39 train loss  0.1502 valid loss 0.006 and accuracy 0.6702\n",
      "Epoch 40 train loss  0.1553 valid loss 0.005 and accuracy 0.6434\n",
      "Epoch 41 train loss  0.1528 valid loss 0.007 and accuracy 0.6972\n",
      "Epoch 42 train loss  0.1477 valid loss 0.006 and accuracy 0.6884\n",
      "Epoch 43 train loss  0.1531 valid loss 0.006 and accuracy 0.6774\n",
      "Epoch 44 train loss  0.1453 valid loss 0.006 and accuracy 0.6945\n",
      "Epoch 45 train loss  0.1426 valid loss 0.007 and accuracy 0.6982\n",
      "Epoch 46 train loss  0.1452 valid loss 0.006 and accuracy 0.6882\n",
      "Epoch 47 train loss  0.1463 valid loss 0.006 and accuracy 0.6991\n",
      "Epoch 48 train loss  0.1473 valid loss 0.006 and accuracy 0.6888\n",
      "Epoch 49 train loss  0.1440 valid loss 0.005 and accuracy 0.6868\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: biology\n",
      "accuracy: tensor([0.3230]), points: 66\n",
      "----------\n",
      "TEST Dominio: biology\n",
      "accuracy: tensor([0.2335]), points: -30\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-seeker",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "civilian-concert",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV\n",
      "Accuracy media 0.2300885\n",
      "Puntos media -18.0\n",
      "[tensor(0.2301)]\n",
      "[-18]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25326037\n",
      "Puntos media 3.0\n",
      "[tensor(0.2632), tensor(0.2434)]\n",
      "[12, -6]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.30088496\n",
      "Puntos media 46.0\n",
      "[tensor(0.3009)]\n",
      "[46]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2796538\n",
      "Puntos media 27.0\n",
      "[tensor(0.2982), tensor(0.2611)]\n",
      "[44, 10]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.29646018\n",
      "Puntos media 42.0\n",
      "[tensor(0.2965)]\n",
      "[42]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24670082\n",
      "Puntos media -3.0\n",
      "[tensor(0.2456), tensor(0.2478)]\n",
      "[-4, -2]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [f'trained_models_v2/logistic_regressor_{CATEGORY}', \n",
    "         f'trained_models_v2/lstm_{CATEGORY}',         \n",
    "         f'trained_models_v2/bilstm_{CATEGORY}']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "spread-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.26548672\n",
      "Puntos media 14.0\n",
      "[tensor(0.2655)]\n",
      "[14]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2578016\n",
      "Puntos media 7.0\n",
      "[tensor(0.2368), tensor(0.2788)]\n",
      "[-12, 26]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.32300884\n",
      "Puntos media 66.0\n",
      "[tensor(0.3230)]\n",
      "[66]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23360115\n",
      "Puntos media -15.0\n",
      "[tensor(0.2061), tensor(0.2611)]\n",
      "[-40, 10]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [f'trained_models_v2/lstm_qa_{CATEGORY}',\n",
    "         f'trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-payroll",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
