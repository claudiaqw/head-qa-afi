{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from training import train, validate, evaluate, evaluate_better, make_embedding_matrix, make_embedding_matrix, evaluator, evaluator_ir\n",
    "from training import get_optimizer\n",
    "\n",
    "from supervised_models import BiLSTM_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=oversampled_training, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3214 valid loss 0.455 and accuracy 0.2500\n",
      "Epoch 1 train loss  50.0188 valid loss 0.455 and accuracy 0.2500\n",
      "Epoch 2 train loss  50.0188 valid loss 0.455 and accuracy 0.2500\n",
      "Epoch 3 train loss  50.0188 valid loss 0.455 and accuracy 0.2500\n",
      "Epoch 4 train loss  22.4972 valid loss 0.052 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.3729 valid loss 0.043 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.3801 valid loss 0.044 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.3943 valid loss 0.044 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.4127 valid loss 0.046 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.4573 valid loss 0.043 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.4597 valid loss 0.044 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.4833 valid loss 0.049 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.4711 valid loss 0.047 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.4544 valid loss 0.047 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.4586 valid loss 0.067 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.4870 valid loss 0.067 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.4894 valid loss 0.051 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.4822 valid loss 0.049 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.4463 valid loss 0.054 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.4499 valid loss 0.053 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.4706 valid loss 0.044 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.4868 valid loss 0.053 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.4802 valid loss 0.056 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.5059 valid loss 0.092 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.4429 valid loss 0.037 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.4446 valid loss 0.046 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.4705 valid loss 0.092 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.4502 valid loss 0.057 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.5024 valid loss 0.064 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.4901 valid loss 0.100 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6033 valid loss 0.056 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.4423 valid loss 0.053 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.4712 valid loss 0.059 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.4593 valid loss 0.047 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.4698 valid loss 0.049 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.4498 valid loss 0.056 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.5006 valid loss 0.052 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.4706 valid loss 0.261 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.4769 valid loss 0.047 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.4810 valid loss 0.063 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.5404 valid loss 0.053 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.4647 valid loss 0.113 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.4856 valid loss 0.045 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.4814 valid loss 0.071 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.5296 valid loss 0.053 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.4674 valid loss 0.059 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.5332 valid loss 0.055 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.4736 valid loss 0.057 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.4716 valid loss 0.064 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.5536 valid loss 0.167 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2291]), -114)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2480]), -22)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_bilstm.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_bilstm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/bilstm'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_model(\n",
       "  (emb): Embedding(20403, 300, padding_idx=0)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(300, 64, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (attn): Linear(in_features=128, out_features=30, bias=True)\n",
       "  (linear): Linear(in_features=3840, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22909921,\n",
       " -19.0,\n",
       " [tensor(0.2389),\n",
       "  tensor(0.2043),\n",
       "  tensor(0.2044),\n",
       "  tensor(0.2597),\n",
       "  tensor(0.2522),\n",
       "  tensor(0.2149)],\n",
       " [-10, -42, -41, 9, 2, -32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24795495,\n",
       " -1.8333333333333333,\n",
       " [tensor(0.2719),\n",
       "  tensor(0.2377),\n",
       "  tensor(0.2149),\n",
       "  tensor(0.2414),\n",
       "  tensor(0.2391),\n",
       "  tensor(0.2727),\n",
       "  tensor(0.2389),\n",
       "  tensor(0.2543),\n",
       "  tensor(0.2576),\n",
       "  tensor(0.2468),\n",
       "  tensor(0.2578),\n",
       "  tensor(0.2423)],\n",
       " [20, -11, -32, -8, -10, 21, -10, 4, 7, -3, 7, -7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
