{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from training import train, validate, evaluate, make_embedding_matrix, make_embedding_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)\n",
    "\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_model(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes, hidden_size=64,\n",
    "                 pretrained_embeddings=None, padding_idx=0, max_length = 110):\n",
    "        super(BiLSTM_model, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size,num_embeddings=self.num_embeddings,\n",
    "                                    padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size, num_embeddings=self.num_embeddings,\n",
    "                                    padding_idx=padding_idx, _weight=pretrained_embeddings)\n",
    "            self.emb.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.3)            \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True, dropout = 0.5,bidirectional = True)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.linear = nn.Linear(self.hidden_size*2*self.max_length, num_classes) \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        out, (ht, ct) = self.lstm(x)\n",
    "        attn = self.attn(out)\n",
    "        attn_weights = F.softmax(torch.tanh(attn), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights, out)\n",
    "        attn_applied = attn_applied.flatten(1) \n",
    "        return F.softmax(self.linear(attn_applied), dim = 0)\n",
    "        #return self.linear(attn_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.7193 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 1 train loss  0.7189 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 2 train loss  0.7179 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 3 train loss  0.7168 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 4 train loss  0.7148 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 5 train loss  0.7123 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 6 train loss  0.7093 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 7 train loss  0.7028 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 8 train loss  0.6988 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 9 train loss  0.6910 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 10 train loss  0.6826 valid loss 0.005 and accuracy 24.0000\n",
      "Epoch 11 train loss  0.6744 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 12 train loss  0.6623 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 13 train loss  0.6541 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 14 train loss  0.6438 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 15 train loss  0.6371 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 16 train loss  0.6293 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 17 train loss  0.6188 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 18 train loss  0.6125 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 19 train loss  0.6034 valid loss 0.006 and accuracy 24.0000\n",
      "Epoch 20 train loss  0.5980 valid loss 0.007 and accuracy 24.0000\n",
      "Epoch 21 train loss  0.5940 valid loss 0.007 and accuracy 24.0000\n",
      "Epoch 22 train loss  0.5881 valid loss 0.007 and accuracy 24.0000\n",
      "Epoch 23 train loss  0.5847 valid loss 0.007 and accuracy 24.0000\n",
      "Epoch 24 train loss  0.5766 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 25 train loss  0.5719 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 26 train loss  0.5704 valid loss 0.007 and accuracy 24.0000\n",
      "Epoch 27 train loss  0.5688 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 28 train loss  0.5653 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 29 train loss  0.5624 valid loss 0.007 and accuracy 24.0000\n",
      "Epoch 30 train loss  0.5590 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 31 train loss  0.5575 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 32 train loss  0.5539 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 33 train loss  0.5487 valid loss 0.009 and accuracy 24.0000\n",
      "Epoch 34 train loss  0.5503 valid loss 0.008 and accuracy 24.0000\n",
      "Epoch 35 train loss  0.5468 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 36 train loss  0.5460 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 37 train loss  0.5390 valid loss 0.011 and accuracy 24.0000\n",
      "Epoch 38 train loss  0.5380 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 39 train loss  0.5394 valid loss 0.009 and accuracy 24.0000\n",
      "Epoch 40 train loss  0.5352 valid loss 0.011 and accuracy 24.0000\n",
      "Epoch 41 train loss  0.5342 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 42 train loss  0.5314 valid loss 0.011 and accuracy 24.0000\n",
      "Epoch 43 train loss  0.5328 valid loss 0.009 and accuracy 24.0000\n",
      "Epoch 44 train loss  0.5258 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 45 train loss  0.5269 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 46 train loss  0.5274 valid loss 0.011 and accuracy 24.0000\n",
      "Epoch 47 train loss  0.5259 valid loss 0.010 and accuracy 24.0000\n",
      "Epoch 48 train loss  0.5230 valid loss 0.011 and accuracy 24.0000\n",
      "Epoch 49 train loss  0.5224 valid loss 0.011 and accuracy 24.0000\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2467]), -18)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2746]), 270)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/bilstm'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
