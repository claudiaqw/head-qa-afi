{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from training import train, validate, evaluate, evaluate_better, make_embedding_matrix, make_embedding_matrix, evaluator, evaluator_ir\n",
    "from training import get_optimizer\n",
    "\n",
    "from supervised_models import BiLSTM_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)\n",
    "\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-30e1431d62a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n\u001b[0;32m      2\u001b[0m                      pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5028 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.5005 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.5004 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.4996 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.4986 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 5 train loss  0.4956 valid loss 0.003 and accuracy 0.7498\n",
      "Epoch 6 train loss  0.4929 valid loss 0.003 and accuracy 0.7493\n",
      "Epoch 7 train loss  0.4889 valid loss 0.003 and accuracy 0.7454\n",
      "Epoch 8 train loss  0.4820 valid loss 0.004 and accuracy 0.7460\n",
      "Epoch 9 train loss  0.4759 valid loss 0.003 and accuracy 0.7438\n",
      "Epoch 10 train loss  0.4667 valid loss 0.003 and accuracy 0.7375\n",
      "Epoch 11 train loss  0.4525 valid loss 0.004 and accuracy 0.7320\n",
      "Epoch 12 train loss  0.4433 valid loss 0.004 and accuracy 0.6991\n",
      "Epoch 13 train loss  0.4274 valid loss 0.004 and accuracy 0.7399\n",
      "Epoch 14 train loss  0.4130 valid loss 0.004 and accuracy 0.7053\n",
      "Epoch 15 train loss  0.4011 valid loss 0.004 and accuracy 0.7257\n",
      "Epoch 16 train loss  0.3871 valid loss 0.004 and accuracy 0.6956\n",
      "Epoch 17 train loss  0.3811 valid loss 0.004 and accuracy 0.7180\n",
      "Epoch 18 train loss  0.3681 valid loss 0.004 and accuracy 0.7151\n",
      "Epoch 19 train loss  0.3552 valid loss 0.005 and accuracy 0.7333\n",
      "Epoch 20 train loss  0.3403 valid loss 0.005 and accuracy 0.7397\n",
      "Epoch 21 train loss  0.3353 valid loss 0.006 and accuracy 0.7307\n",
      "Epoch 22 train loss  0.3339 valid loss 0.005 and accuracy 0.7301\n",
      "Epoch 23 train loss  0.3157 valid loss 0.006 and accuracy 0.7309\n",
      "Epoch 24 train loss  0.3050 valid loss 0.006 and accuracy 0.7368\n",
      "Epoch 25 train loss  0.3002 valid loss 0.006 and accuracy 0.7305\n",
      "Epoch 26 train loss  0.2962 valid loss 0.006 and accuracy 0.7296\n",
      "Epoch 27 train loss  0.2916 valid loss 0.005 and accuracy 0.7254\n",
      "Epoch 28 train loss  0.2826 valid loss 0.006 and accuracy 0.7092\n",
      "Epoch 29 train loss  0.2723 valid loss 0.005 and accuracy 0.7202\n",
      "Epoch 30 train loss  0.2692 valid loss 0.006 and accuracy 0.7195\n",
      "Epoch 31 train loss  0.2603 valid loss 0.006 and accuracy 0.7171\n",
      "Epoch 32 train loss  0.2620 valid loss 0.006 and accuracy 0.7257\n",
      "Epoch 33 train loss  0.2544 valid loss 0.007 and accuracy 0.6950\n",
      "Epoch 34 train loss  0.2514 valid loss 0.007 and accuracy 0.7272\n",
      "Epoch 35 train loss  0.2505 valid loss 0.007 and accuracy 0.7204\n",
      "Epoch 36 train loss  0.2443 valid loss 0.006 and accuracy 0.7171\n",
      "Epoch 37 train loss  0.2380 valid loss 0.006 and accuracy 0.6855\n",
      "Epoch 38 train loss  0.2461 valid loss 0.006 and accuracy 0.6972\n",
      "Epoch 39 train loss  0.2358 valid loss 0.007 and accuracy 0.6829\n",
      "Epoch 40 train loss  0.2294 valid loss 0.006 and accuracy 0.6989\n",
      "Epoch 41 train loss  0.2285 valid loss 0.006 and accuracy 0.6952\n",
      "Epoch 42 train loss  0.2269 valid loss 0.006 and accuracy 0.6713\n",
      "Epoch 43 train loss  0.2270 valid loss 0.007 and accuracy 0.6748\n",
      "Epoch 44 train loss  0.2237 valid loss 0.007 and accuracy 0.6732\n",
      "Epoch 45 train loss  0.2132 valid loss 0.007 and accuracy 0.6807\n",
      "Epoch 46 train loss  0.2218 valid loss 0.008 and accuracy 0.6623\n",
      "Epoch 47 train loss  0.2149 valid loss 0.008 and accuracy 0.6903\n",
      "Epoch 48 train loss  0.2171 valid loss 0.008 and accuracy 0.6884\n",
      "Epoch 49 train loss  0.2092 valid loss 0.007 and accuracy 0.6917\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2482]), -10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2659]), 174)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_bilstm_sig.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_bilstm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/bilstm_sig'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_model(\n",
       "  (emb): Embedding(20403, 300, padding_idx=0)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(300, 64, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (attn): Linear(in_features=128, out_features=30, bias=True)\n",
       "  (linear): Linear(in_features=3840, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "model.load_state_dict(torch.load(os.getcwd() + '/trained_models/bilstm_sig'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.24831116,\n",
       " -1.6666666666666667,\n",
       " [tensor(0.2920),\n",
       "  tensor(0.1957),\n",
       "  tensor(0.2222),\n",
       "  tensor(0.2424),\n",
       "  tensor(0.2788),\n",
       "  tensor(0.2588)],\n",
       " [38, -50, -25, -7, 26, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2658792,\n",
       " 14.5,\n",
       " [tensor(0.2763),\n",
       "  tensor(0.2287),\n",
       "  tensor(0.2544),\n",
       "  tensor(0.2284),\n",
       "  tensor(0.2565),\n",
       "  tensor(0.2987),\n",
       "  tensor(0.2522),\n",
       "  tensor(0.2802),\n",
       "  tensor(0.2751),\n",
       "  tensor(0.2165),\n",
       "  tensor(0.2711),\n",
       "  tensor(0.3524)],\n",
       " [24, -19, 4, -20, 6, 45, 2, 28, 23, -31, 19, 93])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
