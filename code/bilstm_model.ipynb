{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from training import train, validate, evaluate, make_embedding_matrix, make_embedding_matrix, evaluator, evaluator_ir\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)\n",
    "\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_model(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes, hidden_size=64,\n",
    "                 pretrained_embeddings=None, padding_idx=0, max_length = 110):\n",
    "        super(BiLSTM_model, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size,num_embeddings=self.num_embeddings,\n",
    "                                    padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size, num_embeddings=self.num_embeddings,\n",
    "                                    padding_idx=padding_idx, _weight=pretrained_embeddings)\n",
    "            self.emb.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.3)            \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True, dropout = 0.5,bidirectional = True)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.linear = nn.Linear(self.hidden_size*2*self.max_length, num_classes) \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        out, (ht, ct) = self.lstm(x)\n",
    "        attn = self.attn(out)\n",
    "        attn_weights = F.softmax(torch.tanh(attn), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights, out)\n",
    "        attn_applied = attn_applied.flatten(1) \n",
    "        return F.softmax(self.linear(attn_applied), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.7191 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.7182 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.7177 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.7161 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.7143 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 5 train loss  0.7113 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 6 train loss  0.7065 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 7 train loss  0.7032 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 8 train loss  0.6982 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 9 train loss  0.6897 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 10 train loss  0.6806 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 11 train loss  0.6689 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 12 train loss  0.6632 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 13 train loss  0.6516 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 14 train loss  0.6425 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 15 train loss  0.6328 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 16 train loss  0.6277 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 17 train loss  0.6228 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 18 train loss  0.6158 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 19 train loss  0.6067 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 20 train loss  0.6016 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 21 train loss  0.5956 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 22 train loss  0.5927 valid loss 0.006 and accuracy 0.7496\n",
      "Epoch 23 train loss  0.5884 valid loss 0.006 and accuracy 0.7506\n",
      "Epoch 24 train loss  0.5814 valid loss 0.006 and accuracy 0.7498\n",
      "Epoch 25 train loss  0.5768 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 26 train loss  0.5770 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 27 train loss  0.5727 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 28 train loss  0.5688 valid loss 0.006 and accuracy 0.7504\n",
      "Epoch 29 train loss  0.5675 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 30 train loss  0.5625 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 31 train loss  0.5633 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 32 train loss  0.5580 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 33 train loss  0.5548 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 34 train loss  0.5531 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 35 train loss  0.5481 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 36 train loss  0.5487 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 37 train loss  0.5453 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 38 train loss  0.5443 valid loss 0.008 and accuracy 0.7494\n",
      "Epoch 39 train loss  0.5418 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 40 train loss  0.5389 valid loss 0.008 and accuracy 0.7496\n",
      "Epoch 41 train loss  0.5350 valid loss 0.008 and accuracy 0.7491\n",
      "Epoch 42 train loss  0.5347 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 43 train loss  0.5353 valid loss 0.008 and accuracy 0.7502\n",
      "Epoch 44 train loss  0.5302 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 45 train loss  0.5312 valid loss 0.008 and accuracy 0.7502\n",
      "Epoch 46 train loss  0.5300 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 47 train loss  0.5261 valid loss 0.008 and accuracy 0.7502\n",
      "Epoch 48 train loss  0.5263 valid loss 0.008 and accuracy 0.7502\n",
      "Epoch 49 train loss  0.5249 valid loss 0.009 and accuracy 0.7498\n",
      "Epoch 50 train loss  0.5216 valid loss 0.009 and accuracy 0.7494\n",
      "Epoch 51 train loss  0.5244 valid loss 0.008 and accuracy 0.7496\n",
      "Epoch 52 train loss  0.5223 valid loss 0.008 and accuracy 0.7502\n",
      "Epoch 53 train loss  0.5170 valid loss 0.008 and accuracy 0.7504\n",
      "Epoch 54 train loss  0.5202 valid loss 0.009 and accuracy 0.7506\n",
      "Epoch 55 train loss  0.5196 valid loss 0.009 and accuracy 0.7504\n",
      "Epoch 56 train loss  0.5187 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 57 train loss  0.5139 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 58 train loss  0.5125 valid loss 0.009 and accuracy 0.7504\n",
      "Epoch 59 train loss  0.5113 valid loss 0.009 and accuracy 0.7496\n",
      "Epoch 60 train loss  0.5151 valid loss 0.009 and accuracy 0.7506\n",
      "Epoch 61 train loss  0.5149 valid loss 0.008 and accuracy 0.7496\n",
      "Epoch 62 train loss  0.5090 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 63 train loss  0.5093 valid loss 0.009 and accuracy 0.7496\n",
      "Epoch 64 train loss  0.5113 valid loss 0.009 and accuracy 0.7494\n",
      "Epoch 65 train loss  0.5099 valid loss 0.008 and accuracy 0.7498\n",
      "Epoch 66 train loss  0.5064 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 67 train loss  0.5079 valid loss 0.009 and accuracy 0.7500\n",
      "Epoch 68 train loss  0.5054 valid loss 0.009 and accuracy 0.7502\n",
      "Epoch 69 train loss  0.5051 valid loss 0.009 and accuracy 0.7502\n",
      "Epoch 70 train loss  0.5055 valid loss 0.010 and accuracy 0.7494\n",
      "Epoch 71 train loss  0.5035 valid loss 0.009 and accuracy 0.7489\n",
      "Epoch 72 train loss  0.5035 valid loss 0.010 and accuracy 0.7502\n",
      "Epoch 73 train loss  0.5030 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 74 train loss  0.5038 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 75 train loss  0.5009 valid loss 0.010 and accuracy 0.7498\n",
      "Epoch 76 train loss  0.5013 valid loss 0.010 and accuracy 0.7496\n",
      "Epoch 77 train loss  0.5062 valid loss 0.010 and accuracy 0.7500\n",
      "Epoch 78 train loss  0.5009 valid loss 0.009 and accuracy 0.7498\n",
      "Epoch 79 train loss  0.4999 valid loss 0.009 and accuracy 0.7494\n",
      "Epoch 80 train loss  0.5005 valid loss 0.008 and accuracy 0.7491\n",
      "Epoch 81 train loss  0.5007 valid loss 0.009 and accuracy 0.7483\n",
      "Epoch 82 train loss  0.5005 valid loss 0.010 and accuracy 0.7491\n",
      "Epoch 83 train loss  0.4993 valid loss 0.009 and accuracy 0.7498\n",
      "Epoch 84 train loss  0.4955 valid loss 0.009 and accuracy 0.7498\n",
      "Epoch 85 train loss  0.4969 valid loss 0.011 and accuracy 0.7476\n",
      "Epoch 86 train loss  0.4984 valid loss 0.010 and accuracy 0.7480\n",
      "Epoch 87 train loss  0.4949 valid loss 0.012 and accuracy 0.7491\n",
      "Epoch 88 train loss  0.4952 valid loss 0.010 and accuracy 0.7491\n",
      "Epoch 89 train loss  0.4943 valid loss 0.011 and accuracy 0.7494\n",
      "Epoch 90 train loss  0.4936 valid loss 0.010 and accuracy 0.7487\n",
      "Epoch 91 train loss  0.4954 valid loss 0.012 and accuracy 0.7487\n",
      "Epoch 92 train loss  0.4945 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 93 train loss  0.4930 valid loss 0.011 and accuracy 0.7500\n",
      "Epoch 94 train loss  0.4943 valid loss 0.011 and accuracy 0.7494\n",
      "Epoch 95 train loss  0.4939 valid loss 0.011 and accuracy 0.7494\n",
      "Epoch 96 train loss  0.4934 valid loss 0.011 and accuracy 0.7504\n",
      "Epoch 97 train loss  0.4932 valid loss 0.010 and accuracy 0.7498\n",
      "Epoch 98 train loss  0.4930 valid loss 0.010 and accuracy 0.7493\n",
      "Epoch 99 train loss  0.4932 valid loss 0.010 and accuracy 0.7494\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2218]), -154)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2713]), 234)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_bilstm.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_bilstm.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/bilstm'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
