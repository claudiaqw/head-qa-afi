{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'medicine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  49.5839 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 1 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 2 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 3 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 4 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 5 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 6 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 7 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 8 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 9 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 10 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 11 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 12 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 13 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 14 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 15 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 16 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 17 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 18 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 19 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 20 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 21 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 22 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 23 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.7489]), points: 461\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.7559]), points: 937\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6859 valid loss 0.026 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.7086 valid loss 0.026 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.7003 valid loss 0.026 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.6972 valid loss 0.026 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.6966 valid loss 0.026 and accuracy 0.7500\n",
      "Epoch 5 train loss  0.6952 valid loss 0.026 and accuracy 0.7500\n",
      "Epoch 6 train loss  0.6952 valid loss 0.025 and accuracy 0.7500\n",
      "Epoch 7 train loss  0.6932 valid loss 0.025 and accuracy 0.7500\n",
      "Epoch 8 train loss  0.6892 valid loss 0.025 and accuracy 0.7444\n",
      "Epoch 9 train loss  0.6873 valid loss 0.025 and accuracy 0.7444\n",
      "Epoch 10 train loss  0.6750 valid loss 0.025 and accuracy 0.7333\n",
      "Epoch 11 train loss  0.6572 valid loss 0.025 and accuracy 0.7121\n",
      "Epoch 12 train loss  0.6265 valid loss 0.025 and accuracy 0.6931\n",
      "Epoch 13 train loss  0.6007 valid loss 0.025 and accuracy 0.6920\n",
      "Epoch 14 train loss  0.5746 valid loss 0.025 and accuracy 0.6942\n",
      "Epoch 15 train loss  0.4957 valid loss 0.101 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7093 valid loss 0.026 and accuracy 0.6797\n",
      "Epoch 17 train loss  0.5665 valid loss 0.024 and accuracy 0.6908\n",
      "Epoch 18 train loss  0.5399 valid loss 0.023 and accuracy 0.7054\n",
      "Epoch 19 train loss  0.5226 valid loss 0.023 and accuracy 0.7042\n",
      "Epoch 20 train loss  0.5135 valid loss 0.024 and accuracy 0.7031\n",
      "Epoch 21 train loss  0.5110 valid loss 0.027 and accuracy 0.6261\n",
      "Epoch 22 train loss  0.5199 valid loss 0.024 and accuracy 0.7020\n",
      "Epoch 23 train loss  0.4922 valid loss 0.024 and accuracy 0.7087\n",
      "Epoch 24 train loss  0.4870 valid loss 0.025 and accuracy 0.7087\n",
      "Epoch 25 train loss  0.4776 valid loss 0.025 and accuracy 0.7087\n",
      "Epoch 26 train loss  0.4719 valid loss 0.025 and accuracy 0.7042\n",
      "Epoch 27 train loss  0.4648 valid loss 0.025 and accuracy 0.7031\n",
      "Epoch 28 train loss  0.4610 valid loss 0.026 and accuracy 0.7009\n",
      "Epoch 29 train loss  0.4571 valid loss 0.026 and accuracy 0.7087\n",
      "Epoch 30 train loss  0.4539 valid loss 0.026 and accuracy 0.6942\n",
      "Epoch 31 train loss  0.4483 valid loss 0.027 and accuracy 0.6886\n",
      "Epoch 32 train loss  0.4477 valid loss 0.027 and accuracy 0.6819\n",
      "Epoch 33 train loss  0.4429 valid loss 0.028 and accuracy 0.6975\n",
      "Epoch 34 train loss  0.4426 valid loss 0.029 and accuracy 0.6953\n",
      "Epoch 35 train loss  0.4424 valid loss 0.029 and accuracy 0.6998\n",
      "Epoch 36 train loss  0.4390 valid loss 0.029 and accuracy 0.6964\n",
      "Epoch 37 train loss  0.4375 valid loss 0.028 and accuracy 0.7042\n",
      "Epoch 38 train loss  0.4376 valid loss 0.028 and accuracy 0.6931\n",
      "Epoch 39 train loss  0.4344 valid loss 0.029 and accuracy 0.6987\n",
      "Epoch 40 train loss  0.4357 valid loss 0.029 and accuracy 0.7020\n",
      "Epoch 41 train loss  0.4370 valid loss 0.030 and accuracy 0.6920\n",
      "Epoch 42 train loss  0.4304 valid loss 0.030 and accuracy 0.6875\n",
      "Epoch 43 train loss  0.4355 valid loss 0.029 and accuracy 0.6964\n",
      "Epoch 44 train loss  0.4337 valid loss 0.030 and accuracy 0.6964\n",
      "Epoch 45 train loss  0.4307 valid loss 0.030 and accuracy 0.6987\n",
      "Epoch 46 train loss  0.4330 valid loss 0.032 and accuracy 0.6897\n",
      "Epoch 47 train loss  0.4297 valid loss 0.032 and accuracy 0.6797\n",
      "Epoch 48 train loss  0.4303 valid loss 0.029 and accuracy 0.6975\n",
      "Epoch 49 train loss  0.4273 valid loss 0.029 and accuracy 0.6830\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.6017]), points: 325\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.5961]), points: 641\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3882 valid loss 1.175 and accuracy 0.2500\n",
      "Epoch 1 train loss  5.0803 valid loss 0.995 and accuracy 0.2500\n",
      "Epoch 2 train loss  3.5764 valid loss 0.635 and accuracy 0.2500\n",
      "Epoch 3 train loss  1.8625 valid loss 0.361 and accuracy 0.2500\n",
      "Epoch 4 train loss  4.7184 valid loss 0.415 and accuracy 0.2500\n",
      "Epoch 5 train loss  1.2986 valid loss 0.271 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.9460 valid loss 0.322 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.9120 valid loss 0.283 and accuracy 0.2500\n",
      "Epoch 8 train loss  1.0054 valid loss 0.098 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7150 valid loss 0.385 and accuracy 0.2500\n",
      "Epoch 10 train loss  1.1712 valid loss 0.310 and accuracy 0.2500\n",
      "Epoch 11 train loss  1.0082 valid loss 0.349 and accuracy 0.2500\n",
      "Epoch 12 train loss  1.2337 valid loss 0.123 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7399 valid loss 0.295 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.7538 valid loss 0.390 and accuracy 0.2500\n",
      "Epoch 15 train loss  1.1738 valid loss 0.717 and accuracy 0.2500\n",
      "Epoch 16 train loss  2.2092 valid loss 0.643 and accuracy 0.2500\n",
      "Epoch 17 train loss  1.1833 valid loss 0.637 and accuracy 0.2500\n",
      "Epoch 18 train loss  3.0916 valid loss 0.957 and accuracy 0.2500\n",
      "Epoch 19 train loss  2.7121 valid loss 0.179 and accuracy 0.2500\n",
      "Epoch 20 train loss  3.3050 valid loss 0.876 and accuracy 0.2500\n",
      "Epoch 21 train loss  1.6762 valid loss 0.243 and accuracy 0.2500\n",
      "Epoch 22 train loss  1.0345 valid loss 0.069 and accuracy 0.3906\n",
      "Epoch 23 train loss  1.8770 valid loss 0.622 and accuracy 0.2500\n",
      "Epoch 24 train loss  1.5986 valid loss 0.632 and accuracy 0.2500\n",
      "Epoch 25 train loss  1.2631 valid loss 0.283 and accuracy 0.2500\n",
      "Epoch 26 train loss  1.0627 valid loss 0.252 and accuracy 0.2567\n",
      "Epoch 27 train loss  0.9396 valid loss 0.399 and accuracy 0.2500\n",
      "Epoch 28 train loss  5.1889 valid loss 1.810 and accuracy 0.2500\n",
      "Epoch 29 train loss  40.9716 valid loss 2.162 and accuracy 0.2500\n",
      "Epoch 30 train loss  41.2311 valid loss 2.124 and accuracy 0.2500\n",
      "Epoch 31 train loss  40.3297 valid loss 2.086 and accuracy 0.2500\n",
      "Epoch 32 train loss  39.5561 valid loss 2.051 and accuracy 0.2500\n",
      "Epoch 33 train loss  38.9348 valid loss 2.021 and accuracy 0.2500\n",
      "Epoch 34 train loss  38.4231 valid loss 1.994 and accuracy 0.2500\n",
      "Epoch 35 train loss  38.0004 valid loss 1.970 and accuracy 0.2500\n",
      "Epoch 36 train loss  37.5968 valid loss 1.948 and accuracy 0.2500\n",
      "Epoch 37 train loss  37.2250 valid loss 1.926 and accuracy 0.2500\n",
      "Epoch 38 train loss  36.8523 valid loss 1.905 and accuracy 0.2500\n",
      "Epoch 39 train loss  36.4548 valid loss 1.882 and accuracy 0.2500\n",
      "Epoch 40 train loss  36.0153 valid loss 1.858 and accuracy 0.2500\n",
      "Epoch 41 train loss  35.5404 valid loss 1.833 and accuracy 0.2500\n",
      "Epoch 42 train loss  35.0402 valid loss 1.809 and accuracy 0.2500\n",
      "Epoch 43 train loss  34.5287 valid loss 1.786 and accuracy 0.2500\n",
      "Epoch 44 train loss  34.0232 valid loss 1.762 and accuracy 0.2500\n",
      "Epoch 45 train loss  33.5062 valid loss 1.739 and accuracy 0.2500\n",
      "Epoch 46 train loss  32.9625 valid loss 1.714 and accuracy 0.2500\n",
      "Epoch 47 train loss  32.3750 valid loss 1.684 and accuracy 0.2500\n",
      "Epoch 48 train loss  31.6991 valid loss 1.648 and accuracy 0.2500\n",
      "Epoch 49 train loss  30.8742 valid loss 1.601 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.6320]), points: 353\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.6199]), points: 685\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5015 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4955 valid loss 0.003 and accuracy 0.7502\n",
      "Epoch 2 train loss  0.4701 valid loss 0.004 and accuracy 0.7465\n",
      "Epoch 3 train loss  0.4179 valid loss 0.003 and accuracy 0.6996\n",
      "Epoch 4 train loss  0.3643 valid loss 0.005 and accuracy 0.7031\n",
      "Epoch 5 train loss  0.3063 valid loss 0.005 and accuracy 0.6820\n",
      "Epoch 6 train loss  0.2611 valid loss 0.004 and accuracy 0.6559\n",
      "Epoch 7 train loss  0.2121 valid loss 0.007 and accuracy 0.6960\n",
      "Epoch 8 train loss  0.1595 valid loss 0.008 and accuracy 0.7007\n",
      "Epoch 9 train loss  0.1566 valid loss 0.007 and accuracy 0.6515\n",
      "Epoch 10 train loss  0.1363 valid loss 0.007 and accuracy 0.6399\n",
      "Epoch 11 train loss  0.1202 valid loss 0.006 and accuracy 0.6643\n",
      "Epoch 12 train loss  0.0949 valid loss 0.006 and accuracy 0.6820\n",
      "Epoch 13 train loss  0.0782 valid loss 0.007 and accuracy 0.6511\n",
      "Epoch 14 train loss  0.0643 valid loss 0.007 and accuracy 0.6897\n",
      "Epoch 15 train loss  0.0675 valid loss 0.006 and accuracy 0.6432\n",
      "Epoch 16 train loss  0.0696 valid loss 0.007 and accuracy 0.6717\n",
      "Epoch 17 train loss  0.0515 valid loss 0.007 and accuracy 0.6950\n",
      "Epoch 18 train loss  0.0531 valid loss 0.006 and accuracy 0.6770\n",
      "Epoch 19 train loss  0.0690 valid loss 0.005 and accuracy 0.6697\n",
      "Epoch 20 train loss  0.0505 valid loss 0.006 and accuracy 0.6513\n",
      "Epoch 21 train loss  0.0398 valid loss 0.006 and accuracy 0.6533\n",
      "Epoch 22 train loss  0.0456 valid loss 0.008 and accuracy 0.6700\n",
      "Epoch 23 train loss  0.0514 valid loss 0.008 and accuracy 0.6634\n",
      "Epoch 24 train loss  0.0513 valid loss 0.008 and accuracy 0.6792\n",
      "Epoch 25 train loss  0.0473 valid loss 0.006 and accuracy 0.6283\n",
      "Epoch 26 train loss  0.0431 valid loss 0.009 and accuracy 0.6708\n",
      "Epoch 27 train loss  0.0351 valid loss 0.006 and accuracy 0.6594\n",
      "Epoch 28 train loss  0.0330 valid loss 0.006 and accuracy 0.6706\n",
      "Epoch 29 train loss  0.0369 valid loss 0.007 and accuracy 0.6559\n",
      "Epoch 30 train loss  0.0340 valid loss 0.008 and accuracy 0.6642\n",
      "Epoch 31 train loss  0.0356 valid loss 0.006 and accuracy 0.6719\n",
      "Epoch 32 train loss  0.0341 valid loss 0.007 and accuracy 0.6221\n",
      "Epoch 33 train loss  0.0340 valid loss 0.007 and accuracy 0.6471\n",
      "Epoch 34 train loss  0.0345 valid loss 0.006 and accuracy 0.6261\n",
      "Epoch 35 train loss  0.0310 valid loss 0.009 and accuracy 0.6813\n",
      "Epoch 36 train loss  0.0312 valid loss 0.005 and accuracy 0.6393\n",
      "Epoch 37 train loss  0.0299 valid loss 0.007 and accuracy 0.6787\n",
      "Epoch 38 train loss  0.0239 valid loss 0.004 and accuracy 0.6013\n",
      "Epoch 39 train loss  0.0266 valid loss 0.007 and accuracy 0.6623\n",
      "Epoch 40 train loss  0.0211 valid loss 0.004 and accuracy 0.6421\n",
      "Epoch 41 train loss  0.0165 valid loss 0.004 and accuracy 0.6647\n",
      "Epoch 42 train loss  0.0262 valid loss 0.005 and accuracy 0.6520\n",
      "Epoch 43 train loss  0.0294 valid loss 0.007 and accuracy 0.6945\n",
      "Epoch 44 train loss  0.0394 valid loss 0.004 and accuracy 0.6511\n",
      "Epoch 45 train loss  0.0293 valid loss 0.005 and accuracy 0.6603\n",
      "Epoch 46 train loss  0.0261 valid loss 0.006 and accuracy 0.6763\n",
      "Epoch 47 train loss  0.0244 valid loss 0.008 and accuracy 0.6890\n",
      "Epoch 48 train loss  0.0196 valid loss 0.007 and accuracy 0.6642\n",
      "Epoch 49 train loss  0.0165 valid loss 0.008 and accuracy 0.6647\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2511]), points: 1\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2462]), points: -7\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5017 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4952 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.4746 valid loss 0.004 and accuracy 0.7458\n",
      "Epoch 3 train loss  0.4207 valid loss 0.004 and accuracy 0.7031\n",
      "Epoch 4 train loss  0.3649 valid loss 0.004 and accuracy 0.7145\n",
      "Epoch 5 train loss  0.2930 valid loss 0.005 and accuracy 0.6368\n",
      "Epoch 6 train loss  0.2489 valid loss 0.006 and accuracy 0.7022\n",
      "Epoch 7 train loss  0.2024 valid loss 0.007 and accuracy 0.7026\n",
      "Epoch 8 train loss  0.1730 valid loss 0.007 and accuracy 0.7256\n",
      "Epoch 9 train loss  0.1519 valid loss 0.008 and accuracy 0.7325\n",
      "Epoch 10 train loss  0.1427 valid loss 0.007 and accuracy 0.7057\n",
      "Epoch 11 train loss  0.0933 valid loss 0.006 and accuracy 0.6564\n",
      "Epoch 12 train loss  0.0968 valid loss 0.010 and accuracy 0.7046\n",
      "Epoch 13 train loss  0.0844 valid loss 0.009 and accuracy 0.7072\n",
      "Epoch 14 train loss  0.0739 valid loss 0.008 and accuracy 0.7029\n",
      "Epoch 15 train loss  0.0693 valid loss 0.009 and accuracy 0.7026\n",
      "Epoch 16 train loss  0.0611 valid loss 0.007 and accuracy 0.6888\n",
      "Epoch 17 train loss  0.0567 valid loss 0.010 and accuracy 0.6936\n",
      "Epoch 18 train loss  0.0586 valid loss 0.008 and accuracy 0.6972\n",
      "Epoch 19 train loss  0.0539 valid loss 0.010 and accuracy 0.7042\n",
      "Epoch 20 train loss  0.0468 valid loss 0.008 and accuracy 0.6846\n",
      "Epoch 21 train loss  0.0524 valid loss 0.010 and accuracy 0.7031\n",
      "Epoch 22 train loss  0.0559 valid loss 0.010 and accuracy 0.7079\n",
      "Epoch 23 train loss  0.0475 valid loss 0.011 and accuracy 0.7020\n",
      "Epoch 24 train loss  0.0417 valid loss 0.008 and accuracy 0.6899\n",
      "Epoch 25 train loss  0.0439 valid loss 0.008 and accuracy 0.6831\n",
      "Epoch 26 train loss  0.0379 valid loss 0.009 and accuracy 0.6553\n",
      "Epoch 27 train loss  0.0396 valid loss 0.013 and accuracy 0.7195\n",
      "Epoch 28 train loss  0.0486 valid loss 0.007 and accuracy 0.6768\n",
      "Epoch 29 train loss  0.0324 valid loss 0.010 and accuracy 0.6857\n",
      "Epoch 30 train loss  0.0369 valid loss 0.008 and accuracy 0.6634\n",
      "Epoch 31 train loss  0.0348 valid loss 0.008 and accuracy 0.6781\n",
      "Epoch 32 train loss  0.0330 valid loss 0.009 and accuracy 0.6903\n",
      "Epoch 33 train loss  0.0278 valid loss 0.010 and accuracy 0.7055\n",
      "Epoch 34 train loss  0.0364 valid loss 0.009 and accuracy 0.6792\n",
      "Epoch 35 train loss  0.0334 valid loss 0.012 and accuracy 0.6908\n",
      "Epoch 36 train loss  0.0389 valid loss 0.009 and accuracy 0.7075\n",
      "Epoch 37 train loss  0.0325 valid loss 0.009 and accuracy 0.7006\n",
      "Epoch 38 train loss  0.0302 valid loss 0.008 and accuracy 0.6958\n",
      "Epoch 39 train loss  0.0207 valid loss 0.010 and accuracy 0.6941\n",
      "Epoch 40 train loss  0.0176 valid loss 0.007 and accuracy 0.6629\n",
      "Epoch 41 train loss  0.0185 valid loss 0.010 and accuracy 0.7022\n",
      "Epoch 42 train loss  0.0309 valid loss 0.004 and accuracy 0.6680\n",
      "Epoch 43 train loss  0.0324 valid loss 0.009 and accuracy 0.6794\n",
      "Epoch 44 train loss  0.0233 valid loss 0.011 and accuracy 0.7147\n",
      "Epoch 45 train loss  0.0254 valid loss 0.008 and accuracy 0.6711\n",
      "Epoch 46 train loss  0.0259 valid loss 0.009 and accuracy 0.6625\n",
      "Epoch 47 train loss  0.0214 valid loss 0.010 and accuracy 0.7035\n",
      "Epoch 48 train loss  0.0242 valid loss 0.007 and accuracy 0.6794\n",
      "Epoch 49 train loss  0.0240 valid loss 0.011 and accuracy 0.6844\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2294]), points: -19\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2311]), points: -35\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-somalia",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "portable-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models/logistic_regressor_medicine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV\n",
      "Accuracy media 0.74891776\n",
      "Puntos media 461.0\n",
      "[tensor(0.7489)]\n",
      "[461]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.75594306\n",
      "Puntos media 468.5\n",
      "[tensor(0.7543), tensor(0.7576)]\n",
      "[468, 469]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.6017316\n",
      "Puntos media 325.0\n",
      "[tensor(0.6017)]\n",
      "[325]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.5960871\n",
      "Puntos media 320.5\n",
      "[tensor(0.6078), tensor(0.5844)]\n",
      "[332, 309]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.63203466\n",
      "Puntos media 353.0\n",
      "[tensor(0.6320)]\n",
      "[353]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.6198873\n",
      "Puntos media 342.5\n",
      "[tensor(0.6121), tensor(0.6277)]\n",
      "[336, 349]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "computational-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.25108224\n",
      "Puntos media 1.0\n",
      "[tensor(0.2511)]\n",
      "[1]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24619345\n",
      "Puntos media -3.5\n",
      "[tensor(0.2586), tensor(0.2338)]\n",
      "[8, -15]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.22943723\n",
      "Puntos media -19.0\n",
      "[tensor(0.2294)]\n",
      "[-19]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23113525\n",
      "Puntos media -17.5\n",
      "[tensor(0.2155), tensor(0.2468)]\n",
      "[-32, -3]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
