{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'medicine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab\n",
    "\n",
    "vectorizer.label_vocab.vocab2index = {1:1, 0:0}\n",
    "vectorizer.label_vocab.index2vocab = {0:0, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  50.2596 valid loss 1.781 and accuracy 0.5056\n",
      "Epoch 1 train loss  44.0149 valid loss 2.762 and accuracy 0.2935\n",
      "Epoch 2 train loss  50.9618 valid loss 2.762 and accuracy 0.2824\n",
      "Epoch 3 train loss  50.6703 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 4 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 5 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 6 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 7 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 8 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 9 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 10 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 11 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 12 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 13 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 14 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 15 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 16 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 17 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 18 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 19 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 20 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 21 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 22 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 23 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 24 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 25 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 26 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 27 train loss  50.6752 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 28 train loss  50.6753 valid loss 2.762 and accuracy 0.2857\n",
      "Epoch 29 train loss  50.6753 valid loss 2.762 and accuracy 0.2857\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2554]), points: 5\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2376]), points: -23\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6714 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7211 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7044 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6998 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6976 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6954 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6942 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6914 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.6865 valid loss 0.026 and accuracy 0.2522\n",
      "Epoch 9 train loss  0.6745 valid loss 0.026 and accuracy 0.2578\n",
      "Epoch 10 train loss  0.6580 valid loss 0.026 and accuracy 0.2801\n",
      "Epoch 11 train loss  0.6351 valid loss 0.026 and accuracy 0.3147\n",
      "Epoch 12 train loss  0.6090 valid loss 0.026 and accuracy 0.3538\n",
      "Epoch 13 train loss  0.5804 valid loss 0.026 and accuracy 0.3672\n",
      "Epoch 14 train loss  0.5657 valid loss 0.026 and accuracy 0.4129\n",
      "Epoch 15 train loss  0.5347 valid loss 0.026 and accuracy 0.3817\n",
      "Epoch 16 train loss  0.5368 valid loss 0.026 and accuracy 0.4040\n",
      "Epoch 17 train loss  0.5061 valid loss 0.027 and accuracy 0.3906\n",
      "Epoch 18 train loss  0.5122 valid loss 0.026 and accuracy 0.4096\n",
      "Epoch 19 train loss  0.5112 valid loss 0.025 and accuracy 0.4408\n",
      "Epoch 20 train loss  0.4874 valid loss 0.025 and accuracy 0.4263\n",
      "Epoch 21 train loss  0.4835 valid loss 0.026 and accuracy 0.4074\n",
      "Epoch 22 train loss  0.4746 valid loss 0.025 and accuracy 0.4152\n",
      "Epoch 23 train loss  0.4687 valid loss 0.026 and accuracy 0.4163\n",
      "Epoch 24 train loss  0.4682 valid loss 0.027 and accuracy 0.4074\n",
      "Epoch 25 train loss  0.4569 valid loss 0.027 and accuracy 0.4085\n",
      "Epoch 26 train loss  0.4512 valid loss 0.028 and accuracy 0.4062\n",
      "Epoch 27 train loss  0.4529 valid loss 0.026 and accuracy 0.4185\n",
      "Epoch 28 train loss  0.4491 valid loss 0.027 and accuracy 0.4297\n",
      "Epoch 29 train loss  0.4458 valid loss 0.028 and accuracy 0.4152\n",
      "Epoch 30 train loss  0.4435 valid loss 0.028 and accuracy 0.4152\n",
      "Epoch 31 train loss  0.4423 valid loss 0.029 and accuracy 0.4185\n",
      "Epoch 32 train loss  0.4379 valid loss 0.028 and accuracy 0.4141\n",
      "Epoch 33 train loss  0.4382 valid loss 0.028 and accuracy 0.4230\n",
      "Epoch 34 train loss  0.4393 valid loss 0.029 and accuracy 0.4174\n",
      "Epoch 35 train loss  0.4381 valid loss 0.030 and accuracy 0.4129\n",
      "Epoch 36 train loss  0.4351 valid loss 0.031 and accuracy 0.4219\n",
      "Epoch 37 train loss  0.4306 valid loss 0.032 and accuracy 0.4263\n",
      "Epoch 38 train loss  0.4318 valid loss 0.030 and accuracy 0.4163\n",
      "Epoch 39 train loss  0.4334 valid loss 0.031 and accuracy 0.4308\n",
      "Epoch 40 train loss  0.4338 valid loss 0.029 and accuracy 0.4107\n",
      "Epoch 41 train loss  0.4320 valid loss 0.032 and accuracy 0.4330\n",
      "Epoch 42 train loss  0.4315 valid loss 0.032 and accuracy 0.4219\n",
      "Epoch 43 train loss  0.4316 valid loss 0.031 and accuracy 0.4263\n",
      "Epoch 44 train loss  0.4306 valid loss 0.033 and accuracy 0.4252\n",
      "Epoch 45 train loss  0.4288 valid loss 0.033 and accuracy 0.4118\n",
      "Epoch 46 train loss  0.4292 valid loss 0.034 and accuracy 0.4241\n",
      "Epoch 47 train loss  0.4269 valid loss 0.034 and accuracy 0.4230\n",
      "Epoch 48 train loss  0.4280 valid loss 0.034 and accuracy 0.4208\n",
      "Epoch 49 train loss  0.4263 valid loss 0.034 and accuracy 0.4252\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2294]), points: -19\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2181]), points: -59\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3829 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  50.7812 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  50.7812 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  50.7812 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  50.7812 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  50.7812 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  50.7812 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  81.5559 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 8 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 9 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 10 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 11 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 12 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 13 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 14 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 15 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 16 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 17 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 18 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 19 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 20 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 21 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 22 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 23 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 41 train loss  49.2188 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 42 train loss  49.0053 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 43 train loss  44.9350 valid loss 0.789 and accuracy 0.7500\n",
      "Epoch 44 train loss  40.2143 valid loss 0.731 and accuracy 0.7500\n",
      "Epoch 45 train loss  37.1838 valid loss 0.675 and accuracy 0.7500\n",
      "Epoch 46 train loss  34.2430 valid loss 0.621 and accuracy 0.7500\n",
      "Epoch 47 train loss  31.3962 valid loss 0.568 and accuracy 0.7500\n",
      "Epoch 48 train loss  28.6531 valid loss 0.518 and accuracy 0.7500\n",
      "Epoch 49 train loss  26.0368 valid loss 0.470 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2468]), points: -3\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2138]), points: -67\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(oversampled_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab\n",
    "\n",
    "vectorizer.label_vocab.vocab2index = {1:1, 0:0}\n",
    "vectorizer.label_vocab.index2vocab = {0:0, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5018 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4943 valid loss 0.003 and accuracy 0.7498\n",
      "Epoch 2 train loss  0.4642 valid loss 0.004 and accuracy 0.7298\n",
      "Epoch 3 train loss  0.4050 valid loss 0.004 and accuracy 0.7314\n",
      "Epoch 4 train loss  0.3450 valid loss 0.004 and accuracy 0.7226\n",
      "Epoch 5 train loss  0.2816 valid loss 0.004 and accuracy 0.6450\n",
      "Epoch 6 train loss  0.2571 valid loss 0.005 and accuracy 0.6410\n",
      "Epoch 7 train loss  0.2041 valid loss 0.007 and accuracy 0.7031\n",
      "Epoch 8 train loss  0.1722 valid loss 0.007 and accuracy 0.7074\n",
      "Epoch 9 train loss  0.1633 valid loss 0.006 and accuracy 0.6912\n",
      "Epoch 10 train loss  0.1230 valid loss 0.007 and accuracy 0.6278\n",
      "Epoch 11 train loss  0.1097 valid loss 0.008 and accuracy 0.6686\n",
      "Epoch 12 train loss  0.1075 valid loss 0.007 and accuracy 0.6901\n",
      "Epoch 13 train loss  0.0810 valid loss 0.008 and accuracy 0.6987\n",
      "Epoch 14 train loss  0.0708 valid loss 0.008 and accuracy 0.6833\n",
      "Epoch 15 train loss  0.0648 valid loss 0.007 and accuracy 0.6932\n",
      "Epoch 16 train loss  0.0565 valid loss 0.008 and accuracy 0.6991\n",
      "Epoch 17 train loss  0.0573 valid loss 0.009 and accuracy 0.6923\n",
      "Epoch 18 train loss  0.0617 valid loss 0.007 and accuracy 0.7040\n",
      "Epoch 19 train loss  0.0671 valid loss 0.010 and accuracy 0.7066\n",
      "Epoch 20 train loss  0.0591 valid loss 0.008 and accuracy 0.6669\n",
      "Epoch 21 train loss  0.0454 valid loss 0.011 and accuracy 0.7119\n",
      "Epoch 22 train loss  0.0484 valid loss 0.009 and accuracy 0.6577\n",
      "Epoch 23 train loss  0.0472 valid loss 0.008 and accuracy 0.7002\n",
      "Epoch 24 train loss  0.0393 valid loss 0.008 and accuracy 0.7079\n",
      "Epoch 25 train loss  0.0386 valid loss 0.008 and accuracy 0.6783\n",
      "Epoch 26 train loss  0.0362 valid loss 0.010 and accuracy 0.7136\n",
      "Epoch 27 train loss  0.0403 valid loss 0.007 and accuracy 0.6546\n",
      "Epoch 28 train loss  0.0428 valid loss 0.009 and accuracy 0.6868\n",
      "Epoch 29 train loss  0.0462 valid loss 0.008 and accuracy 0.6779\n",
      "Epoch 30 train loss  0.0415 valid loss 0.009 and accuracy 0.7064\n",
      "Epoch 31 train loss  0.0440 valid loss 0.007 and accuracy 0.6719\n",
      "Epoch 32 train loss  0.0327 valid loss 0.010 and accuracy 0.7127\n",
      "Epoch 33 train loss  0.0285 valid loss 0.008 and accuracy 0.7068\n",
      "Epoch 34 train loss  0.0316 valid loss 0.010 and accuracy 0.6873\n",
      "Epoch 35 train loss  0.0287 valid loss 0.010 and accuracy 0.7017\n",
      "Epoch 36 train loss  0.0302 valid loss 0.007 and accuracy 0.6873\n",
      "Epoch 37 train loss  0.0432 valid loss 0.007 and accuracy 0.7044\n",
      "Epoch 38 train loss  0.0393 valid loss 0.009 and accuracy 0.6937\n",
      "Epoch 39 train loss  0.0236 valid loss 0.010 and accuracy 0.7105\n",
      "Epoch 40 train loss  0.0218 valid loss 0.010 and accuracy 0.7015\n",
      "Epoch 41 train loss  0.0269 valid loss 0.009 and accuracy 0.6789\n",
      "Epoch 42 train loss  0.0191 valid loss 0.010 and accuracy 0.6928\n",
      "Epoch 43 train loss  0.0149 valid loss 0.011 and accuracy 0.6748\n",
      "Epoch 44 train loss  0.0457 valid loss 0.009 and accuracy 0.6983\n",
      "Epoch 45 train loss  0.0408 valid loss 0.010 and accuracy 0.7129\n",
      "Epoch 46 train loss  0.0295 valid loss 0.009 and accuracy 0.6882\n",
      "Epoch 47 train loss  0.0169 valid loss 0.010 and accuracy 0.6915\n",
      "Epoch 48 train loss  0.0174 valid loss 0.010 and accuracy 0.6906\n",
      "Epoch 49 train loss  0.0273 valid loss 0.009 and accuracy 0.6722\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2381]), points: -11\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2527]), points: 5\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5017 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4951 valid loss 0.003 and accuracy 0.7496\n",
      "Epoch 2 train loss  0.4694 valid loss 0.004 and accuracy 0.7460\n",
      "Epoch 3 train loss  0.4222 valid loss 0.004 and accuracy 0.5665\n",
      "Epoch 4 train loss  0.3602 valid loss 0.005 and accuracy 0.6445\n",
      "Epoch 5 train loss  0.2998 valid loss 0.005 and accuracy 0.6921\n",
      "Epoch 6 train loss  0.2536 valid loss 0.006 and accuracy 0.6783\n",
      "Epoch 7 train loss  0.2044 valid loss 0.007 and accuracy 0.7191\n",
      "Epoch 8 train loss  0.1763 valid loss 0.009 and accuracy 0.7009\n",
      "Epoch 9 train loss  0.1483 valid loss 0.008 and accuracy 0.6537\n",
      "Epoch 10 train loss  0.1209 valid loss 0.006 and accuracy 0.6901\n",
      "Epoch 11 train loss  0.0960 valid loss 0.009 and accuracy 0.6947\n",
      "Epoch 12 train loss  0.0793 valid loss 0.008 and accuracy 0.6618\n",
      "Epoch 13 train loss  0.0825 valid loss 0.010 and accuracy 0.7075\n",
      "Epoch 14 train loss  0.0801 valid loss 0.008 and accuracy 0.6840\n",
      "Epoch 15 train loss  0.0679 valid loss 0.009 and accuracy 0.6846\n",
      "Epoch 16 train loss  0.0632 valid loss 0.007 and accuracy 0.6748\n",
      "Epoch 17 train loss  0.0558 valid loss 0.011 and accuracy 0.6921\n",
      "Epoch 18 train loss  0.0558 valid loss 0.009 and accuracy 0.6513\n",
      "Epoch 19 train loss  0.0591 valid loss 0.008 and accuracy 0.6700\n",
      "Epoch 20 train loss  0.0499 valid loss 0.011 and accuracy 0.6860\n",
      "Epoch 21 train loss  0.0471 valid loss 0.008 and accuracy 0.6733\n",
      "Epoch 22 train loss  0.0405 valid loss 0.008 and accuracy 0.6847\n",
      "Epoch 23 train loss  0.0355 valid loss 0.010 and accuracy 0.6912\n",
      "Epoch 24 train loss  0.0528 valid loss 0.009 and accuracy 0.7075\n",
      "Epoch 25 train loss  0.0544 valid loss 0.010 and accuracy 0.6991\n",
      "Epoch 26 train loss  0.0384 valid loss 0.008 and accuracy 0.6954\n",
      "Epoch 27 train loss  0.0362 valid loss 0.009 and accuracy 0.6974\n",
      "Epoch 28 train loss  0.0409 valid loss 0.010 and accuracy 0.7103\n",
      "Epoch 29 train loss  0.0360 valid loss 0.011 and accuracy 0.7077\n",
      "Epoch 30 train loss  0.0314 valid loss 0.011 and accuracy 0.6915\n",
      "Epoch 31 train loss  0.0317 valid loss 0.010 and accuracy 0.6919\n",
      "Epoch 32 train loss  0.0334 valid loss 0.009 and accuracy 0.7057\n",
      "Epoch 33 train loss  0.0329 valid loss 0.011 and accuracy 0.6994\n",
      "Epoch 34 train loss  0.0365 valid loss 0.009 and accuracy 0.6930\n",
      "Epoch 35 train loss  0.0273 valid loss 0.011 and accuracy 0.6825\n",
      "Epoch 36 train loss  0.0273 valid loss 0.011 and accuracy 0.6671\n",
      "Epoch 37 train loss  0.0271 valid loss 0.010 and accuracy 0.7057\n",
      "Epoch 38 train loss  0.0329 valid loss 0.012 and accuracy 0.6597\n",
      "Epoch 39 train loss  0.0420 valid loss 0.010 and accuracy 0.6811\n",
      "Epoch 40 train loss  0.0318 valid loss 0.011 and accuracy 0.6746\n",
      "Epoch 41 train loss  0.0187 valid loss 0.011 and accuracy 0.6847\n",
      "Epoch 42 train loss  0.0237 valid loss 0.010 and accuracy 0.6792\n",
      "Epoch 43 train loss  0.0336 valid loss 0.011 and accuracy 0.7127\n",
      "Epoch 44 train loss  0.0305 valid loss 0.010 and accuracy 0.6825\n",
      "Epoch 45 train loss  0.0206 valid loss 0.011 and accuracy 0.6763\n",
      "Epoch 46 train loss  0.0181 valid loss 0.013 and accuracy 0.7057\n",
      "Epoch 47 train loss  0.0173 valid loss 0.013 and accuracy 0.6798\n",
      "Epoch 48 train loss  0.0215 valid loss 0.011 and accuracy 0.6864\n",
      "Epoch 49 train loss  0.0375 valid loss 0.012 and accuracy 0.6746\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2381]), points: -11\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2592]), points: 17\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-somalia",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "portable-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models/logistic_regressor_medicine\n",
      "DEV\n",
      "Accuracy media 0.25541127\n",
      "Puntos media 5.0\n",
      "[tensor(0.2554)]\n",
      "[5]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23756345\n",
      "Puntos media -11.5\n",
      "[tensor(0.2457), tensor(0.2294)]\n",
      "[-4, -19]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.22943723\n",
      "Puntos media -19.0\n",
      "[tensor(0.2294)]\n",
      "[-19]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.21812025\n",
      "Puntos media -29.5\n",
      "[tensor(0.2284), tensor(0.2078)]\n",
      "[-20, -39]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.24675325\n",
      "Puntos media -3.0\n",
      "[tensor(0.2468)]\n",
      "[-3]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.21386588\n",
      "Puntos media -33.5\n",
      "[tensor(0.1940), tensor(0.2338)]\n",
      "[-52, -15]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models/basic_lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "computational-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.23809524\n",
      "Puntos media -11.0\n",
      "[tensor(0.2381)]\n",
      "[-11]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25266832\n",
      "Puntos media 2.5\n",
      "[tensor(0.2672), tensor(0.2381)]\n",
      "[16, -11]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.23809524\n",
      "Puntos media -11.0\n",
      "[tensor(0.2381)]\n",
      "[-11]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25914317\n",
      "Puntos media 8.5\n",
      "[tensor(0.2759), tensor(0.2424)]\n",
      "[24, -7]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
