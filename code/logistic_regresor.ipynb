{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from utils_data import random_oversamplig, mixed_oversampling, translate_instance, translate_instance_ir, similarity_instance_ir\n",
    "\n",
    "\n",
    "from training import train, validate, evaluate, evaluator, evaluate_better, get_optimizer\n",
    "\n",
    "from supervised_models import LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boring-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legitimate-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rural-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [{'aid': 1, 'atext': 'Son de tipo todo o nada.'}, {'aid': 2, 'atext': 'Son hiperpolarizantes.'}, {'aid': 3, 'atext': 'Se pueden sumar.'}, {'aid': 4, 'atext': 'Se propagan a largas distancias.'}, {'aid': 5, 'atext': 'Presentan un periodo refractario.'}], 'category': 'biology', 'image': '', 'name': 'Cuaderno_2013_1_B', 'qid': 1, 'qtext': 'Los potenciales postsin√°pticos excitadores:', 'ra': 3, 'year': '2013'}\n"
     ]
    }
   ],
   "source": [
    "for d in training:\n",
    "    print(d)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "# mixed_oversampling_training = mixed_oversampling(training_instances, translate_instance, similarity_instance)\n",
    "# mixed_oversampling_training_ir = mixed_oversampling(training_instances_ir, translate_instance_ir, similarity_instance_ir)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)\n",
    "# save_dataset_to_pickle('../data/mixed_oversampling_training.pickle', mixed_oversampling_training)\n",
    "# save_dataset_to_pickle('../data/mixed_oversampling_training_ir.pickle', mixed_oversampling_training_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "running-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=oversampled_training, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "overall-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tamil-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(model, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "friendly-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  49.8963 valid loss 0.417 and accuracy 0.3256\n",
      "Epoch 1 train loss  49.5299 valid loss 0.493 and accuracy 0.3563\n",
      "Epoch 2 train loss  49.7183 valid loss 0.455 and accuracy 0.3221\n",
      "Epoch 3 train loss  49.7255 valid loss 0.455 and accuracy 0.2717\n",
      "Epoch 4 train loss  49.8110 valid loss 0.417 and accuracy 0.2680\n",
      "Epoch 5 train loss  50.0348 valid loss 0.436 and accuracy 0.2686\n",
      "Epoch 6 train loss  50.0348 valid loss 0.398 and accuracy 0.2678\n",
      "Epoch 7 train loss  50.0348 valid loss 0.455 and accuracy 0.2691\n",
      "Epoch 8 train loss  50.0348 valid loss 0.436 and accuracy 0.2682\n",
      "Epoch 9 train loss  50.0349 valid loss 0.455 and accuracy 0.2684\n",
      "Epoch 10 train loss  50.0396 valid loss 0.379 and accuracy 0.2678\n",
      "Epoch 11 train loss  50.0397 valid loss 0.455 and accuracy 0.2689\n",
      "Epoch 12 train loss  50.0303 valid loss 0.398 and accuracy 0.2684\n",
      "Epoch 13 train loss  50.0351 valid loss 0.427 and accuracy 0.2684\n",
      "Epoch 14 train loss  50.0493 valid loss 0.436 and accuracy 0.2693\n",
      "Epoch 15 train loss  50.0447 valid loss 0.398 and accuracy 0.2678\n",
      "Epoch 16 train loss  50.0354 valid loss 0.398 and accuracy 0.2689\n",
      "Epoch 17 train loss  50.0497 valid loss 0.455 and accuracy 0.2689\n",
      "Epoch 18 train loss  50.0217 valid loss 0.493 and accuracy 0.2689\n",
      "Epoch 19 train loss  50.0363 valid loss 0.474 and accuracy 0.2680\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "neutral-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stunning-departure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2365]), -74)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "turned-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occupied-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2440]), -66)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nonprofit-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/logistic_regressor'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "serious-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_lreg.pickle', training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fresh-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = load_dataset_from_pickle('../data/train_results_lreg.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "preliminary-living",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(trainset.max_length, 1)\n",
    "model.load_state_dict(torch.load(os.getcwd() + '/trained_models/logistic_regressor'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "compliant-fountain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23650944,\n",
       " -12.333333333333334,\n",
       " [tensor(0.2743),\n",
       "  tensor(0.2130),\n",
       "  tensor(0.2267),\n",
       "  tensor(0.2511),\n",
       "  tensor(0.2478),\n",
       "  tensor(0.2061)],\n",
       " [22, -34, -21, 1, -2, -40])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exclusive-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24385573,\n",
       " -5.5,\n",
       " [tensor(0.2588),\n",
       "  tensor(0.2287),\n",
       "  tensor(0.2325),\n",
       "  tensor(0.2500),\n",
       "  tensor(0.2478),\n",
       "  tensor(0.2900),\n",
       "  tensor(0.2257),\n",
       "  tensor(0.2543),\n",
       "  tensor(0.2445),\n",
       "  tensor(0.2424),\n",
       "  tensor(0.2489),\n",
       "  tensor(0.2026)],\n",
       " [8, -19, -16, 0, -2, 37, -22, 4, -5, -7, -1, -43])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-albany",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
