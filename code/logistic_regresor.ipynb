{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from training import train, validate, evaluate\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boring-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# data_en = load_dataset('head_qa', 'en')\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legitimate-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rural-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [{'aid': 1, 'atext': 'Son de tipo todo o nada.'}, {'aid': 2, 'atext': 'Son hiperpolarizantes.'}, {'aid': 3, 'atext': 'Se pueden sumar.'}, {'aid': 4, 'atext': 'Se propagan a largas distancias.'}, {'aid': 5, 'atext': 'Presentan un periodo refractario.'}], 'category': 'biology', 'image': '', 'name': 'Cuaderno_2013_1_B', 'qid': 1, 'qtext': 'Los potenciales postsin√°pticos excitadores:', 'ra': 3, 'year': '2013'}\n"
     ]
    }
   ],
   "source": [
    "for d in training:\n",
    "    print(d)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "running-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "overall-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polar-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, x_size, n_classes): \n",
    "        super(LogisticRegression, self).__init__()             \n",
    "        self.linear = nn.Linear(x_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x.float())\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "every-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tamil-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "friendly-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  21.6436 valid loss 0.152 and accuracy 24.0000\n",
      "Epoch 1 train loss  21.8005 valid loss 0.171 and accuracy 24.0118\n",
      "Epoch 2 train loss  21.9615 valid loss 0.171 and accuracy 24.0000\n",
      "Epoch 3 train loss  21.7669 valid loss 0.133 and accuracy 24.0000\n",
      "Epoch 4 train loss  21.7888 valid loss 0.171 and accuracy 23.9941\n",
      "Epoch 5 train loss  21.6099 valid loss 0.227 and accuracy 24.0118\n",
      "Epoch 6 train loss  21.7605 valid loss 0.178 and accuracy 24.0059\n",
      "Epoch 7 train loss  21.6213 valid loss 0.171 and accuracy 24.0000\n",
      "Epoch 8 train loss  21.7413 valid loss 0.133 and accuracy 23.9941\n",
      "Epoch 9 train loss  21.7339 valid loss 0.190 and accuracy 24.0059\n",
      "Epoch 10 train loss  21.7410 valid loss 0.133 and accuracy 24.0118\n",
      "Epoch 11 train loss  21.7199 valid loss 0.227 and accuracy 24.0000\n",
      "Epoch 12 train loss  21.7986 valid loss 0.152 and accuracy 24.0000\n",
      "Epoch 13 train loss  21.7735 valid loss 0.133 and accuracy 24.0118\n",
      "Epoch 14 train loss  21.8494 valid loss 0.152 and accuracy 24.0000\n",
      "Epoch 15 train loss  21.7584 valid loss 0.133 and accuracy 23.9941\n",
      "Epoch 16 train loss  21.7260 valid loss 0.152 and accuracy 23.9824\n",
      "Epoch 17 train loss  21.8206 valid loss 0.265 and accuracy 24.0118\n",
      "Epoch 18 train loss  21.8070 valid loss 0.190 and accuracy 23.9882\n",
      "Epoch 19 train loss  21.7611 valid loss 0.152 and accuracy 24.0059\n",
      "Epoch 20 train loss  21.8300 valid loss 0.095 and accuracy 23.9941\n",
      "Epoch 21 train loss  21.6702 valid loss 0.133 and accuracy 24.0176\n",
      "Epoch 22 train loss  21.7804 valid loss 0.171 and accuracy 24.0000\n",
      "Epoch 23 train loss  21.6503 valid loss 0.190 and accuracy 24.0118\n",
      "Epoch 24 train loss  21.7617 valid loss 0.190 and accuracy 24.0000\n",
      "Epoch 25 train loss  21.8425 valid loss 0.171 and accuracy 23.9941\n",
      "Epoch 26 train loss  21.7328 valid loss 0.190 and accuracy 23.9882\n",
      "Epoch 27 train loss  21.7626 valid loss 0.209 and accuracy 23.9941\n",
      "Epoch 28 train loss  21.7548 valid loss 0.152 and accuracy 24.0294\n",
      "Epoch 29 train loss  21.8888 valid loss 0.171 and accuracy 24.0059\n",
      "Epoch 30 train loss  21.6967 valid loss 0.227 and accuracy 23.9941\n",
      "Epoch 31 train loss  21.7173 valid loss 0.133 and accuracy 24.0000\n",
      "Epoch 32 train loss  21.6531 valid loss 0.171 and accuracy 23.9765\n",
      "Epoch 33 train loss  21.7762 valid loss 0.227 and accuracy 24.0118\n",
      "Epoch 34 train loss  21.6978 valid loss 0.152 and accuracy 24.0176\n",
      "Epoch 35 train loss  21.9203 valid loss 0.133 and accuracy 24.0000\n",
      "Epoch 36 train loss  21.5837 valid loss 0.171 and accuracy 23.9824\n",
      "Epoch 37 train loss  21.7227 valid loss 0.171 and accuracy 24.0000\n",
      "Epoch 38 train loss  21.6492 valid loss 0.152 and accuracy 23.9941\n",
      "Epoch 39 train loss  21.8321 valid loss 0.227 and accuracy 23.9882\n",
      "Epoch 40 train loss  21.6576 valid loss 0.133 and accuracy 24.0000\n",
      "Epoch 41 train loss  21.6527 valid loss 0.190 and accuracy 24.0000\n",
      "Epoch 42 train loss  21.6585 valid loss 0.246 and accuracy 23.9882\n",
      "Epoch 43 train loss  21.8598 valid loss 0.171 and accuracy 23.9941\n",
      "Epoch 44 train loss  21.7611 valid loss 0.171 and accuracy 24.0059\n",
      "Epoch 45 train loss  21.7852 valid loss 0.190 and accuracy 23.9765\n",
      "Epoch 46 train loss  21.5974 valid loss 0.114 and accuracy 24.0059\n",
      "Epoch 47 train loss  21.6537 valid loss 0.152 and accuracy 23.9941\n",
      "Epoch 48 train loss  21.6194 valid loss 0.095 and accuracy 23.9882\n",
      "Epoch 49 train loss  21.8166 valid loss 0.246 and accuracy 24.0000\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "neutral-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stunning-departure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2540]), 22)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "turned-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "occupied-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2695]), 214)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nonprofit-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/logistic_regressor'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
