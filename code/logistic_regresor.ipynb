{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_training\n",
    "from training import train, validate_answer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\CLAUDIA\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\d6803d1e84273cdc4a2cf3c5102945d166555f47b299ecbc5266d582f408f8e2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# data_en = load_dataset('head_qa', 'en')\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Cuaderno_2013_1_B', 'year': '2013', 'category': 'biology', 'qid': 1, 'qtext': 'Los potenciales postsinápticos excitadores:', 'ra': 3, 'image': '', 'answers': [{'aid': 1, 'atext': 'Son de tipo todo o nada.'}, {'aid': 2, 'atext': 'Son hiperpolarizantes.'}, {'aid': 3, 'atext': 'Se pueden sumar.'}, {'aid': 4, 'atext': 'Se propagan a largas distancias.'}, {'aid': 5, 'atext': 'Presentan un periodo refractario.'}]}\n"
     ]
    }
   ],
   "source": [
    "for d in training:\n",
    "    print(d)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = parse_training(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary()\n",
    "vectorizer = Vectorizer.vectorize_training(training_instances)\n",
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.,\n",
       "         12., 13.]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Los potenciales postsinápticos excitadores:',\n",
       " 'answer': 'Son de tipo todo o nada.',\n",
       " 'label': 0,\n",
       " 'sample_tok': ['Los',\n",
       "  'potenciales',\n",
       "  'postsinápticos',\n",
       "  'excitadores',\n",
       "  ':',\n",
       "  'SEP',\n",
       "  'Son',\n",
       "  'de',\n",
       "  'tipo',\n",
       "  'todo',\n",
       "  'o',\n",
       "  'nada',\n",
       "  '.'],\n",
       " 'category': 'biology'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.,\n",
      "        12., 13.])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "for x, y in trainset:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, x_size, n_classes): \n",
    "        super(LogisticRegression, self).__init__()             \n",
    "        self.linear = nn.Linear(x_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(dataset.max_length, 1)\n",
    "optimizer = get_optimizer(model, lr = 0.0001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sample):\n",
    "        qtext, answers = sample['qtext'], sample['answers']\n",
    "        q = nlp(qtext)\n",
    "        tok_qtext = [token.text for token in q]\n",
    "        right_answer = sample['ra']\n",
    "        X, Y = [], []\n",
    "        for answer in answers:\n",
    "            aid, atext = answer['aid'], answer['atext']\n",
    "            a = nlp(atext)\n",
    "            tok_atext = [token.text for token in a]\n",
    "            instance_x = tok_qtext + ['SEP'] + tok_atext\n",
    "            instance_y = 1 if right_answer == aid else 0\n",
    "            x, y = trainset.vectorize(instance_x, instance_y)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            print(X)\n",
    "        return torch.Tensor(X), torch.Tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dl, test_dl, validate, encoder, epochs=100):\n",
    "    y_trues, y_preds = [], []\n",
    "    epochs_results = []\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        for x, y in train_dl:\n",
    "            batch = y.shape[0]\n",
    "            out = model(x.float())\n",
    "            loss = F.binary_cross_entropy(out, y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += batch\n",
    "            sum_loss += batch*(loss.item())\n",
    "        train_loss = sum_loss/total\n",
    "        test_acc, points = validate(model, test_dl, encoder)\n",
    "        #y_trues.append(y_real)\n",
    "        #y_preds.append(y_pred)\n",
    "        epochs_results.append([train_loss, points, test_acc])\n",
    "        print(\"Epoch %s train loss  %.4f points %.3f and accuracy %.4f\" %\n",
    "              (i, train_loss, points, test_acc))\n",
    "    return epochs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 9.2000e+01, 4.7920e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01])]\n",
      "[tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 9.2000e+01, 4.7920e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01]), tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 2.0800e+02, 6.8770e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01])]\n",
      "[tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 9.2000e+01, 4.7920e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01]), tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 2.0800e+02, 6.8770e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01]), tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 2.0800e+02, 6.8770e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01])]\n",
      "[tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 9.2000e+01, 4.7920e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01]), tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 2.0800e+02, 6.8770e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01]), tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 2.0800e+02, 6.8770e+03, 8.0000e+00, 0.0000e+00, 1.3000e+01]), tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.2000e+01, 3.3730e+03, 8.0000e+00, 4.7920e+03, 3.7800e+02,\n",
      "        2.3000e+01, 6.8590e+03, 0.0000e+00, 1.9000e+01, 1.5710e+03, 8.0000e+00,\n",
      "        2.8400e+02, 2.0600e+02, 6.3000e+01, 9.9560e+03, 1.2260e+03, 5.0000e+00,\n",
      "        6.0000e+00, 2.0800e+02, 6.8770e+03, 8.0000e+00, 1.2267e+04, 1.3000e+01])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-48c7ee3baa20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_answer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-160-d11d1c8a3276>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_dl, test_dl, validate, encoder, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m#y_trues.append(y_real)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#y_preds.append(y_pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study\\Master en Data Science AFI\\tfm-afi\\head-qa-afi\\code\\training.py\u001b[0m in \u001b[0;36mvalidate_answer\u001b[1;34m(model, dataloader, encoder, pytorch_model)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mright\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study\\Master en Data Science AFI\\tfm-afi\\head-qa-afi\\code\\training.py\u001b[0m in \u001b[0;36mevaluator\u001b[1;34m(model, instance, encoder)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvalidate_answer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpytorch_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m#x, y = torch.Tensor(x), torch.Tensor(y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-159-cb4e65f9e03a>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(sample)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, trainset, validation, validate_answer, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Cuaderno_2015_1_B',\n",
       " 'year': '2015',\n",
       " 'category': 'biology',\n",
       " 'qid': 1,\n",
       " 'qtext': 'El potencial de equilibrio para un ión permeante a través de una membrana se calcula mediante:',\n",
       " 'ra': 4,\n",
       " 'image': '',\n",
       " 'answers': [{'aid': 1, 'atext': 'El equilibrio de Gibbs-Donnan.'},\n",
       "  {'aid': 2, 'atext': 'La ecuación de Goldman-Hodgkin-Katz.'},\n",
       "  {'aid': 3, 'atext': 'La ecuación de Ohm.'},\n",
       "  {'aid': 4, 'atext': 'La ecuación de Nernst.'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
