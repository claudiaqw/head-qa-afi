{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from training import train, validate, evaluate, evaluator\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boring-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legitimate-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rural-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [{'aid': 1, 'atext': 'Son de tipo todo o nada.'}, {'aid': 2, 'atext': 'Son hiperpolarizantes.'}, {'aid': 3, 'atext': 'Se pueden sumar.'}, {'aid': 4, 'atext': 'Se propagan a largas distancias.'}, {'aid': 5, 'atext': 'Presentan un periodo refractario.'}], 'category': 'biology', 'image': '', 'name': 'Cuaderno_2013_1_B', 'qid': 1, 'qtext': 'Los potenciales postsin√°pticos excitadores:', 'ra': 3, 'year': '2013'}\n"
     ]
    }
   ],
   "source": [
    "for d in training:\n",
    "    print(d)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = parse_dataset(training)\n",
    "validation_instances = parse_dataset(validation)\n",
    "testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "running-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modular-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "overall-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polar-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, x_size, n_classes): \n",
    "        super(LogisticRegression, self).__init__()             \n",
    "        self.linear = nn.Linear(x_size, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x.float())\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "every-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tamil-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(model, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "friendly-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  20.7112 valid loss 0.171 and accuracy 0.7502\n",
      "Epoch 1 train loss  19.9758 valid loss 0.095 and accuracy 0.7494\n",
      "Epoch 2 train loss  19.9752 valid loss 0.152 and accuracy 0.7491\n",
      "Epoch 3 train loss  19.9773 valid loss 0.095 and accuracy 0.7498\n",
      "Epoch 4 train loss  19.9856 valid loss 0.133 and accuracy 0.7482\n",
      "Epoch 5 train loss  20.0564 valid loss 0.076 and accuracy 0.7500\n",
      "Epoch 6 train loss  19.9997 valid loss 0.057 and accuracy 0.7498\n",
      "Epoch 7 train loss  19.9920 valid loss 0.227 and accuracy 0.7500\n",
      "Epoch 8 train loss  20.0070 valid loss 0.114 and accuracy 0.7494\n",
      "Epoch 9 train loss  20.0142 valid loss 0.152 and accuracy 0.7500\n",
      "Epoch 10 train loss  20.0593 valid loss 0.152 and accuracy 0.7482\n",
      "Epoch 11 train loss  20.0892 valid loss 0.133 and accuracy 0.7465\n",
      "Epoch 12 train loss  20.2460 valid loss 0.133 and accuracy 0.7450\n",
      "Epoch 13 train loss  20.7745 valid loss 0.114 and accuracy 0.7500\n",
      "Epoch 14 train loss  20.0616 valid loss 0.171 and accuracy 0.7485\n",
      "Epoch 15 train loss  20.0617 valid loss 0.152 and accuracy 0.7489\n",
      "Epoch 16 train loss  20.0541 valid loss 0.227 and accuracy 0.7485\n",
      "Epoch 17 train loss  20.0703 valid loss 0.171 and accuracy 0.7491\n",
      "Epoch 18 train loss  20.0099 valid loss 0.171 and accuracy 0.7496\n",
      "Epoch 19 train loss  20.0174 valid loss 0.190 and accuracy 0.7491\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "neutral-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stunning-departure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2313]), -102)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "turned-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "occupied-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2422]), -86)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.getcwd() + '/trained_models/logistic_regressor'\n",
    "# torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "serious-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_lreg_sigmoid.pickle', training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = load_dataset_from_pickle('../data/train_results_lreg.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-living",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
