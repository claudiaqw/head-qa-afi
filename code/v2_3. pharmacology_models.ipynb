{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'pharmacology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  43.4278 valid loss 1.225 and accuracy 0.7221\n",
      "Epoch 1 train loss  42.8614 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 2 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 3 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 4 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 5 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 6 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 7 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 8 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 9 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 10 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 11 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 12 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 13 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 14 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 15 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 16 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 17 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 18 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 19 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 20 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 21 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 22 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 23 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 24 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 25 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 26 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 27 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 28 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n",
      "Epoch 29 train loss  42.1228 valid loss 0.921 and accuracy 0.7411\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2133]), points: -33\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2407]), points: -17\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6613 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7151 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6968 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6915 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6882 valid loss 0.024 and accuracy 0.2511\n",
      "Epoch 5 train loss  0.6854 valid loss 0.024 and accuracy 0.2567\n",
      "Epoch 6 train loss  0.6825 valid loss 0.024 and accuracy 0.2667\n",
      "Epoch 7 train loss  0.6775 valid loss 0.024 and accuracy 0.2723\n",
      "Epoch 8 train loss  0.6718 valid loss 0.024 and accuracy 0.2768\n",
      "Epoch 9 train loss  0.6616 valid loss 0.024 and accuracy 0.2891\n",
      "Epoch 10 train loss  0.6360 valid loss 0.023 and accuracy 0.3270\n",
      "Epoch 11 train loss  0.6033 valid loss 0.025 and accuracy 0.3571\n",
      "Epoch 12 train loss  0.5353 valid loss 0.025 and accuracy 0.3973\n",
      "Epoch 13 train loss  0.4856 valid loss 0.024 and accuracy 0.5357\n",
      "Epoch 14 train loss  0.4248 valid loss 0.025 and accuracy 0.5536\n",
      "Epoch 15 train loss  0.3780 valid loss 0.026 and accuracy 0.5748\n",
      "Epoch 16 train loss  0.3425 valid loss 0.031 and accuracy 0.5379\n",
      "Epoch 17 train loss  0.3094 valid loss 0.031 and accuracy 0.5603\n",
      "Epoch 18 train loss  0.2853 valid loss 0.032 and accuracy 0.5670\n",
      "Epoch 19 train loss  0.2597 valid loss 0.033 and accuracy 0.5882\n",
      "Epoch 20 train loss  0.2391 valid loss 0.037 and accuracy 0.5837\n",
      "Epoch 21 train loss  0.2211 valid loss 0.040 and accuracy 0.5100\n",
      "Epoch 22 train loss  0.2091 valid loss 0.039 and accuracy 0.5201\n",
      "Epoch 23 train loss  0.1955 valid loss 0.038 and accuracy 0.5547\n",
      "Epoch 24 train loss  0.1842 valid loss 0.036 and accuracy 0.5871\n",
      "Epoch 25 train loss  0.1789 valid loss 0.039 and accuracy 0.5971\n",
      "Epoch 26 train loss  0.1652 valid loss 0.043 and accuracy 0.5915\n",
      "Epoch 27 train loss  0.1706 valid loss 0.035 and accuracy 0.6194\n",
      "Epoch 28 train loss  0.1565 valid loss 0.040 and accuracy 0.5804\n",
      "Epoch 29 train loss  0.1544 valid loss 0.035 and accuracy 0.5904\n",
      "Epoch 30 train loss  0.1346 valid loss 0.037 and accuracy 0.5926\n",
      "Epoch 31 train loss  0.1275 valid loss 0.038 and accuracy 0.5893\n",
      "Epoch 32 train loss  0.1345 valid loss 0.036 and accuracy 0.5915\n",
      "Epoch 33 train loss  0.1262 valid loss 0.041 and accuracy 0.5893\n",
      "Epoch 34 train loss  0.1279 valid loss 0.039 and accuracy 0.5848\n",
      "Epoch 35 train loss  0.1169 valid loss 0.040 and accuracy 0.5904\n",
      "Epoch 36 train loss  0.1177 valid loss 0.040 and accuracy 0.5971\n",
      "Epoch 37 train loss  0.1073 valid loss 0.045 and accuracy 0.5725\n",
      "Epoch 38 train loss  0.1137 valid loss 0.046 and accuracy 0.5971\n",
      "Epoch 39 train loss  0.1162 valid loss 0.043 and accuracy 0.6016\n",
      "Epoch 40 train loss  0.1106 valid loss 0.041 and accuracy 0.5982\n",
      "Epoch 41 train loss  0.1083 valid loss 0.043 and accuracy 0.5982\n",
      "Epoch 42 train loss  0.1056 valid loss 0.041 and accuracy 0.6016\n",
      "Epoch 43 train loss  0.1001 valid loss 0.040 and accuracy 0.6027\n",
      "Epoch 44 train loss  0.1036 valid loss 0.042 and accuracy 0.5759\n",
      "Epoch 45 train loss  0.1004 valid loss 0.040 and accuracy 0.5982\n",
      "Epoch 46 train loss  0.0966 valid loss 0.039 and accuracy 0.5893\n",
      "Epoch 47 train loss  0.1000 valid loss 0.040 and accuracy 0.5949\n",
      "Epoch 48 train loss  0.0953 valid loss 0.041 and accuracy 0.5993\n",
      "Epoch 49 train loss  0.0961 valid loss 0.043 and accuracy 0.5826\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2178]), points: -29\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2757]), points: 47\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4418 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  57.7020 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 22 train loss  54.1550 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 23 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 41 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 42 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 43 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 44 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 45 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 46 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 47 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 48 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 49 train loss  42.2980 valid loss 0.921 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2133]), points: -33\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2363]), points: -25\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "mixed_training_ir = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training_ir, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4408 valid loss 0.180 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8875 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6999 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7168 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6747 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6721 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6713 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6642 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.6708 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.6674 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.6809 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.6686 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.6643 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.6634 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.6615 valid loss 0.031 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.6656 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.6790 valid loss 0.031 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.6878 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.6875 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.7148 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7346 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.7225 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.7303 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.7259 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.6726 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6625 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.6483 valid loss 0.031 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.7011 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6646 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6406 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6574 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6937 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6929 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6575 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.6507 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6364 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.6370 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.6414 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.6477 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.6612 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6623 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.6869 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.6900 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.6767 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.6609 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.6827 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.6787 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.7159 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.7100 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.6663 valid loss 0.029 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2133]), points: -33\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2385]), points: -21\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4362 valid loss 0.205 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8291 valid loss 0.075 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.8159 valid loss 0.031 and accuracy 0.2522\n",
      "Epoch 3 train loss  0.7409 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6913 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6899 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6913 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6901 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.6886 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.6918 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.6933 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.6953 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.6988 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7046 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.7136 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7021 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7073 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.7042 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.7032 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.7003 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.6969 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.6963 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.6947 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6974 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.6967 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6954 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.6936 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6833 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6758 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6528 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6825 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6666 valid loss 0.031 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6543 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6750 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.6821 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6755 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.6789 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.6779 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.6727 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.6867 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6899 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.6906 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.6812 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.6883 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.6876 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.7069 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.7070 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.6779 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.6373 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.6787 valid loss 0.027 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: pharmacology\n",
      "accuracy: tensor([0.2222]), points: -25\n",
      "----------\n",
      "TEST Dominio: pharmacology\n",
      "accuracy: tensor([0.2560]), points: 11\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-waters",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "every-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models_v2/logistic_regressor_pharmacology\n",
      "DEV\n",
      "Accuracy media 0.21333334\n",
      "Puntos media -33.0\n",
      "[tensor(0.2133)]\n",
      "[-33]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24068221\n",
      "Puntos media -8.5\n",
      "[tensor(0.2325), tensor(0.2489)]\n",
      "[-16, -1]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.21777777\n",
      "Puntos media -29.0\n",
      "[tensor(0.2178)]\n",
      "[-29]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.27564543\n",
      "Puntos media 23.5\n",
      "[tensor(0.2456), tensor(0.3057)]\n",
      "[-4, 51]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.21333334\n",
      "Puntos media -33.0\n",
      "[tensor(0.2133)]\n",
      "[-33]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23630583\n",
      "Puntos media -12.5\n",
      "[tensor(0.2281), tensor(0.2445)]\n",
      "[-20, -5]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [f'trained_models_v2/logistic_regressor_{CATEGORY}', \n",
    "         f'trained_models_v2/lstm_{CATEGORY}',         \n",
    "         f'trained_models_v2/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "centered-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.21333334\n",
      "Puntos media -33.0\n",
      "[tensor(0.2133)]\n",
      "[-33]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23851797\n",
      "Puntos media -10.5\n",
      "[tensor(0.2412), tensor(0.2358)]\n",
      "[-8, -13]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.22222222\n",
      "Puntos media -25.0\n",
      "[tensor(0.2222)]\n",
      "[-25]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25604266\n",
      "Puntos media 5.5\n",
      "[tensor(0.2675), tensor(0.2445)]\n",
      "[16, -5]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [f'trained_models_v2/lstm_qa_{CATEGORY}',\n",
    "         f'trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-archives",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
