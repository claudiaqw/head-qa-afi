{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'medicine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "classified-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.label_vocab.vocab2index = {1:1, 0:0}\n",
    "vectorizer.label_vocab.index2vocab = {0:0, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  31.6583 valid loss 2.302 and accuracy 0.3181\n",
      "Epoch 1 train loss  56.2315 valid loss 2.283 and accuracy 0.2690\n",
      "Epoch 2 train loss  57.1161 valid loss 2.762 and accuracy 0.2533\n",
      "Epoch 3 train loss  57.4692 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 22 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 23 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 24 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 25 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 26 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 27 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 28 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 29 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2511]), points: 1\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2441]), points: -11\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6786 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7070 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6992 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6947 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6899 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6880 valid loss 0.024 and accuracy 0.2545\n",
      "Epoch 6 train loss  0.6851 valid loss 0.024 and accuracy 0.2634\n",
      "Epoch 7 train loss  0.6805 valid loss 0.024 and accuracy 0.2768\n",
      "Epoch 8 train loss  0.6745 valid loss 0.024 and accuracy 0.2902\n",
      "Epoch 9 train loss  0.6691 valid loss 0.024 and accuracy 0.3036\n",
      "Epoch 10 train loss  0.6583 valid loss 0.024 and accuracy 0.3538\n",
      "Epoch 11 train loss  0.6398 valid loss 0.024 and accuracy 0.3895\n",
      "Epoch 12 train loss  0.6112 valid loss 0.024 and accuracy 0.4007\n",
      "Epoch 13 train loss  0.5772 valid loss 0.024 and accuracy 0.4286\n",
      "Epoch 14 train loss  0.5486 valid loss 0.025 and accuracy 0.4364\n",
      "Epoch 15 train loss  0.5069 valid loss 0.025 and accuracy 0.4342\n",
      "Epoch 16 train loss  0.4752 valid loss 0.026 and accuracy 0.4576\n",
      "Epoch 17 train loss  0.4587 valid loss 0.026 and accuracy 0.4654\n",
      "Epoch 18 train loss  0.4348 valid loss 0.029 and accuracy 0.4799\n",
      "Epoch 19 train loss  0.4099 valid loss 0.032 and accuracy 0.4699\n",
      "Epoch 20 train loss  0.3903 valid loss 0.034 and accuracy 0.4676\n",
      "Epoch 21 train loss  0.3735 valid loss 0.036 and accuracy 0.4732\n",
      "Epoch 22 train loss  0.3540 valid loss 0.038 and accuracy 0.4676\n",
      "Epoch 23 train loss  0.3513 valid loss 0.038 and accuracy 0.4788\n",
      "Epoch 24 train loss  0.3358 valid loss 0.038 and accuracy 0.4788\n",
      "Epoch 25 train loss  0.3279 valid loss 0.039 and accuracy 0.4911\n",
      "Epoch 26 train loss  0.3194 valid loss 0.043 and accuracy 0.4721\n",
      "Epoch 27 train loss  0.3159 valid loss 0.046 and accuracy 0.4900\n",
      "Epoch 28 train loss  0.3067 valid loss 0.044 and accuracy 0.4587\n",
      "Epoch 29 train loss  0.3049 valid loss 0.047 and accuracy 0.4944\n",
      "Epoch 30 train loss  0.2906 valid loss 0.049 and accuracy 0.4810\n",
      "Epoch 31 train loss  0.2981 valid loss 0.047 and accuracy 0.4766\n",
      "Epoch 32 train loss  0.2886 valid loss 0.045 and accuracy 0.5067\n",
      "Epoch 33 train loss  0.2771 valid loss 0.047 and accuracy 0.5056\n",
      "Epoch 34 train loss  0.2834 valid loss 0.047 and accuracy 0.4855\n",
      "Epoch 35 train loss  0.2763 valid loss 0.043 and accuracy 0.4989\n",
      "Epoch 36 train loss  0.2659 valid loss 0.046 and accuracy 0.4922\n",
      "Epoch 37 train loss  0.2657 valid loss 0.046 and accuracy 0.5011\n",
      "Epoch 38 train loss  0.2650 valid loss 0.045 and accuracy 0.5100\n",
      "Epoch 39 train loss  0.2623 valid loss 0.044 and accuracy 0.5134\n",
      "Epoch 40 train loss  0.2634 valid loss 0.045 and accuracy 0.5000\n",
      "Epoch 41 train loss  0.2648 valid loss 0.046 and accuracy 0.5033\n",
      "Epoch 42 train loss  0.2588 valid loss 0.049 and accuracy 0.5045\n",
      "Epoch 43 train loss  0.2566 valid loss 0.051 and accuracy 0.5067\n",
      "Epoch 44 train loss  0.2597 valid loss 0.048 and accuracy 0.5022\n",
      "Epoch 45 train loss  0.2561 valid loss 0.046 and accuracy 0.5134\n",
      "Epoch 46 train loss  0.2533 valid loss 0.045 and accuracy 0.5156\n",
      "Epoch 47 train loss  0.2513 valid loss 0.048 and accuracy 0.5190\n",
      "Epoch 48 train loss  0.2488 valid loss 0.047 and accuracy 0.5156\n",
      "Epoch 49 train loss  0.2445 valid loss 0.051 and accuracy 0.5100\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2771]), points: 25\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2052]), points: -83\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4452 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  48.5994 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 22 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 23 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 24 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 25 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 26 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 27 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 28 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 29 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 30 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 31 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 32 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 33 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 34 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 35 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 36 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 37 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 38 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 39 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 40 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 41 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 42 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 43 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 44 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 45 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 46 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 47 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 48 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 49 train loss  57.4495 valid loss 2.762 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2511]), points: 1\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2441]), points: -11\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "mixed_training_ir = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training_ir, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab\n",
    "\n",
    "vectorizer.label_vocab.vocab2index = {1:1, 0:0}\n",
    "vectorizer.label_vocab.index2vocab = {0:0, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4378 valid loss 0.209 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8067 valid loss 0.112 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.8706 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7028 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6776 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6911 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.7019 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6948 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.7041 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7091 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7020 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7149 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.7059 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7016 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.7071 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7059 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7002 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.6973 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.6963 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.6844 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7097 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.6904 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.6830 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6939 valid loss 0.025 and accuracy 0.2567\n",
      "Epoch 24 train loss  0.6713 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6923 valid loss 0.027 and accuracy 0.2511\n",
      "Epoch 26 train loss  0.6530 valid loss 0.032 and accuracy 0.2969\n",
      "Epoch 27 train loss  0.6485 valid loss 0.031 and accuracy 0.3092\n",
      "Epoch 28 train loss  0.6085 valid loss 0.033 and accuracy 0.3248\n",
      "Epoch 29 train loss  0.5651 valid loss 0.038 and accuracy 0.3170\n",
      "Epoch 30 train loss  0.5123 valid loss 0.041 and accuracy 0.3125\n",
      "Epoch 31 train loss  0.4570 valid loss 0.049 and accuracy 0.3114\n",
      "Epoch 32 train loss  0.3858 valid loss 0.049 and accuracy 0.3560\n",
      "Epoch 33 train loss  0.3011 valid loss 0.053 and accuracy 0.3739\n",
      "Epoch 34 train loss  0.2233 valid loss 0.053 and accuracy 0.3951\n",
      "Epoch 35 train loss  0.1669 valid loss 0.059 and accuracy 0.4464\n",
      "Epoch 36 train loss  0.1329 valid loss 0.068 and accuracy 0.4141\n",
      "Epoch 37 train loss  0.1207 valid loss 0.063 and accuracy 0.4621\n",
      "Epoch 38 train loss  0.0945 valid loss 0.069 and accuracy 0.4710\n",
      "Epoch 39 train loss  0.0955 valid loss 0.069 and accuracy 0.4442\n",
      "Epoch 40 train loss  0.0788 valid loss 0.063 and accuracy 0.5335\n",
      "Epoch 41 train loss  0.0438 valid loss 0.056 and accuracy 0.5558\n",
      "Epoch 42 train loss  0.0315 valid loss 0.065 and accuracy 0.5368\n",
      "Epoch 43 train loss  0.0222 valid loss 0.074 and accuracy 0.5145\n",
      "Epoch 44 train loss  0.0153 valid loss 0.076 and accuracy 0.5246\n",
      "Epoch 45 train loss  0.0154 valid loss 0.080 and accuracy 0.5223\n",
      "Epoch 46 train loss  0.0138 valid loss 0.071 and accuracy 0.5391\n",
      "Epoch 47 train loss  0.0152 valid loss 0.080 and accuracy 0.5190\n",
      "Epoch 48 train loss  0.0097 valid loss 0.082 and accuracy 0.5246\n",
      "Epoch 49 train loss  0.0059 valid loss 0.077 and accuracy 0.5324\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2727]), points: 21\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2441]), points: -11\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4453 valid loss 0.180 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8463 valid loss 0.074 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7834 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6805 valid loss 0.048 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6757 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.7092 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6992 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.7149 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.7075 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7060 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7129 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7187 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.7188 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7135 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.7064 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7027 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7004 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.6998 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.6966 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.7096 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7009 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.6968 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.7027 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6906 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.7113 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6875 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.7041 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6730 valid loss 0.029 and accuracy 0.2701\n",
      "Epoch 28 train loss  0.6647 valid loss 0.029 and accuracy 0.2667\n",
      "Epoch 29 train loss  0.6427 valid loss 0.033 and accuracy 0.2857\n",
      "Epoch 30 train loss  0.6037 valid loss 0.035 and accuracy 0.2891\n",
      "Epoch 31 train loss  0.5725 valid loss 0.039 and accuracy 0.2857\n",
      "Epoch 32 train loss  0.5239 valid loss 0.046 and accuracy 0.2835\n",
      "Epoch 33 train loss  0.4776 valid loss 0.045 and accuracy 0.3103\n",
      "Epoch 34 train loss  0.4156 valid loss 0.046 and accuracy 0.3114\n",
      "Epoch 35 train loss  0.3519 valid loss 0.051 and accuracy 0.3281\n",
      "Epoch 36 train loss  0.2775 valid loss 0.054 and accuracy 0.3527\n",
      "Epoch 37 train loss  0.2220 valid loss 0.054 and accuracy 0.3884\n",
      "Epoch 38 train loss  0.1756 valid loss 0.067 and accuracy 0.3917\n",
      "Epoch 39 train loss  0.1622 valid loss 0.062 and accuracy 0.3929\n",
      "Epoch 40 train loss  0.1439 valid loss 0.061 and accuracy 0.4230\n",
      "Epoch 41 train loss  0.1243 valid loss 0.061 and accuracy 0.4342\n",
      "Epoch 42 train loss  0.1303 valid loss 0.063 and accuracy 0.4464\n",
      "Epoch 43 train loss  0.0904 valid loss 0.050 and accuracy 0.5167\n",
      "Epoch 44 train loss  0.0642 valid loss 0.044 and accuracy 0.5737\n",
      "Epoch 45 train loss  0.0754 valid loss 0.048 and accuracy 0.5770\n",
      "Epoch 46 train loss  0.0487 valid loss 0.049 and accuracy 0.5960\n",
      "Epoch 47 train loss  0.0348 valid loss 0.048 and accuracy 0.6060\n",
      "Epoch 48 train loss  0.0244 valid loss 0.053 and accuracy 0.5301\n",
      "Epoch 49 train loss  0.0148 valid loss 0.053 and accuracy 0.5525\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: medicine\n",
      "accuracy: tensor([0.2424]), points: -7\n",
      "----------\n",
      "TEST Dominio: medicine\n",
      "accuracy: tensor([0.2549]), points: 9\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-somalia",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "portable-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models_v2/logistic_regressor_medicine\n",
      "DEV\n",
      "Accuracy media 0.25108224\n",
      "Puntos media 1.0\n",
      "[tensor(0.2511)]\n",
      "[1]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24405695\n",
      "Puntos media -5.5\n",
      "[tensor(0.2457), tensor(0.2424)]\n",
      "[-4, -7]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.27705628\n",
      "Puntos media 25.0\n",
      "[tensor(0.2771)]\n",
      "[25]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.20515189\n",
      "Puntos media -41.5\n",
      "[tensor(0.2198), tensor(0.1905)]\n",
      "[-28, -55]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.25108224\n",
      "Puntos media 1.0\n",
      "[tensor(0.2511)]\n",
      "[1]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24405695\n",
      "Puntos media -5.5\n",
      "[tensor(0.2457), tensor(0.2424)]\n",
      "[-4, -7]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models_v2/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models_v2/lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models_v2/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "computational-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.27272728\n",
      "Puntos media 21.0\n",
      "[tensor(0.2727)]\n",
      "[21]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2440103\n",
      "Puntos media -5.5\n",
      "[tensor(0.2672), tensor(0.2208)]\n",
      "[16, -27]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.24242425\n",
      "Puntos media -7.0\n",
      "[tensor(0.2424)]\n",
      "[-7]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25490746\n",
      "Puntos media 4.5\n",
      "[tensor(0.2328), tensor(0.2771)]\n",
      "[-16, 25]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models_v2/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-overhead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
