{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'chemistry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  44.8732 valid loss 1.151 and accuracy 0.7422\n",
      "Epoch 1 train loss  30.8754 valid loss 2.729 and accuracy 0.3650\n",
      "Epoch 2 train loss  55.4212 valid loss 2.532 and accuracy 0.2600\n",
      "Epoch 3 train loss  57.4654 valid loss 2.532 and accuracy 0.2645\n",
      "Epoch 4 train loss  57.6747 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 5 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 6 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 7 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 8 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 9 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 10 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 11 train loss  57.7071 valid loss 2.762 and accuracy 0.2567\n",
      "Epoch 12 train loss  57.8242 valid loss 2.532 and accuracy 0.2634\n",
      "Epoch 13 train loss  57.5479 valid loss 2.532 and accuracy 0.2634\n",
      "Epoch 14 train loss  57.5547 valid loss 2.532 and accuracy 0.2656\n",
      "Epoch 15 train loss  56.6449 valid loss 2.762 and accuracy 0.2533\n",
      "Epoch 16 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 17 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 18 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 19 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 20 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 21 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 22 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 23 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 24 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 25 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 26 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 27 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 28 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n",
      "Epoch 29 train loss  57.6389 valid loss 2.762 and accuracy 0.2545\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: chemistry\n",
      "accuracy: tensor([0.2105]), points: -36\n",
      "----------\n",
      "TEST Dominio: chemistry\n",
      "accuracy: tensor([0.2358]), points: -26\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6541 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7101 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6955 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6936 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6913 valid loss 0.025 and accuracy 0.2489\n",
      "Epoch 5 train loss  0.6884 valid loss 0.025 and accuracy 0.2556\n",
      "Epoch 6 train loss  0.6857 valid loss 0.025 and accuracy 0.2612\n",
      "Epoch 7 train loss  0.6804 valid loss 0.025 and accuracy 0.2645\n",
      "Epoch 8 train loss  0.6713 valid loss 0.025 and accuracy 0.2701\n",
      "Epoch 9 train loss  0.6575 valid loss 0.025 and accuracy 0.3237\n",
      "Epoch 10 train loss  0.6210 valid loss 0.026 and accuracy 0.3795\n",
      "Epoch 11 train loss  0.5796 valid loss 0.026 and accuracy 0.4364\n",
      "Epoch 12 train loss  0.5199 valid loss 0.027 and accuracy 0.5089\n",
      "Epoch 13 train loss  0.4638 valid loss 0.029 and accuracy 0.5569\n",
      "Epoch 14 train loss  0.4080 valid loss 0.031 and accuracy 0.5558\n",
      "Epoch 15 train loss  0.3836 valid loss 0.034 and accuracy 0.5681\n",
      "Epoch 16 train loss  0.3422 valid loss 0.035 and accuracy 0.5993\n",
      "Epoch 17 train loss  0.3081 valid loss 0.037 and accuracy 0.5982\n",
      "Epoch 18 train loss  0.2837 valid loss 0.039 and accuracy 0.5993\n",
      "Epoch 19 train loss  0.2686 valid loss 0.043 and accuracy 0.4877\n",
      "Epoch 20 train loss  0.2607 valid loss 0.039 and accuracy 0.6083\n",
      "Epoch 21 train loss  0.2463 valid loss 0.041 and accuracy 0.6150\n",
      "Epoch 22 train loss  0.2313 valid loss 0.041 and accuracy 0.6127\n",
      "Epoch 23 train loss  0.2226 valid loss 0.044 and accuracy 0.6116\n",
      "Epoch 24 train loss  0.2135 valid loss 0.046 and accuracy 0.5424\n",
      "Epoch 25 train loss  0.2030 valid loss 0.045 and accuracy 0.6283\n",
      "Epoch 26 train loss  0.1901 valid loss 0.046 and accuracy 0.6161\n",
      "Epoch 27 train loss  0.1753 valid loss 0.047 and accuracy 0.6161\n",
      "Epoch 28 train loss  0.1776 valid loss 0.048 and accuracy 0.6150\n",
      "Epoch 29 train loss  0.1605 valid loss 0.051 and accuracy 0.6261\n",
      "Epoch 30 train loss  0.1481 valid loss 0.049 and accuracy 0.6317\n",
      "Epoch 31 train loss  0.1514 valid loss 0.049 and accuracy 0.6328\n",
      "Epoch 32 train loss  0.1450 valid loss 0.049 and accuracy 0.6261\n",
      "Epoch 33 train loss  0.1550 valid loss 0.048 and accuracy 0.6328\n",
      "Epoch 34 train loss  0.1300 valid loss 0.056 and accuracy 0.5435\n",
      "Epoch 35 train loss  0.1434 valid loss 0.050 and accuracy 0.6295\n",
      "Epoch 36 train loss  0.1278 valid loss 0.047 and accuracy 0.6194\n",
      "Epoch 37 train loss  0.1359 valid loss 0.050 and accuracy 0.6205\n",
      "Epoch 38 train loss  0.1180 valid loss 0.051 and accuracy 0.6306\n",
      "Epoch 39 train loss  0.1250 valid loss 0.053 and accuracy 0.6306\n",
      "Epoch 40 train loss  0.1219 valid loss 0.050 and accuracy 0.6261\n",
      "Epoch 41 train loss  0.1174 valid loss 0.053 and accuracy 0.6283\n",
      "Epoch 42 train loss  0.1115 valid loss 0.053 and accuracy 0.6239\n",
      "Epoch 43 train loss  0.1106 valid loss 0.052 and accuracy 0.6295\n",
      "Epoch 44 train loss  0.1157 valid loss 0.051 and accuracy 0.6306\n",
      "Epoch 45 train loss  0.1067 valid loss 0.051 and accuracy 0.6306\n",
      "Epoch 46 train loss  0.1086 valid loss 0.052 and accuracy 0.6339\n",
      "Epoch 47 train loss  0.1028 valid loss 0.050 and accuracy 0.6283\n",
      "Epoch 48 train loss  0.0953 valid loss 0.051 and accuracy 0.6261\n",
      "Epoch 49 train loss  0.0945 valid loss 0.052 and accuracy 0.6306\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: chemistry\n",
      "accuracy: tensor([0.2675]), points: 16\n",
      "----------\n",
      "TEST Dominio: chemistry\n",
      "accuracy: tensor([0.3210]), points: 130\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4231 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 22 train loss  57.5758 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 23 train loss  51.2650 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 41 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 42 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 43 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 44 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 45 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 46 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 47 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 48 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 49 train loss  42.4242 valid loss 0.921 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: chemistry\n",
      "accuracy: tensor([0.2061]), points: -40\n",
      "----------\n",
      "TEST Dominio: chemistry\n",
      "accuracy: tensor([0.2402]), points: -18\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "mixed_training_ir = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training_ir, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4358 valid loss 0.212 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8573 valid loss 0.086 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7966 valid loss 0.037 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7037 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6803 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6845 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6865 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6879 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.6827 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.6833 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.6872 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.6905 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.6866 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.6696 valid loss 0.031 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.6623 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.6888 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.6852 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.7114 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.7180 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.7164 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.6918 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.6810 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.6880 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6994 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.7149 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6920 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.6809 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6742 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6786 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6667 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6494 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6806 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6720 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6899 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.6954 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6971 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.6965 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.7157 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.7162 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.7108 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6940 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.7181 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.7224 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.7226 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.7224 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.7106 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.7070 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.7063 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.7080 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.7026 valid loss 0.026 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: chemistry\n",
      "accuracy: tensor([0.2500]), points: 0\n",
      "----------\n",
      "TEST Dominio: chemistry\n",
      "accuracy: tensor([0.2009]), points: -90\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4387 valid loss 0.208 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8319 valid loss 0.106 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.8092 valid loss 0.043 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7443 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6813 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.6797 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.6848 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6916 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.6899 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7004 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7016 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7006 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.7029 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7053 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.7138 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7013 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.6928 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.6968 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.7037 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.7097 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7089 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.7059 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.7120 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6992 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.6993 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.7078 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.7087 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.7031 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6969 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6958 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6943 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6955 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6932 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6942 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.6966 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6974 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.7019 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.6908 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.6727 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.6736 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6638 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.6588 valid loss 0.028 and accuracy 0.2612\n",
      "Epoch 42 train loss  0.6284 valid loss 0.030 and accuracy 0.2679\n",
      "Epoch 43 train loss  0.5555 valid loss 0.036 and accuracy 0.2667\n",
      "Epoch 44 train loss  0.4864 valid loss 0.039 and accuracy 0.2991\n",
      "Epoch 45 train loss  0.4200 valid loss 0.043 and accuracy 0.3304\n",
      "Epoch 46 train loss  0.3503 valid loss 0.050 and accuracy 0.3348\n",
      "Epoch 47 train loss  0.3079 valid loss 0.035 and accuracy 0.3940\n",
      "Epoch 48 train loss  0.2558 valid loss 0.033 and accuracy 0.4576\n",
      "Epoch 49 train loss  0.2023 valid loss 0.035 and accuracy 0.4554\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: chemistry\n",
      "accuracy: tensor([0.2632]), points: 12\n",
      "----------\n",
      "TEST Dominio: chemistry\n",
      "accuracy: tensor([0.3057]), points: 102\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-ultimate",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "welcome-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models_v2/logistic_regressor_chemistry\n",
      "DEV\n",
      "Accuracy media 0.21052632\n",
      "Puntos media -36.0\n",
      "[tensor(0.2105)]\n",
      "[-36]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23548257\n",
      "Puntos media -13.0\n",
      "[tensor(0.2727), tensor(0.1982)]\n",
      "[21, -47]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.26754385\n",
      "Puntos media 16.0\n",
      "[tensor(0.2675)]\n",
      "[16]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.3207754\n",
      "Puntos media 65.0\n",
      "[tensor(0.3420), tensor(0.2996)]\n",
      "[85, 45]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.20614035\n",
      "Puntos media -40.0\n",
      "[tensor(0.2061)]\n",
      "[-40]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.23981157\n",
      "Puntos media -9.0\n",
      "[tensor(0.2814), tensor(0.1982)]\n",
      "[29, -47]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models_v2/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models_v2/lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models_v2/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "entertaining-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.25\n",
      "Puntos media 0.0\n",
      "[tensor(0.2500)]\n",
      "[0]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.20085055\n",
      "Puntos media -45.0\n",
      "[tensor(0.2035), tensor(0.1982)]\n",
      "[-43, -47]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.2631579\n",
      "Puntos media 12.0\n",
      "[tensor(0.2632)]\n",
      "[12]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.3056239\n",
      "Puntos media 51.0\n",
      "[tensor(0.3117), tensor(0.2996)]\n",
      "[57, 45]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models_v2/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-metallic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
