{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vocabulary, Vectorizer, HeadQA, clean_words, parse_dataset, save_dataset_to_pickle, load_dataset_from_pickle \n",
    "from utils_data import random_oversamplig, mixed_oversampling, similarity_instance, translate_instance, translate_instance_ir, similarity_instance_ir\n",
    "\n",
    "from training import train, validate, evaluate, evaluator, evaluate_better, get_optimizer\n",
    "\n",
    "from supervised_models import LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boring-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sonic-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adapted-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_instances = parse_dataset(training)\n",
    "# validation_instances = parse_dataset(validation)\n",
    "# testing_instances = parse_dataset(testing)\n",
    "\n",
    "# oversampled_training = random_oversamplig(training_instances)\n",
    "# mixed_oversampling_training = mixed_oversampling(training_instances, translate_instance, similarity_instance)\n",
    "\n",
    "# save_dataset_to_pickle('../data/training.pickle', training_instances)\n",
    "# save_dataset_to_pickle('../data/validation.pickle', validation_instances)\n",
    "# save_dataset_to_pickle('../data/testing.pickle', testing_instances)\n",
    "# save_dataset_to_pickle('../data/oversampled_training.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "urban-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "running-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(mixed_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "modular-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=mixed_training, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "overall-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tamil-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(model, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "friendly-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  51.0915 valid loss 0.322 and accuracy 0.4458\n",
      "Epoch 1 train loss  51.4025 valid loss 0.360 and accuracy 0.4662\n",
      "Epoch 2 train loss  48.6275 valid loss 0.322 and accuracy 0.5327\n",
      "Epoch 3 train loss  47.6087 valid loss 0.303 and accuracy 0.5414\n",
      "Epoch 4 train loss  50.0376 valid loss 0.322 and accuracy 0.3945\n",
      "Epoch 5 train loss  54.0387 valid loss 0.379 and accuracy 0.3305\n",
      "Epoch 6 train loss  55.0258 valid loss 0.361 and accuracy 0.3381\n",
      "Epoch 7 train loss  53.9642 valid loss 0.379 and accuracy 0.4131\n",
      "Epoch 8 train loss  53.0682 valid loss 0.360 and accuracy 0.3776\n",
      "Epoch 9 train loss  54.5461 valid loss 0.417 and accuracy 0.3818\n",
      "Epoch 10 train loss  55.9252 valid loss 0.436 and accuracy 0.2763\n",
      "Epoch 11 train loss  57.0213 valid loss 0.398 and accuracy 0.2651\n",
      "Epoch 12 train loss  57.0392 valid loss 0.436 and accuracy 0.2728\n",
      "Epoch 13 train loss  56.8008 valid loss 0.379 and accuracy 0.2895\n",
      "Epoch 14 train loss  56.3669 valid loss 0.455 and accuracy 0.2994\n",
      "Epoch 15 train loss  56.3736 valid loss 0.417 and accuracy 0.3092\n",
      "Epoch 16 train loss  56.4660 valid loss 0.379 and accuracy 0.3086\n",
      "Epoch 17 train loss  56.3457 valid loss 0.360 and accuracy 0.3204\n",
      "Epoch 18 train loss  55.8400 valid loss 0.512 and accuracy 0.2901\n",
      "Epoch 19 train loss  56.8238 valid loss 0.398 and accuracy 0.2921\n"
     ]
    }
   ],
   "source": [
    "training_results = train(model, optimizer, train_dt, valid_dt, validate, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "neutral-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stunning-departure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2321]), -98)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "turned-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occupied-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2403]), -106)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nonprofit-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models_v2/logistic_regressor'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "serious-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('results_v2/train_results_lreg_v2.pickle', training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fresh-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = load_dataset_from_pickle('results_v2/train_results_lreg_v2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "preliminary-living",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(trainset.max_length, 1)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "critical-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23211032,\n",
       " -16.333333333333332,\n",
       " [tensor(0.2566),\n",
       "  tensor(0.2043),\n",
       "  tensor(0.2267),\n",
       "  tensor(0.2511),\n",
       "  tensor(0.2478),\n",
       "  tensor(0.2061)],\n",
       " [6, -42, -21, 1, -2, -40])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ongoing-column",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24021287,\n",
       " -8.833333333333334,\n",
       " [tensor(0.2632),\n",
       "  tensor(0.2242),\n",
       "  tensor(0.2237),\n",
       "  tensor(0.2414),\n",
       "  tensor(0.2478),\n",
       "  tensor(0.2727),\n",
       "  tensor(0.2434),\n",
       "  tensor(0.2586),\n",
       "  tensor(0.2358),\n",
       "  tensor(0.2424),\n",
       "  tensor(0.2311),\n",
       "  tensor(0.1982)],\n",
       " [12, -23, -24, -8, -2, 21, -6, 8, -13, -7, -17, -47])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-virtue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
