{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'nursery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  45.9939 valid loss 0.921 and accuracy 0.7176\n",
      "Epoch 1 train loss  41.5816 valid loss 0.806 and accuracy 0.6373\n",
      "Epoch 2 train loss  48.7944 valid loss 2.431 and accuracy 0.3984\n",
      "Epoch 3 train loss  52.9548 valid loss 2.762 and accuracy 0.2924\n",
      "Epoch 4 train loss  56.2437 valid loss 2.762 and accuracy 0.2578\n",
      "Epoch 5 train loss  57.0490 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 6 train loss  57.1488 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 7 train loss  57.1488 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 8 train loss  57.1485 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 9 train loss  57.1485 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 10 train loss  57.0547 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 11 train loss  57.0546 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 12 train loss  57.0545 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 13 train loss  57.0544 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 14 train loss  57.0535 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 15 train loss  57.0529 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 16 train loss  57.0523 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 17 train loss  57.0517 valid loss 2.762 and accuracy 0.2533\n",
      "Epoch 18 train loss  57.0542 valid loss 2.762 and accuracy 0.2511\n",
      "Epoch 19 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 22 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 23 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 24 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 25 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 26 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 27 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 28 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 29 train loss  57.1801 valid loss 2.762 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: nursery\n",
      "accuracy: tensor([0.2087]), points: -38\n",
      "----------\n",
      "TEST Dominio: nursery\n",
      "accuracy: tensor([0.2440]), points: -11\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.7161 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7027 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6951 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6930 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6900 valid loss 0.025 and accuracy 0.2511\n",
      "Epoch 5 train loss  0.6879 valid loss 0.025 and accuracy 0.2556\n",
      "Epoch 6 train loss  0.6838 valid loss 0.025 and accuracy 0.2667\n",
      "Epoch 7 train loss  0.6784 valid loss 0.025 and accuracy 0.2746\n",
      "Epoch 8 train loss  0.6769 valid loss 0.024 and accuracy 0.2801\n",
      "Epoch 9 train loss  0.6662 valid loss 0.024 and accuracy 0.2879\n",
      "Epoch 10 train loss  0.6531 valid loss 0.024 and accuracy 0.3147\n",
      "Epoch 11 train loss  0.6303 valid loss 0.023 and accuracy 0.4062\n",
      "Epoch 12 train loss  0.5987 valid loss 0.022 and accuracy 0.4565\n",
      "Epoch 13 train loss  0.5464 valid loss 0.022 and accuracy 0.4721\n",
      "Epoch 14 train loss  0.5042 valid loss 0.021 and accuracy 0.5123\n",
      "Epoch 15 train loss  0.4577 valid loss 0.021 and accuracy 0.5257\n",
      "Epoch 16 train loss  0.4146 valid loss 0.022 and accuracy 0.5335\n",
      "Epoch 17 train loss  0.3739 valid loss 0.022 and accuracy 0.5379\n",
      "Epoch 18 train loss  0.3469 valid loss 0.024 and accuracy 0.5502\n",
      "Epoch 19 train loss  0.3181 valid loss 0.024 and accuracy 0.5525\n",
      "Epoch 20 train loss  0.3038 valid loss 0.027 and accuracy 0.5658\n",
      "Epoch 21 train loss  0.2809 valid loss 0.030 and accuracy 0.5491\n",
      "Epoch 22 train loss  0.2680 valid loss 0.031 and accuracy 0.5569\n",
      "Epoch 23 train loss  0.2606 valid loss 0.028 and accuracy 0.5636\n",
      "Epoch 24 train loss  0.2479 valid loss 0.030 and accuracy 0.5670\n",
      "Epoch 25 train loss  0.2243 valid loss 0.035 and accuracy 0.5703\n",
      "Epoch 26 train loss  0.2242 valid loss 0.035 and accuracy 0.5781\n",
      "Epoch 27 train loss  0.2183 valid loss 0.033 and accuracy 0.5670\n",
      "Epoch 28 train loss  0.2045 valid loss 0.037 and accuracy 0.5625\n",
      "Epoch 29 train loss  0.2039 valid loss 0.038 and accuracy 0.5703\n",
      "Epoch 30 train loss  0.2003 valid loss 0.035 and accuracy 0.5647\n",
      "Epoch 31 train loss  0.1975 valid loss 0.038 and accuracy 0.5658\n",
      "Epoch 32 train loss  0.1828 valid loss 0.042 and accuracy 0.5692\n",
      "Epoch 33 train loss  0.1952 valid loss 0.039 and accuracy 0.5513\n",
      "Epoch 34 train loss  0.1947 valid loss 0.045 and accuracy 0.5748\n",
      "Epoch 35 train loss  0.1784 valid loss 0.042 and accuracy 0.5681\n",
      "Epoch 36 train loss  0.1734 valid loss 0.040 and accuracy 0.5647\n",
      "Epoch 37 train loss  0.1771 valid loss 0.041 and accuracy 0.5658\n",
      "Epoch 38 train loss  0.1733 valid loss 0.041 and accuracy 0.5781\n",
      "Epoch 39 train loss  0.1639 valid loss 0.043 and accuracy 0.5714\n",
      "Epoch 40 train loss  0.1705 valid loss 0.047 and accuracy 0.5871\n",
      "Epoch 41 train loss  0.1564 valid loss 0.049 and accuracy 0.5748\n",
      "Epoch 42 train loss  0.1621 valid loss 0.046 and accuracy 0.5837\n",
      "Epoch 43 train loss  0.1654 valid loss 0.049 and accuracy 0.5592\n",
      "Epoch 44 train loss  0.1629 valid loss 0.046 and accuracy 0.5658\n",
      "Epoch 45 train loss  0.1621 valid loss 0.046 and accuracy 0.5748\n",
      "Epoch 46 train loss  0.1599 valid loss 0.048 and accuracy 0.5592\n",
      "Epoch 47 train loss  0.1505 valid loss 0.051 and accuracy 0.5781\n",
      "Epoch 48 train loss  0.1617 valid loss 0.043 and accuracy 0.5558\n",
      "Epoch 49 train loss  0.1511 valid loss 0.048 and accuracy 0.5770\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: nursery\n",
      "accuracy: tensor([0.2087]), points: -38\n",
      "----------\n",
      "TEST Dominio: nursery\n",
      "accuracy: tensor([0.2703]), points: 37\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4468 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  57.1429 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  48.2518 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 20 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 21 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 22 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 23 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 24 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 25 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 41 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 42 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 43 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 44 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 45 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 46 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 47 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 48 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 49 train loss  42.8571 valid loss 0.921 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: nursery\n",
      "accuracy: tensor([0.2087]), points: -38\n",
      "----------\n",
      "TEST Dominio: nursery\n",
      "accuracy: tensor([0.2440]), points: -11\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "mixed_training_ir = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training_ir, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4733 valid loss 0.138 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8241 valid loss 0.080 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7956 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7441 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.7352 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.7297 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.7274 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.7223 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.7252 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7186 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7092 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7044 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.7038 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7003 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.6996 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7039 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7125 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.6952 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.6659 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.6830 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.6894 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.6774 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.7001 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.7072 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.7144 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.7169 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.7031 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6992 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6980 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6983 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6971 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6950 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.7075 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6821 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.6847 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.7081 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.7068 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.7157 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.7062 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.7030 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6961 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.6947 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.6867 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.6724 valid loss 0.029 and accuracy 0.2511\n",
      "Epoch 44 train loss  0.6046 valid loss 0.035 and accuracy 0.2522\n",
      "Epoch 45 train loss  0.5130 valid loss 0.053 and accuracy 0.2533\n",
      "Epoch 46 train loss  0.4483 valid loss 0.054 and accuracy 0.2645\n",
      "Epoch 47 train loss  0.3510 valid loss 0.067 and accuracy 0.2891\n",
      "Epoch 48 train loss  0.2410 valid loss 0.074 and accuracy 0.3092\n",
      "Epoch 49 train loss  0.1572 valid loss 0.079 and accuracy 0.3371\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: nursery\n",
      "accuracy: tensor([0.2478]), points: -2\n",
      "----------\n",
      "TEST Dominio: nursery\n",
      "accuracy: tensor([0.2659]), points: 29\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4700 valid loss 0.156 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.8508 valid loss 0.068 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7906 valid loss 0.036 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7671 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.7221 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.7170 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.7242 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.7213 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.7193 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7137 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7035 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7028 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.7012 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7032 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.7193 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7138 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7032 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.7035 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.6818 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.6600 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7041 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.7074 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.7122 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.7190 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.7124 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.7168 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.7026 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6991 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6997 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6964 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6969 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6964 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6922 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6911 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.7014 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6889 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.6641 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.6661 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.6643 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.5853 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.5588 valid loss 0.035 and accuracy 0.2589\n",
      "Epoch 41 train loss  0.4456 valid loss 0.046 and accuracy 0.2556\n",
      "Epoch 42 train loss  0.3449 valid loss 0.075 and accuracy 0.2589\n",
      "Epoch 43 train loss  0.2819 valid loss 0.053 and accuracy 0.2958\n",
      "Epoch 44 train loss  0.1964 valid loss 0.052 and accuracy 0.3482\n",
      "Epoch 45 train loss  0.1434 valid loss 0.054 and accuracy 0.3906\n",
      "Epoch 46 train loss  0.1116 valid loss 0.061 and accuracy 0.3616\n",
      "Epoch 47 train loss  0.0938 valid loss 0.054 and accuracy 0.4051\n",
      "Epoch 48 train loss  0.0826 valid loss 0.045 and accuracy 0.4487\n",
      "Epoch 49 train loss  0.0640 valid loss 0.044 and accuracy 0.4509\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: nursery\n",
      "accuracy: tensor([0.1957]), points: -50\n",
      "----------\n",
      "TEST Dominio: nursery\n",
      "accuracy: tensor([0.2505]), points: 1\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-sensitivity",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "above-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models_v2/logistic_regressor_nursery\n",
      "DEV\n",
      "Accuracy media 0.20869565\n",
      "Puntos media -38.0\n",
      "[tensor(0.2087)]\n",
      "[-38]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2437471\n",
      "Puntos media -5.5\n",
      "[tensor(0.2332), tensor(0.2543)]\n",
      "[-15, 4]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.20869565\n",
      "Puntos media -38.0\n",
      "[tensor(0.2087)]\n",
      "[-38]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.270218\n",
      "Puntos media 18.5\n",
      "[tensor(0.2646), tensor(0.2759)]\n",
      "[13, 24]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.20869565\n",
      "Puntos media -38.0\n",
      "[tensor(0.2087)]\n",
      "[-38]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2437471\n",
      "Puntos media -5.5\n",
      "[tensor(0.2332), tensor(0.2543)]\n",
      "[-15, 4]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [f'trained_models_v2/logistic_regressor_{CATEGORY}', \n",
    "         f'trained_models_v2/lstm_{CATEGORY}',         \n",
    "         f'trained_models_v2/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "starting-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.24782608\n",
      "Puntos media -2.0\n",
      "[tensor(0.2478)]\n",
      "[-2]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.2659077\n",
      "Puntos media 14.5\n",
      "[tensor(0.2646), tensor(0.2672)]\n",
      "[13, 16]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.19565217\n",
      "Puntos media -50.0\n",
      "[tensor(0.1957)]\n",
      "[-50]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.25064752\n",
      "Puntos media 0.5\n",
      "[tensor(0.2556), tensor(0.2457)]\n",
      "[5, -4]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [f'trained_models_v2/lstm_qa_{CATEGORY}',\n",
    "         f'trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
