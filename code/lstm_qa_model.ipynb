{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rural-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vectorizer, HeadQA, HeadQA_IR, clean_words, parse_dataset, parse_ir_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from utils_data import random_undersampling\n",
    "from training import evaluate, train_ir, validate_ir, evaluator_ir, load_embeddings_from_file, make_embedding_matrix\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "from ir_models import LSTM_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "applied-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_instances = parse_ir_dataset(training)\n",
    "#validation_instances = parse_ir_dataset(validation)\n",
    "#testing_instances = parse_ir_dataset(testing)\n",
    "\n",
    "#oversampled_training = random_oversamplig(training_instances)\n",
    "#undersampled_training = random_undersampling(training_instances)\n",
    "\n",
    "#save_dataset_to_pickle('../data/training_ir.pickle', training_instances)\n",
    "#save_dataset_to_pickle('../data/validation_ir.pickle', validation_instances)\n",
    "#save_dataset_to_pickle('../data/testing_ir.pickle', testing_instances)\n",
    "#save_dataset_to_pickle('../data/oversampled_training_ir.pickle', oversampled_training)\n",
    "# save_dataset_to_pickle('../data/undersampled_training_ir.pickle', undersampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reasonable-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')\n",
    "undersampled_training = load_dataset_from_pickle('../data/undersampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controlling-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amino-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greek-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incorrect-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "organized-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tough-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "silent-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indirect-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "applicable-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ir(model, dataloader):\n",
    "    model.eval()\n",
    "    loss, right, total = 0, 0, 0\n",
    "    y_true, y_preds = [], []\n",
    "    for x_0, x_1, y in dataloader:\n",
    "        batch = y.shape[0]\n",
    "        out = model(x_0.long(), x_1.long())\n",
    "        loss = F.binary_cross_entropy(out, y.float())\n",
    "        loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        # pred = torch.max(out, dim=1)[1]\n",
    "        pred = torch.where(out > 0.4, 1, 0)\n",
    "        y_true.append(y)\n",
    "        y_preds.append(pred)\n",
    "        right += (pred == y).float().sum().item()\n",
    "    return loss/total, right/total, y_true, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "designing-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.5017 valid loss 0.003 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.4927 valid loss 0.003 and accuracy 0.7496\n",
      "Epoch 2 train loss  0.4625 valid loss 0.004 and accuracy 0.7294\n",
      "Epoch 3 train loss  0.4077 valid loss 0.005 and accuracy 0.6996\n",
      "Epoch 4 train loss  0.3547 valid loss 0.005 and accuracy 0.7066\n",
      "Epoch 5 train loss  0.2838 valid loss 0.005 and accuracy 0.6776\n",
      "Epoch 6 train loss  0.2543 valid loss 0.005 and accuracy 0.6524\n",
      "Epoch 7 train loss  0.1998 valid loss 0.005 and accuracy 0.6452\n",
      "Epoch 8 train loss  0.1666 valid loss 0.007 and accuracy 0.7204\n",
      "Epoch 9 train loss  0.1435 valid loss 0.009 and accuracy 0.6851\n",
      "Epoch 10 train loss  0.1217 valid loss 0.009 and accuracy 0.6562\n",
      "Epoch 11 train loss  0.1068 valid loss 0.007 and accuracy 0.6301\n",
      "Epoch 12 train loss  0.0974 valid loss 0.008 and accuracy 0.6557\n",
      "Epoch 13 train loss  0.0780 valid loss 0.009 and accuracy 0.6564\n",
      "Epoch 14 train loss  0.0770 valid loss 0.008 and accuracy 0.6404\n",
      "Epoch 15 train loss  0.0686 valid loss 0.010 and accuracy 0.6838\n",
      "Epoch 16 train loss  0.0638 valid loss 0.010 and accuracy 0.6443\n",
      "Epoch 17 train loss  0.0578 valid loss 0.011 and accuracy 0.6877\n",
      "Epoch 18 train loss  0.0586 valid loss 0.010 and accuracy 0.6101\n",
      "Epoch 19 train loss  0.0490 valid loss 0.013 and accuracy 0.6893\n",
      "Epoch 20 train loss  0.0421 valid loss 0.011 and accuracy 0.6649\n",
      "Epoch 21 train loss  0.0484 valid loss 0.011 and accuracy 0.6711\n",
      "Epoch 22 train loss  0.0537 valid loss 0.010 and accuracy 0.6631\n",
      "Epoch 23 train loss  0.0486 valid loss 0.009 and accuracy 0.6572\n",
      "Epoch 24 train loss  0.0431 valid loss 0.013 and accuracy 0.7039\n",
      "Epoch 25 train loss  0.0414 valid loss 0.012 and accuracy 0.6715\n",
      "Epoch 26 train loss  0.0383 valid loss 0.011 and accuracy 0.6619\n",
      "Epoch 27 train loss  0.0423 valid loss 0.012 and accuracy 0.6866\n",
      "Epoch 28 train loss  0.0420 valid loss 0.014 and accuracy 0.6960\n",
      "Epoch 29 train loss  0.0346 valid loss 0.013 and accuracy 0.6750\n",
      "Epoch 30 train loss  0.0373 valid loss 0.013 and accuracy 0.7000\n",
      "Epoch 31 train loss  0.0539 valid loss 0.013 and accuracy 0.6866\n",
      "Epoch 32 train loss  0.0341 valid loss 0.013 and accuracy 0.6827\n",
      "Epoch 33 train loss  0.0323 valid loss 0.011 and accuracy 0.6910\n",
      "Epoch 34 train loss  0.0289 valid loss 0.012 and accuracy 0.6735\n",
      "Epoch 35 train loss  0.0288 valid loss 0.011 and accuracy 0.6783\n",
      "Epoch 36 train loss  0.0286 valid loss 0.013 and accuracy 0.6993\n",
      "Epoch 37 train loss  0.0270 valid loss 0.013 and accuracy 0.6921\n",
      "Epoch 38 train loss  0.0328 valid loss 0.011 and accuracy 0.6465\n",
      "Epoch 39 train loss  0.0290 valid loss 0.012 and accuracy 0.6904\n",
      "Epoch 40 train loss  0.0254 valid loss 0.013 and accuracy 0.6658\n",
      "Epoch 41 train loss  0.0264 valid loss 0.012 and accuracy 0.6914\n",
      "Epoch 42 train loss  0.0260 valid loss 0.014 and accuracy 0.6489\n",
      "Epoch 43 train loss  0.0351 valid loss 0.012 and accuracy 0.6601\n",
      "Epoch 44 train loss  0.0353 valid loss 0.013 and accuracy 0.6895\n",
      "Epoch 45 train loss  0.0289 valid loss 0.012 and accuracy 0.6904\n",
      "Epoch 46 train loss  0.0253 valid loss 0.010 and accuracy 0.6695\n",
      "Epoch 47 train loss  0.0219 valid loss 0.013 and accuracy 0.6458\n",
      "Epoch 48 train loss  0.0267 valid loss 0.015 and accuracy 0.6831\n",
      "Epoch 49 train loss  0.0266 valid loss 0.012 and accuracy 0.6858\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(model, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "raising-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2584]), 46)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greatest-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2659]), 174)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "secret-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('../data/train_results_lstm_qa_sig.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('../data/train_results_lstm_qa_sig.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spiritual-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/lstm_qa_sig'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
