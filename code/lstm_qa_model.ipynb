{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "rural-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vectorizer, HeadQA, HeadQA_IR, clean_words, parse_dataset, parse_ir_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import evaluate, train_ir, validate_ir, evaluator_ir, load_embeddings_from_file, make_embedding_matrix\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "applied-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_instances = parse_ir_dataset(training)\n",
    "#validation_instances = parse_ir_dataset(validation)\n",
    "#testing_instances = parse_ir_dataset(testing)\n",
    "\n",
    "#oversampled_training = random_oversamplig(training_instances)\n",
    "\n",
    "#save_dataset_to_pickle('../data/training_ir.pickle', training_instances)\n",
    "#save_dataset_to_pickle('../data/validation_ir.pickle', validation_instances)\n",
    "#save_dataset_to_pickle('../data/testing_ir.pickle', testing_instances)\n",
    "#save_dataset_to_pickle('../data/oversampled_training_ir.pickle', oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reasonable-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "oversampled_training = load_dataset_from_pickle('../data/oversampled_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intermediate-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Los potenciales postsinápticos excitadores:',\n",
       " 'answer': 'Son de tipo todo o nada.',\n",
       " 'tok_qtext': ['Los', 'potenciales', 'postsinápticos', 'excitadores', ':'],\n",
       " 'tok_atext': ['Son', 'de', 'tipo', 'todo', 'o', 'nada', '.'],\n",
       " 'label': 0,\n",
       " 'category': 'biology'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(oversampled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controlling-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amino-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "greek-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "transsexual-growth",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LSTM_QA(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, x_size, n_classes, embedding_size=300,\n",
    "                 padding_idx=0, pretrained_embeddings=None): \n",
    "        super(LSTM_QA, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size,num_embeddings=self.vocab_size,\n",
    "                                    padding_idx=padding_idx)\n",
    "        else:\n",
    "            print('Loading pretrained embeddings...')\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=self.embedding_size, num_embeddings=self.vocab_size,\n",
    "                                    padding_idx=padding_idx, _weight=pretrained_embeddings)\n",
    "            self.emb.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True, dropout=0.5,bidirectional=True)\n",
    "        self.cosine = nn.CosineSimilarity(dim=1)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, 64)  \n",
    "        self.linear1 = nn.Linear(64, self.n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_0, x_1):\n",
    "        x_0 = self.emb(x_0)\n",
    "        x_1 = self.emb(x_1)\n",
    "        out_0, (ht_0, ct_0) = self.lstm(x_0)\n",
    "        out_1, (ht_1, ct_1) = self.lstm(x_1)        \n",
    "        x = self.cosine(out_0, out_1)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "incorrect-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "organized-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "# word_to_idx, embeddings = load_embeddings_from_file(embedding_file)\n",
    "\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle', word_to_idx)\n",
    "# save_dataset_to_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "tough-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "silent-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "indirect-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "applicable-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ir(model, dataloader):\n",
    "    model.eval()\n",
    "    loss, right, total = 0, 0, 0\n",
    "    y_true, y_preds = [], []\n",
    "    for x_0, x_1, y in dataloader:\n",
    "        batch = y.shape[0]\n",
    "        out = model(x_0.long(), x_1.long())\n",
    "        loss = F.binary_cross_entropy(out, y.float())\n",
    "        loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        # pred = torch.max(out, dim=1)[1]\n",
    "        pred = torch.where(out > 0.4, 1, 0)\n",
    "        y_true.append(y)\n",
    "        y_preds.append(pred)\n",
    "        right += (pred == y).float().sum().item()\n",
    "    return loss/total, right/total, y_true, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "designing-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6901 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 1 train loss  0.6566 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 2 train loss  0.6217 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 3 train loss  0.5883 valid loss 0.005 and accuracy 0.7500\n",
      "Epoch 4 train loss  0.5594 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 5 train loss  0.5392 valid loss 0.006 and accuracy 0.7494\n",
      "Epoch 6 train loss  0.5273 valid loss 0.005 and accuracy 0.7494\n",
      "Epoch 7 train loss  0.5106 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 8 train loss  0.4978 valid loss 0.006 and accuracy 0.7485\n",
      "Epoch 9 train loss  0.4886 valid loss 0.006 and accuracy 0.7493\n",
      "Epoch 10 train loss  0.4845 valid loss 0.006 and accuracy 0.7493\n",
      "Epoch 11 train loss  0.4787 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 12 train loss  0.4733 valid loss 0.006 and accuracy 0.7496\n",
      "Epoch 13 train loss  0.4690 valid loss 0.006 and accuracy 0.7506\n",
      "Epoch 14 train loss  0.4654 valid loss 0.006 and accuracy 0.7483\n",
      "Epoch 15 train loss  0.4615 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 16 train loss  0.4600 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 17 train loss  0.4543 valid loss 0.006 and accuracy 0.7498\n",
      "Epoch 18 train loss  0.4540 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 19 train loss  0.4509 valid loss 0.006 and accuracy 0.7485\n",
      "Epoch 20 train loss  0.4483 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 21 train loss  0.4454 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 22 train loss  0.4425 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 23 train loss  0.4471 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 24 train loss  0.4422 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 25 train loss  0.4410 valid loss 0.007 and accuracy 0.7487\n",
      "Epoch 26 train loss  0.4414 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 27 train loss  0.4382 valid loss 0.007 and accuracy 0.7506\n",
      "Epoch 28 train loss  0.4370 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 29 train loss  0.4350 valid loss 0.007 and accuracy 0.7489\n",
      "Epoch 30 train loss  0.4365 valid loss 0.006 and accuracy 0.7502\n",
      "Epoch 31 train loss  0.4348 valid loss 0.007 and accuracy 0.7506\n",
      "Epoch 32 train loss  0.4336 valid loss 0.006 and accuracy 0.7494\n",
      "Epoch 33 train loss  0.4333 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 34 train loss  0.4328 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 35 train loss  0.4310 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 36 train loss  0.4318 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 37 train loss  0.4280 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 38 train loss  0.4259 valid loss 0.007 and accuracy 0.7487\n",
      "Epoch 39 train loss  0.4262 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 40 train loss  0.4278 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 41 train loss  0.4279 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 42 train loss  0.4251 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 43 train loss  0.4241 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 44 train loss  0.4240 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 45 train loss  0.4238 valid loss 0.006 and accuracy 0.7500\n",
      "Epoch 46 train loss  0.4249 valid loss 0.006 and accuracy 0.7494\n",
      "Epoch 47 train loss  0.4226 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 48 train loss  0.4213 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 49 train loss  0.4241 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 50 train loss  0.4214 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 51 train loss  0.4192 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 52 train loss  0.4196 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 53 train loss  0.4208 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 54 train loss  0.4190 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 55 train loss  0.4202 valid loss 0.006 and accuracy 0.7496\n",
      "Epoch 56 train loss  0.4178 valid loss 0.007 and accuracy 0.7487\n",
      "Epoch 57 train loss  0.4189 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 58 train loss  0.4192 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 59 train loss  0.4160 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 60 train loss  0.4168 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 61 train loss  0.4181 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 62 train loss  0.4166 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 63 train loss  0.4132 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 64 train loss  0.4148 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 65 train loss  0.4166 valid loss 0.008 and accuracy 0.7491\n",
      "Epoch 66 train loss  0.4147 valid loss 0.007 and accuracy 0.7489\n",
      "Epoch 67 train loss  0.4134 valid loss 0.008 and accuracy 0.7494\n",
      "Epoch 68 train loss  0.4140 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 69 train loss  0.4148 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 70 train loss  0.4141 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 71 train loss  0.4144 valid loss 0.007 and accuracy 0.7487\n",
      "Epoch 72 train loss  0.4135 valid loss 0.007 and accuracy 0.7493\n",
      "Epoch 73 train loss  0.4133 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 74 train loss  0.4154 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 75 train loss  0.4120 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 76 train loss  0.4128 valid loss 0.006 and accuracy 0.7494\n",
      "Epoch 77 train loss  0.4113 valid loss 0.007 and accuracy 0.7491\n",
      "Epoch 78 train loss  0.4117 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 79 train loss  0.4094 valid loss 0.008 and accuracy 0.7493\n",
      "Epoch 80 train loss  0.4106 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 81 train loss  0.4103 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 82 train loss  0.4098 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 83 train loss  0.4122 valid loss 0.007 and accuracy 0.7506\n",
      "Epoch 84 train loss  0.4109 valid loss 0.007 and accuracy 0.7506\n",
      "Epoch 85 train loss  0.4083 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 86 train loss  0.4092 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 87 train loss  0.4086 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 88 train loss  0.4092 valid loss 0.007 and accuracy 0.7504\n",
      "Epoch 89 train loss  0.4084 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 90 train loss  0.4076 valid loss 0.007 and accuracy 0.7500\n",
      "Epoch 91 train loss  0.4076 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 92 train loss  0.4082 valid loss 0.008 and accuracy 0.7509\n",
      "Epoch 93 train loss  0.4081 valid loss 0.007 and accuracy 0.7494\n",
      "Epoch 94 train loss  0.4072 valid loss 0.007 and accuracy 0.7502\n",
      "Epoch 95 train loss  0.4083 valid loss 0.007 and accuracy 0.7496\n",
      "Epoch 96 train loss  0.4084 valid loss 0.007 and accuracy 0.7498\n",
      "Epoch 97 train loss  0.4080 valid loss 0.008 and accuracy 0.7500\n",
      "Epoch 98 train loss  0.4056 valid loss 0.008 and accuracy 0.7502\n",
      "Epoch 99 train loss  0.4059 valid loss 0.007 and accuracy 0.7494\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(model, optimizer, train_dt, valid_dt, validate_ir, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "raising-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2775]), 150)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "greatest-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2695]), 214)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "spiritual-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models/lstm_qa'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
