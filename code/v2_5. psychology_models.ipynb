{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import  Vocabulary, Vectorizer, HeadQA, HeadQA_IR\n",
    "from utils_data import parse_dataset, parse_ir_dataset, random_oversamplig, random_undersampling\n",
    "from utils_data import filter_by_category, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "import training\n",
    "from training import get_optimizer, train, train_ir, validate, validate_ir, evaluator, evaluator_ir, evaluate\n",
    "from training import load_embeddings_from_file, make_embedding_matrix\n",
    "from training import pad_seq, encoder_bert, encoder_bert_ir, encoder_bert_instance, encoder_bert_ir_instance\n",
    "from training import evaluator_bert, evaluator_bert_ir, evaluate_better\n",
    "\n",
    "from supervised_models import LogisticRegression, BasicLSTM, BiLSTM_model\n",
    "from ir_models import LSTM_QA, LSTM_CNN_QA, BERT_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "former-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'psychology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )\n",
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tennessee",
   "metadata": {},
   "source": [
    "### Modelos supervisados puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing.pickle')\n",
    "\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "behavioral-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "subtle-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_training(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sonic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "validset = HeadQA(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=30)\n",
    "testset = HeadQA(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hungarian-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-acceptance",
   "metadata": {},
   "source": [
    "#### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fleet-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "optimizer = get_optimizer(logistic_regressor, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  43.0091 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 1 train loss  42.7286 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 2 train loss  42.7286 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 3 train loss  42.7286 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 4 train loss  42.7286 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 5 train loss  42.7286 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 6 train loss  42.7285 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 7 train loss  42.7285 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 8 train loss  42.7285 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 9 train loss  42.7285 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 10 train loss  42.7285 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 11 train loss  42.7284 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 12 train loss  42.7284 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 13 train loss  42.7284 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 14 train loss  42.7284 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 15 train loss  42.7283 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 16 train loss  42.7283 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 17 train loss  42.7283 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 18 train loss  42.7282 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 19 train loss  42.7282 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 20 train loss  42.7281 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 21 train loss  42.7281 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 22 train loss  42.7280 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 23 train loss  42.7280 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 24 train loss  42.7279 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 25 train loss  42.7278 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 26 train loss  42.7278 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 27 train loss  42.7277 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 28 train loss  42.7276 valid loss 0.921 and accuracy 0.7422\n",
      "Epoch 29 train loss  42.7275 valid loss 0.921 and accuracy 0.7422\n"
     ]
    }
   ],
   "source": [
    "training_results = train(logistic_regressor, optimizer, train_dt, valid_dt, validate, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2611]), points: 10\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2418]), points: -15\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(logistic_regressor, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(logistic_regressor, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/logistic_regressor_{CATEGORY}'\n",
    "torch.save(logistic_regressor.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-muscle",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rolled-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "optimizer = get_optimizer(lstm, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "laughing-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.6869 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.6977 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6913 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.6887 valid loss 0.024 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.6866 valid loss 0.024 and accuracy 0.2511\n",
      "Epoch 5 train loss  0.6852 valid loss 0.024 and accuracy 0.2556\n",
      "Epoch 6 train loss  0.6807 valid loss 0.024 and accuracy 0.2600\n",
      "Epoch 7 train loss  0.6759 valid loss 0.023 and accuracy 0.2857\n",
      "Epoch 8 train loss  0.6709 valid loss 0.023 and accuracy 0.2924\n",
      "Epoch 9 train loss  0.6600 valid loss 0.023 and accuracy 0.3337\n",
      "Epoch 10 train loss  0.6402 valid loss 0.023 and accuracy 0.3929\n",
      "Epoch 11 train loss  0.6060 valid loss 0.023 and accuracy 0.4342\n",
      "Epoch 12 train loss  0.5654 valid loss 0.023 and accuracy 0.4609\n",
      "Epoch 13 train loss  0.5134 valid loss 0.023 and accuracy 0.4777\n",
      "Epoch 14 train loss  0.4632 valid loss 0.024 and accuracy 0.4810\n",
      "Epoch 15 train loss  0.4276 valid loss 0.025 and accuracy 0.5000\n",
      "Epoch 16 train loss  0.3838 valid loss 0.026 and accuracy 0.5201\n",
      "Epoch 17 train loss  0.3520 valid loss 0.028 and accuracy 0.5056\n",
      "Epoch 18 train loss  0.3302 valid loss 0.028 and accuracy 0.5402\n",
      "Epoch 19 train loss  0.3112 valid loss 0.029 and accuracy 0.5446\n",
      "Epoch 20 train loss  0.2881 valid loss 0.030 and accuracy 0.5525\n",
      "Epoch 21 train loss  0.2802 valid loss 0.030 and accuracy 0.5513\n",
      "Epoch 22 train loss  0.2536 valid loss 0.032 and accuracy 0.5614\n",
      "Epoch 23 train loss  0.2374 valid loss 0.033 and accuracy 0.5580\n",
      "Epoch 24 train loss  0.2397 valid loss 0.034 and accuracy 0.5536\n",
      "Epoch 25 train loss  0.2338 valid loss 0.037 and accuracy 0.5580\n",
      "Epoch 26 train loss  0.2196 valid loss 0.038 and accuracy 0.5569\n",
      "Epoch 27 train loss  0.2192 valid loss 0.035 and accuracy 0.5703\n",
      "Epoch 28 train loss  0.2094 valid loss 0.039 and accuracy 0.5525\n",
      "Epoch 29 train loss  0.2023 valid loss 0.041 and accuracy 0.5413\n",
      "Epoch 30 train loss  0.1950 valid loss 0.039 and accuracy 0.5603\n",
      "Epoch 31 train loss  0.1777 valid loss 0.041 and accuracy 0.5536\n",
      "Epoch 32 train loss  0.1815 valid loss 0.040 and accuracy 0.5502\n",
      "Epoch 33 train loss  0.1690 valid loss 0.040 and accuracy 0.5525\n",
      "Epoch 34 train loss  0.1763 valid loss 0.040 and accuracy 0.5592\n",
      "Epoch 35 train loss  0.1719 valid loss 0.038 and accuracy 0.5670\n",
      "Epoch 36 train loss  0.1648 valid loss 0.039 and accuracy 0.5636\n",
      "Epoch 37 train loss  0.1614 valid loss 0.039 and accuracy 0.5725\n",
      "Epoch 38 train loss  0.1556 valid loss 0.040 and accuracy 0.5781\n",
      "Epoch 39 train loss  0.1564 valid loss 0.042 and accuracy 0.5625\n",
      "Epoch 40 train loss  0.1588 valid loss 0.042 and accuracy 0.5692\n",
      "Epoch 41 train loss  0.1479 valid loss 0.042 and accuracy 0.5781\n",
      "Epoch 42 train loss  0.1522 valid loss 0.043 and accuracy 0.5569\n",
      "Epoch 43 train loss  0.1439 valid loss 0.040 and accuracy 0.5580\n",
      "Epoch 44 train loss  0.1449 valid loss 0.040 and accuracy 0.5603\n",
      "Epoch 45 train loss  0.1392 valid loss 0.041 and accuracy 0.5547\n",
      "Epoch 46 train loss  0.1425 valid loss 0.042 and accuracy 0.5692\n",
      "Epoch 47 train loss  0.1387 valid loss 0.044 and accuracy 0.5703\n",
      "Epoch 48 train loss  0.1325 valid loss 0.047 and accuracy 0.5703\n",
      "Epoch 49 train loss  0.1367 valid loss 0.048 and accuracy 0.5625\n"
     ]
    }
   ],
   "source": [
    "training_results = train(lstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neural-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.3009]), points: 46\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2484]), points: -3\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "animated-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_{CATEGORY}'\n",
    "torch.save(lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-northwest",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "burning-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "miniature-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "optimizer = get_optimizer(bilstm, lr = 0.01, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "described-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4343 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 1 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 2 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 3 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 4 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 5 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 6 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 7 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 8 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 9 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 10 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 11 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 12 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 13 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 14 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 15 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 16 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 17 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 18 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 19 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 20 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 21 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 22 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 23 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 24 train loss  57.1970 valid loss 2.762 and accuracy 0.2500\n",
      "Epoch 25 train loss  78.9499 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 26 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 27 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 28 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 29 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 30 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 31 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 32 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 33 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 34 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 35 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 36 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 37 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 38 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 39 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 40 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 41 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 42 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 43 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 44 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 45 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 46 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 47 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 48 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n",
      "Epoch 49 train loss  42.8030 valid loss 0.921 and accuracy 0.7500\n"
     ]
    }
   ],
   "source": [
    "training_results = train(bilstm, optimizer, train_dt, valid_dt, validate, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "operating-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2478]), points: -2\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2418]), points: -15\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(bilstm, dev_categ, trainset.encode, evaluator)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(bilstm, test_categ, trainset.encode, evaluator)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/bilstm_{CATEGORY}'\n",
    "torch.save(bilstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-oliver",
   "metadata": {},
   "source": [
    "### Modelos supervisados IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "swiss-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "mixed_training_ir = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "local-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_categ = filter_by_category(mixed_training_ir, category=CATEGORY)\n",
    "validation_categ = filter_by_category(validation_instances, category=CATEGORY)\n",
    "testing_categ = filter_by_category(testing_instances, category=CATEGORY)\n",
    "\n",
    "dev_categ = filter_by_category(validation, category=CATEGORY)\n",
    "test_categ = filter_by_category(testing, category=CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "excessive-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(training_categ)\n",
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "geological-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=training_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_categ, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_categ, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "numerous-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hawaiian-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')\n",
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-christianity",
   "metadata": {},
   "source": [
    "#### LSTM-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "medieval-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fourth-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4379 valid loss 0.180 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7679 valid loss 0.093 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.6700 valid loss 0.095 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.7141 valid loss 0.067 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.7683 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.7264 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.7112 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.7231 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.7147 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7074 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7006 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7026 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.6916 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.6823 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.6836 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.7086 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.7170 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.7195 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.7185 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.7108 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7184 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.7091 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.7041 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6999 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.6981 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6965 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.6956 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6991 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.7028 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6860 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6907 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6895 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6800 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6630 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.6664 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6556 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.6614 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.6694 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.6849 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.6846 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6606 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.6601 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.6744 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.6890 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.6767 valid loss 0.027 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.6597 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.6796 valid loss 0.029 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.6559 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.6520 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.6827 valid loss 0.029 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dress-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2743]), points: 22\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2484]), points: -3\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_qa_{CATEGORY}'\n",
    "torch.save(lstm_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-worker",
   "metadata": {},
   "source": [
    "#### LSTM-QA/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(lstm_cnn_qa, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capital-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.4318 valid loss 0.179 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.7051 valid loss 0.136 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.7541 valid loss 0.109 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.8325 valid loss 0.030 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.7264 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.7142 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.7022 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.6929 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.6995 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.7016 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.7073 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.7025 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.7064 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.7028 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.6986 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.6987 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.6964 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.6989 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.7002 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.6960 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.7045 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.7007 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.6984 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.6925 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.7077 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.6962 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.7031 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.6961 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.6972 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.6884 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.6824 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.6906 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.6795 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.6819 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.7139 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.6907 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.7111 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.6966 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.6998 valid loss 0.025 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.6901 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.6910 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.6903 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.6866 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.7075 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.6913 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.7012 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.6978 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.7033 valid loss 0.026 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.6773 valid loss 0.028 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.6896 valid loss 0.030 and accuracy 0.2556\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(lstm_cnn_qa, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-telephone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Dominio: psychology\n",
      "accuracy: tensor([0.2389]), points: -10\n",
      "----------\n",
      "TEST Dominio: psychology\n",
      "accuracy: tensor([0.2813]), points: 57\n"
     ]
    }
   ],
   "source": [
    "acc, points = evaluate(lstm_cnn_qa, dev_categ, trainset.encode, evaluator_ir)\n",
    "print(f'DEV Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')\n",
    "print('----------')\n",
    "acc, points = evaluate(lstm_cnn_qa, test_categ, trainset.encode, evaluator_ir)\n",
    "print(f'TEST Dominio: {CATEGORY}')\n",
    "print(f'accuracy: {acc}, points: {points}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "changing-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + f'/trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "torch.save(lstm_cnn_qa.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-converter",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "typical-holmes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\mds\\TFM\\head-qa-afi\\code/trained_models_v2/logistic_regressor_psychology\n",
      "DEV\n",
      "Accuracy media 0.26106194\n",
      "Puntos media 10.0\n",
      "[tensor(0.2611)]\n",
      "[10]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24173912\n",
      "Puntos media -7.5\n",
      "[tensor(0.2435), tensor(0.2400)]\n",
      "[-6, -9]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.30088496\n",
      "Puntos media 46.0\n",
      "[tensor(0.3009)]\n",
      "[46]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24811596\n",
      "Puntos media -1.5\n",
      "[tensor(0.2696), tensor(0.2267)]\n",
      "[18, -21]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.24778761\n",
      "Puntos media -2.0\n",
      "[tensor(0.2478)]\n",
      "[-2]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24173912\n",
      "Puntos media -7.5\n",
      "[tensor(0.2435), tensor(0.2400)]\n",
      "[-6, -9]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regressor = LogisticRegression(trainset.max_length, 1)\n",
    "lstm = BasicLSTM(len(vocab), 64, trainset.max_length, 1, embedding_dim=100)\n",
    "bilstm = BiLSTM_model(embedding_matrix.shape[1], embedding_matrix.shape[0], 1, \n",
    "                     pretrained_embeddings=embedding_matrix, max_length=trainset.max_length)\n",
    "\n",
    "models = [logistic_regressor, lstm, bilstm]\n",
    "paths = [os.getcwd() + f'/trained_models_v2/logistic_regressor_{CATEGORY}', \n",
    "         os.getcwd() + f'/trained_models_v2/lstm_{CATEGORY}',         \n",
    "         os.getcwd() + f'/trained_models_v2/bilstm_{CATEGORY}']\n",
    "\n",
    "print(paths[0])\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "joint-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Loading pretrained embeddings...\n",
      "DEV\n",
      "Accuracy media 0.27433628\n",
      "Puntos media 22.0\n",
      "[tensor(0.2743)]\n",
      "[22]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.24816425\n",
      "Puntos media -1.5\n",
      "[tensor(0.2652), tensor(0.2311)]\n",
      "[14, -17]\n",
      "---------\n",
      "\n",
      "DEV\n",
      "Accuracy media 0.23893805\n",
      "Puntos media -10.0\n",
      "[tensor(0.2389)]\n",
      "[-10]\n",
      "---------\n",
      "TEST\n",
      "Accuracy media 0.28111112\n",
      "Puntos media 28.5\n",
      "[tensor(0.3000), tensor(0.2622)]\n",
      "[46, 11]\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_qa = LSTM_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "lstm_cnn_qa = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "models = [lstm_qa, lstm_cnn_qa]\n",
    "\n",
    "paths = [os.getcwd() + f'/trained_models_v2/lstm_qa_{CATEGORY}',\n",
    "         os.getcwd() + f'/trained_models_v2/lstm_cnn_qa_{CATEGORY}'\n",
    "        ]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.load_state_dict(torch.load(paths[i]))\n",
    "    model.eval()\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, dev_categ, trainset.encode, evaluator_ir)\n",
    "    print('DEV')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    acc, points, acc_list, points_list = evaluate_better(model, test_categ, trainset.encode, evaluator_ir)\n",
    "    print('TEST')\n",
    "    print('Accuracy media', acc)\n",
    "    print('Puntos media', points)\n",
    "    print(acc_list)\n",
    "    print(points_list)\n",
    "    print('---------')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-favorite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
