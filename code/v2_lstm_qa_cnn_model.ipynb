{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rural-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils_data import Vectorizer, HeadQA, HeadQA_IR, clean_words, parse_dataset, parse_ir_dataset, random_oversamplig, save_dataset_to_pickle, load_dataset_from_pickle\n",
    "from training import train, validate, evaluate, evaluator_ir, train_ir, validate_ir, load_embeddings_from_file, make_embedding_matrix\n",
    "from training import get_optimizer, evaluate_better\n",
    "\n",
    "import transformers\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "from ir_models import LSTM_CNN_QA\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset head_qa (C:\\Users\\tec005m\\.cache\\huggingface\\datasets\\head_qa\\es\\1.1.0\\473dc5357942a3ff52963bd73cad0d167bd1bbc1ca5ca0732ee7372b480dd735)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_es = load_dataset('head_qa', 'es' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, testing = data_es['train'], data_es['validation'], data_es['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = load_dataset_from_pickle('../data/training_ir.pickle')\n",
    "validation_instances = load_dataset_from_pickle('../data/validation_ir.pickle')\n",
    "testing_instances = load_dataset_from_pickle('../data/testing_ir.pickle')\n",
    "mixed_training = load_dataset_from_pickle('../data/mixed_oversampling_training_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "urban-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer.vectorize_ir_dataset(mixed_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlling-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.sentence_vocab\n",
    "label_vocab = vectorizer.label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amino-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = HeadQA_IR(instances=mixed_training, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "validset = HeadQA_IR(instances=validation_instances, vectorizer=vectorizer, right_padding=False, max_length=15)\n",
    "testset = HeadQA_IR(instances=testing_instances, vectorizer=vectorizer, right_padding=False, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greek-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dt = DataLoader(trainset, batch_size=batch_size,drop_last=True)\n",
    "valid_dt = DataLoader(validset, batch_size=batch_size,drop_last=True)\n",
    "test_dt = DataLoader(testset, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tough-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = load_dataset_from_pickle('trained_models/biomedical_embeddings/word_to_index_ir.pickle')\n",
    "embeddings = load_dataset_from_pickle('trained_models/biomedical_embeddings/wordvectors_ir.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "silent-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"trained_models/biomedical_embeddings/Scielo_wiki_FastText300.vec\"\n",
    "words = vocab.vocab2index.keys()\n",
    "embedding_matrix = make_embedding_matrix(embedding_file, list(words), word_to_idx, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indirect-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "optimizer = get_optimizer(model, lr = 0.001, wd = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "designing-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tec005m\\Anaconda3\\envs\\afi\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss  0.3662 valid loss 0.058 and accuracy 0.2500\n",
      "Epoch 1 train loss  0.4419 valid loss 0.040 and accuracy 0.2500\n",
      "Epoch 2 train loss  0.4381 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 3 train loss  0.4143 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 4 train loss  0.4194 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 5 train loss  0.4207 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 6 train loss  0.4175 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 7 train loss  0.4179 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 8 train loss  0.4154 valid loss 0.036 and accuracy 0.2500\n",
      "Epoch 9 train loss  0.4214 valid loss 0.036 and accuracy 0.2500\n",
      "Epoch 10 train loss  0.4289 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 11 train loss  0.4270 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 12 train loss  0.4217 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 13 train loss  0.4192 valid loss 0.037 and accuracy 0.2500\n",
      "Epoch 14 train loss  0.4260 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 15 train loss  0.4228 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 16 train loss  0.4279 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 17 train loss  0.4174 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 18 train loss  0.4170 valid loss 0.036 and accuracy 0.2500\n",
      "Epoch 19 train loss  0.4272 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 20 train loss  0.4259 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 21 train loss  0.4257 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 22 train loss  0.4291 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 23 train loss  0.4273 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 24 train loss  0.4275 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 25 train loss  0.4267 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 26 train loss  0.4299 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 27 train loss  0.4297 valid loss 0.037 and accuracy 0.2500\n",
      "Epoch 28 train loss  0.4283 valid loss 0.038 and accuracy 0.2500\n",
      "Epoch 29 train loss  0.4268 valid loss 0.037 and accuracy 0.2500\n",
      "Epoch 30 train loss  0.4478 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 31 train loss  0.4318 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 32 train loss  0.4368 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 33 train loss  0.4290 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 34 train loss  0.4356 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 35 train loss  0.4288 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 36 train loss  0.4256 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 37 train loss  0.4514 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 38 train loss  0.4325 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 39 train loss  0.4211 valid loss 0.034 and accuracy 0.2500\n",
      "Epoch 40 train loss  0.4381 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 41 train loss  0.4282 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 42 train loss  0.4264 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 43 train loss  0.4217 valid loss 0.031 and accuracy 0.2500\n",
      "Epoch 44 train loss  0.4206 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 45 train loss  0.4141 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 46 train loss  0.4133 valid loss 0.035 and accuracy 0.2500\n",
      "Epoch 47 train loss  0.4238 valid loss 0.033 and accuracy 0.2500\n",
      "Epoch 48 train loss  0.4063 valid loss 0.032 and accuracy 0.2500\n",
      "Epoch 49 train loss  0.4113 valid loss 0.032 and accuracy 0.2500\n"
     ]
    }
   ],
   "source": [
    "training_results = train_ir(model, optimizer, train_dt, valid_dt, validate_ir, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spiritual-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2538]), 42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, testing, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "built-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2540]), 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points = evaluate(model, validation, trainset.encode, evaluator_ir)\n",
    "acc, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "leading-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_to_pickle('results_v2/train_results_lstm_cnn_qa.pickle', training_results)\n",
    "training_results = load_dataset_from_pickle('results_v2/train_results_lstm_cnn_qa.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "complimentary-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/trained_models_v2/lstm_cnn_qa'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cheap-adoption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM_CNN_QA(\n",
       "  (emb): Embedding(28821, 300, padding_idx=0)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(300, 64, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (conv): Conv1d(2, 10, kernel_size=(3,), stride=(1,))\n",
       "  (cosine): CosineSimilarity()\n",
       "  (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear1): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "model = LSTM_CNN_QA(vocab_size=len(vocab), hidden_size=64, x_size=trainset.max_length, n_classes=1, embedding_size=300,\n",
    "               pretrained_embeddings=embedding_matrix)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "studied-authorization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25407282,\n",
       " 3.6666666666666665,\n",
       " [tensor(0.2832),\n",
       "  tensor(0.2652),\n",
       "  tensor(0.2267),\n",
       "  tensor(0.2251),\n",
       "  tensor(0.2655),\n",
       "  tensor(0.2588)],\n",
       " [30, 14, -21, -23, 14, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, validation, trainset.encode, evaluator_ir)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "solar-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2538183,\n",
       " 3.5,\n",
       " [tensor(0.2412),\n",
       "  tensor(0.2825),\n",
       "  tensor(0.2675),\n",
       "  tensor(0.2672),\n",
       "  tensor(0.2435),\n",
       "  tensor(0.3030),\n",
       "  tensor(0.2124),\n",
       "  tensor(0.2371),\n",
       "  tensor(0.2183),\n",
       "  tensor(0.2641),\n",
       "  tensor(0.2622),\n",
       "  tensor(0.2467)],\n",
       " [-8, 29, 16, 16, -6, 49, -34, -12, -29, 13, 11, -3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, points, acc_list, points_list = evaluate_better(model, testing, trainset.encode, evaluator_ir)\n",
    "acc, points, acc_list, points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-utilization",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
